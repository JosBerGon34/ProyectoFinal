{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.VAR SELECTION:\n",
    "\n",
    "Vamos a preparar los dataframes, descargandolos de la base de datos sql \"database9k.db\" ademas de escoger las variables y preparar los dataframes para un FeatureEngineering óptimo. Por lo que haremos una primera tanteada de\n",
    "modelos de clasificacion y escogeremos en funcion del F1 score, precisión , etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Creacion de Subgrupos de paises:\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "import sqlite3\n",
    "#1.1 Nos conectamos a la base de datos.\n",
    "database_path = os.path.abspath(\"C:/GitHubRepos/ProyectoFinal/data/processed/Dataset9k.db\")\n",
    "print(f\"Database path: {database_path}\")\n",
    "conn = create_engine(f'sqlite:///{database_path}')\n",
    "# Execute the SQL query, ya hemos visto en el visualizador de sql que los datos que deberian ser numericos hay algunas columnas que son formato texto, \n",
    "# hacemos un query para transformarlos justo antes de almacenarlos en un dataframe\n",
    "\n",
    "#DF inflacion\n",
    "query = \"\"\"\n",
    "SELECT País, REPLACE(Inflación, ',', '.') AS Inflación_Float\n",
    "FROM EuroInfl\n",
    "\"\"\"\n",
    "#Creamos un diccionario para almacenar la conversion\n",
    "dtype_dict = {'Inflación_Float': float}\n",
    "# Load the DataFrame with the dtype dictionary\n",
    "df_inf = pd.read_sql_query(query, conn, dtype=dtype_dict)\n",
    "df_inf.drop_duplicates(subset='País', keep='first', inplace=True)\n",
    "\n",
    "#DF PPC\n",
    "query = f\"\"\"\n",
    "SELECT País, ROUND(PPC) AS PPC_INTEGER\n",
    "FROM EuroPPC\n",
    "\"\"\"\n",
    "df_PPC = pd.read_sql_query(query, conn)\n",
    "df_PPC.drop_duplicates(subset='País', keep='first', inplace=True)\n",
    "print(df_PPC)\n",
    "\n",
    "#DF SBMM\n",
    "query = \"\"\"\n",
    "SELECT País, REPLACE(SBMM, ',', '.') AS SBMM_Float\n",
    "FROM EuroSBMM\n",
    "\"\"\"\n",
    "dtype_dict = {'SBMM': float}\n",
    "df_SBMM = pd.read_sql_query(query, conn)\n",
    "df_SBMM.drop_duplicates(subset='País', keep='first', inplace=True)\n",
    "df_SBMM['País'] = df_SBMM['País'].str.strip().str.lower()\n",
    "\n",
    "conn.dispose()\n",
    "print(df_SBMM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No hay forma de cambiar la columna paises de la tabla SBMM, por lo que habra que realizar un diccionario de equivalencias para hacer coincidir la columna paises en los tres\n",
    "#Dataframes de graficos, realizaremos posteriormente una clusterizacion para elegir 3 subgrupos de paises con economias similares y poder compararlos\n",
    "#A nivel general con la Union Europea.\n",
    "país_equivalencias = {\n",
    "    'bulgaria :bg:': 'Bulgaria',\n",
    "    'rumanía :ro:': 'Rumania',\n",
    "    'hungría :hu:': 'Hungría',\n",
    "    'croacia :hr:': 'Croacia',\n",
    "    'polonia :pl:': 'Polonia',\n",
    "    'letonia :lv:': 'Letonia',\n",
    "    'eslovaquia :sk:': 'República Eslovaca',\n",
    "    'grecia :gr:': 'Grecia',\n",
    "    'lituania :lt:': 'Lituania',\n",
    "    'estonia :ee:': 'Estonia',\n",
    "    'portugal :pt:': 'Portugal',\n",
    "    'república checa :cz:': 'República Checa',\n",
    "    'malta :mt:': 'Malta',\n",
    "    'chipre :cy:': 'Chipre',\n",
    "    'eslovenia :si:': 'Eslovenia',\n",
    "    'españa :es:': 'España',\n",
    "    'italia :it:': 'Italia',\n",
    "    'media ue :eu:': 'Unión Europea',\n",
    "    'francia :fr:': 'Francia',\n",
    "    'suecia :se:': 'Suecia',\n",
    "    'finlandia :fi:': 'Finlandia',\n",
    "    'austria :at:': 'Austria',\n",
    "    'países bajos :nl:': 'Países Bajos',\n",
    "    'bélgica :be:': 'Bélgica',\n",
    "    'irlanda :ie:': 'Irlanda',\n",
    "    'alemania :de:': 'Alemania',\n",
    "    'dinamarca :dk:': 'Dinamarca',\n",
    "    'luxemburgo :lu:': 'Luxemburgo',\n",
    "}\n",
    "df_SBMM['País'] = df_SBMM['País'].map(país_equivalencias)\n",
    "print(df_SBMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Antes de la concatenacion de los dataframe hemos de hacer coincidir el tipo de datos, coincidir el index \n",
    "\n",
    "df_inf.reset_index(drop=True, inplace=True)\n",
    "print(df_inf['País'].dtype)\n",
    "print(df_inf['Inflación_Float'].dtype)\n",
    "print(df_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_PPC dtypes\n",
    "df_PPC.reset_index(drop=True, inplace=True)\n",
    "print(df_PPC['País'].dtype)\n",
    "print(df_PPC['PPC_INTEGER'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_SBMM\n",
    "df_SBMM.reset_index(drop=True, inplace=True)\n",
    "print(df_SBMM['País'].dtype)\n",
    "print(df_SBMM['SBMM_Float'].dtype)\n",
    "#Aqui tenemos el problema, hemos de pasar la columna SBMM_FLoat a float64\n",
    "df_SBMM['SBMM_Float'] = df_SBMM['SBMM_Float'].astype('float64')\n",
    "print(df_SBMM['SBMM_Float'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparamos el dataframe concatenado filtrando por valores unicos de paises y sus valores numericos en cada uno de los dataframes.\n",
    "# Encontrar países comunes\n",
    "paises_comunes = set(df_inf['País']).intersection(set(df_PPC['País']), set(df_SBMM['País']))\n",
    "\n",
    "# Filtrar DataFrames para países comunes y establecer 'País' como índice\n",
    "df_i1 = df_inf[df_inf['País'].isin(paises_comunes)].set_index('País')\n",
    "df_P1 = df_PPC[df_PPC['País'].isin(paises_comunes)].set_index('País')\n",
    "df_S1 = df_SBMM[df_SBMM['País'].isin(paises_comunes)].set_index('País')\n",
    "# Concatenar los DataFrames por columnas\n",
    "df_IPS = pd.concat([df_i1, df_P1, df_S1], axis=1)\n",
    "#Eliminamos variables usadas que no son inutiles.\n",
    "df_IPS.to_csv('C:/GitHubRepos/ProyectoFinal/data/processed/EuEco.csv', index=True)\n",
    "del df_i1, df_P1, df_S1, df_inf, df_SBMM, df_PPC, país_equivalencias, dtype_dict, paises_comunes, df_IPS\n",
    "df1= pd.read_csv('C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\EuEco.csv')\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#1.5 Representacion Graficos Economicos.\n",
    "# Crear una grupo de graficos lineales con 3 subplots en una fila\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 8))\n",
    "# Graficar cada DataFrame en un subplot diferente\n",
    "axes[0].plot(df1['País'], df1['Inflación_Float'])\n",
    "axes[0].set_title('Inflacion Europa 2023')\n",
    "axes[0].set_xlabel('Paises Euro')\n",
    "axes[0].set_ylabel('Porcentaje')\n",
    "axes[0].tick_params(axis='x', rotation=90)  \n",
    "\n",
    "axes[1].plot(df1['País'], df1['SBMM_Float'], color='Green')\n",
    "axes[1].set_title('Sueldo Bruto Mensual Medio')\n",
    "axes[1].set_xlabel('Paises Euro')\n",
    "axes[1].set_ylabel('Euros')\n",
    "axes[1].set_ylim(ymin=0) \n",
    "axes[1].tick_params(axis='x', rotation=90)  \n",
    "\n",
    "axes[2].plot(df1['País'], df1['PPC_INTEGER'],  color='Orange')\n",
    "axes[2].set_title('PIB Per Capita Anual')\n",
    "axes[2].set_xlabel('Paises Euro')\n",
    "axes[2].set_ylabel('Miles de Euros')\n",
    "axes[2].set_ylim(ymin=0)  \n",
    "axes[2].tick_params(axis='x', rotation=90) \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "df1.set_index('País', inplace=True)\n",
    "for column in ['Inflación_Float', 'PPC_INTEGER', 'SBMM_Float']:\n",
    "    sns.lineplot(data=df1, x=df1.index, y=column, label=column)\n",
    "plt.title('Evolución de indicadores económicos por país')\n",
    "plt.xlabel('País')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend(title='Indicador')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()\n",
    "del axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora vamos a crear 3 subgrupos de paises: \n",
    "#Los haremos automaticamente con KMeans, podemos predecir los grupos\n",
    "#por las caracteristicas economicas.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "\n",
    "X = df1[['Inflación_Float', 'PPC_INTEGER', 'SBMM_Float']]\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(X)\n",
    "df1['cluster'] = kmeans.labels_\n",
    "fig = px.scatter_3d(df1, x='Inflación_Float', y='PPC_INTEGER', z='SBMM_Float',\n",
    "              color='cluster',\n",
    "              opacity=0.7,\n",
    "              color_continuous_scale='viridis',\n",
    "              symbol='cluster',\n",
    "              size_max=18)\n",
    "fig.update_layout(\n",
    "    title='Clustering de países en 3D',\n",
    "    scene = dict(\n",
    "        xaxis_title='Inflación',\n",
    "        yaxis_title='PPC',\n",
    "        zaxis_title='SBMM'\n",
    "    ),\n",
    "    legend=dict(\n",
    "        title='Cluster',\n",
    "        yanchor=\"top\",\n",
    "        xanchor=\"right\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora llamamos a los valores unicos del indice default del df1 que se agrupan en cada cluster para seleccionar ademas \n",
    "#del criterio matematico, razones geograficas y culturales.\n",
    "\n",
    "for i in range(kmeans.n_clusters):\n",
    "    cluster_i = df1[df1['cluster'] == i].index\n",
    "    print(f\"Países en el cluster {i}:\")\n",
    "    print(cluster_i.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos el df base del servidor sql.\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\Dataset9k.db')\n",
    "query = \"SELECT * FROM ZonaEuroESS\"\n",
    "df_encuestas = pd.read_sql_query(query, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Factorizamos nuestra variable target ()\n",
    "import json\n",
    "import os\n",
    "mapping = {'SocialComunista': 0, 'Socialista': 1, 'Demócrata(Centro)': 2, 'Conservador': 3, 'ExtremaDerecha': 4}\n",
    "def agrupar_lrscale(valor):\n",
    "    if 0 <= valor <= 2:\n",
    "        return 'SocialComunista'\n",
    "    elif 3 == valor <= 4:\n",
    "        return 'Socialista'\n",
    "    elif 5 == valor <= 6:\n",
    "        return 'Demócrata(Centro)'\n",
    "    elif 7 == valor <= 8:\n",
    "        return 'Conservador'\n",
    "    else:\n",
    "        return 'ExtremaDerecha'\n",
    "\n",
    "\n",
    "df_encuestas['ideo'] = df_encuestas['lrscale'].apply(agrupar_lrscale)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_encuestas['ideo_fc'] = df_encuestas['ideo'].map(mapping)\n",
    "\n",
    "\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/ideo_factors.json\"\n",
    "\n",
    "\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mapping, f)\n",
    "df_encuestas = df_encuestas.apply(lambda col: col.astype(int) if col.dtype == 'float64' else col)\n",
    "print(df_encuestas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Supongamos que df_encuestas y df1 ya están cargados\n",
    "\n",
    "# Fusionar los dataframes\n",
    "df_encuestas = pd.merge(df_encuestas, df1[['cluster']], left_on='cntry', right_index=True, how='left')\n",
    "\n",
    "# Manejar valores faltantes\n",
    "df_encuestas['cluster'] = df_encuestas['cluster'].fillna(3)\n",
    "#CLUSTER 0 = PAISES RICOS. CLUSTER 1 = PAISES INFERIORES A LA MEDIA DE LA UE(CLUSTER 2), CLUSTER 2= ZONA MEDIA EUW(ITALIA, ESPAÑA, CLUSTER 3= PERIFERIA EUW(ISRAEL, TURQUIA, ETC.))\n",
    "print(df_encuestas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora que tenemos etiquetadas las muestras por Clusters, zonas geograficas y grupos economicos. Podemos reducir las muestras para trabajar con ellas.\n",
    "#1) Vamos a reducir el dataframe a 2mil muestras por cluster. Vamos a eliminar las muestras del Cluster NUmero 3 en nuestro nuevo dataframe ya que no nos interesa los paises perifericos a europa por diferencias culturales entre otros motivos.\n",
    "import pandas as pd\n",
    "df_clusters = {}\n",
    "for cluster in df_encuestas['cluster'].unique():\n",
    "    df_clusters[cluster] = df_encuestas[df_encuestas['cluster'] == cluster]\n",
    "\n",
    "tamano_muestra = 2000\n",
    "\n",
    "dfC0a2 = pd.concat([\n",
    "    df_clusters[0].sample(n=tamano_muestra, random_state=42),\n",
    "    df_clusters[1].sample(n=tamano_muestra, random_state=42),\n",
    "    df_clusters[2].sample(n=tamano_muestra, random_state=42),\n",
    "    df_clusters[3].sample(n=tamano_muestra, random_state=42)\n",
    "])\n",
    "\n",
    "print(dfC0a2 .head())  \n",
    "print(dfC0a2 ['cluster'].value_counts())\n",
    "dfC0a2 = dfC0a2.apply(lambda col: col.astype(int) if col.dtype == 'float64' else col)\n",
    "dfc= dfC0a2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, df1, df_clusters, df_encuestas, dfC0a2['cntry'], dfC0a2['orig'], dfC0a2['ideo'], dfC0a2['lrscale'], mapping\n",
    "#Eliminamos las columnas sin factorizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#Eliminamos CSV y archivos innecesarios. Tenemos el raw en la database, si queremos trabajar de forma distina con ellos.\n",
    "import os\n",
    "def eliminar_archivo_csv(ruta_archivo):\n",
    "  try:\n",
    "    os.remove(ruta_archivo)\n",
    "    print(f\"El archivo CSV '{ruta_archivo}' ha sido eliminado.\")\n",
    "  except FileNotFoundError:\n",
    "    print(f\"El archivo CSV '{ruta_archivo}' no existe.\")\n",
    "  except Exception as e:\n",
    "    print(f\"Ocurrió un error al eliminar el archivo CSV: {e}\")\n",
    "ruta_del_archivo = \"C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\dfencuestas.csv\"  # Reemplaza con la ruta real de tu archivo\n",
    "eliminar_archivo_csv(ruta_del_archivo)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el dataframe de 6mil muestras antes de hacer test de score para seleccionar el dataframe para el Eda.\n",
    "dfC0a2.to_csv('C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\df6k.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos probar un Random Forest con 3 dataframes con distintos numeros de variables, nos quedaremos con el que obtenga mas score de los tres.\n",
    "#La variable ['ideo_fc'] es nuestra variable target.\n",
    "#Modelo uno 11 variables(10 predictoras, 1 target), 16 vars, 21 vars.\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, chi2 #(chi2 es el criterio matematico apropiado para vars categoricas factorizadas.)\n",
    "\n",
    "def seleccionar_mejores_variables(df, k, target='ideo_fc'):\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    selector = SelectKBest(chi2, k=k) \n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    mejores_features = X.columns[selector.get_support()]\n",
    "    print(f\"Mejores features ({k} features):\", mejores_features)\n",
    "    df_seleccionado = df[[target] + list(mejores_features)]\n",
    "    return df_seleccionado\n",
    "d11= seleccionar_mejores_variables(dfC0a2, 10)  # 10 predictoras + 1 objetivo = 11\n",
    "d16 = seleccionar_mejores_variables(dfC0a2, 15)  # 15 predictoras + 1 objetivo = 16\n",
    "d21 = seleccionar_mejores_variables(dfC0a2, 20)  # 20 predictoras + 1 objetivo = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Supongamos que ya tienes los dataframes df_11, df_16 y df_21\n",
    "\n",
    "# Función para entrenar y evaluar un modelo\n",
    "def entrenar_y_evaluar(X_train, y_train, X_test, y_test):\n",
    "    modelo = RandomForestClassifier(random_state=42)\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return modelo  # Devuelve el modelo entrenado\n",
    "\n",
    "# Entrenar y evaluar con df_11\n",
    "X_11 = d11.drop('ideo_fc', axis=1)\n",
    "y_11 = d11['ideo_fc']\n",
    "X_train_11, X_test_11, y_train_11, y_test_11 = train_test_split(\n",
    "    X_11, y_11, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Resultados con df_11:\")\n",
    "modelo_11 = entrenar_y_evaluar(X_train_11, y_train_11, X_test_11, y_test_11)\n",
    "\n",
    "# Entrenar y evaluar con df_16\n",
    "X_16 = d16.drop('ideo_fc', axis=1)\n",
    "y_16 = d16['ideo_fc']\n",
    "X_train_16, X_test_16, y_train_16, y_test_16 = train_test_split(\n",
    "    X_16, y_16, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"\\nResultados con df_16:\")\n",
    "modelo_16 = entrenar_y_evaluar(X_train_16, y_train_16, X_test_16, y_test_16)\n",
    "\n",
    "# Entrenar y evaluar con df_21\n",
    "X_21 = d21.drop('ideo_fc', axis=1)\n",
    "y_21 = d21['ideo_fc']\n",
    "X_train_21, X_test_21, y_train_21, y_test_21 = train_test_split(\n",
    "    X_21, y_21, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"\\nResultados con df_21:\")\n",
    "modelo_21 = entrenar_y_evaluar(X_train_21, y_train_21, X_test_21, y_test_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Función para entrenar y evaluar un modelo (adaptada para regresión logística)\n",
    "def entrenar_y_evaluar_logreg(X_train, y_train, X_test, y_test):\n",
    "    modelo = LogisticRegression(random_state=42, max_iter=1000)  # Aumentamos max_iter si es necesario\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return modelo\n",
    "\n",
    "# Entrenar y evaluar con df_11_seleccionado\n",
    "X_11 = d11.drop('ideo_fc', axis=1)\n",
    "y_11 = d11['ideo_fc']\n",
    "X_train_11, X_test_11, y_train_11, y_test_11 = train_test_split(\n",
    "    X_11, y_11, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Resultados con df_11_seleccionado (Regresión Logística):\")\n",
    "modelo_11_logreg = entrenar_y_evaluar_logreg(X_train_11, y_train_11, X_test_11, y_test_11)\n",
    "\n",
    "# Entrenar y evaluar con df_16_seleccionado\n",
    "X_16 = d16.drop('ideo_fc', axis=1)\n",
    "y_16 = d16['ideo_fc']\n",
    "X_train_16, X_test_16, y_train_16, y_test_16 = train_test_split(\n",
    "    X_16, y_16, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"\\nResultados con df_16_seleccionado (Regresión Logística):\")\n",
    "modelo_16_logreg = entrenar_y_evaluar_logreg(X_train_16, y_train_16, X_test_16, y_test_16)\n",
    "\n",
    "# Entrenar y evaluar con df_21_seleccionado\n",
    "X_21 = d21.drop('ideo_fc', axis=1)\n",
    "y_21 = d21['ideo_fc']\n",
    "X_train_21, X_test_21, y_train_21, y_test_21 = train_test_split(\n",
    "    X_21, y_21, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"\\nResultados con df_21_seleccionado (Regresión Logística):\")\n",
    "modelo_21_logreg = entrenar_y_evaluar_logreg(X_train_21, y_train_21, X_test_21, y_test_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dado los bajos resultados obtenidos, aun siendo en default, voy a hacer un repaso de la naturaleza de las variables de nuestro dataframe, y factorizar escalas de intensidad 0 a 10.\n",
    "#Probaremos nuestra target tanto factorizada como en escala.\n",
    "print(dfC0a2.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloque1(media): dfC0a2['netusoft', 'nwsptot', 'ppltrst', 'rdtot', 'tvtot']\n",
    "#Vamos a unir todas las variables que hacen referencia al uso de medios de comunicaciones oficiales.\n",
    "#Nuestro criterio no sera el tiempo sino el habito, si es nulo, ocasional o rutinario.\n",
    "#['netusoft','nwsptot','rdtot','tvtot']\n",
    "#Factorizamos nuestra variable target ()\n",
    "mapping = {'Aleatoriamente': 0, 'Ocasionalmente': 1, 'Rutinariamente': 2}\n",
    "def categorizar_frecuencia(valor):\n",
    "    if 0 <= valor <= 2:\n",
    "        return 'Aleatoriamente'\n",
    "    elif valor == 3:\n",
    "        return 'Ocasionalmente'\n",
    "    elif valor >= 4:\n",
    "        return 'Rutinariamente'\n",
    "    else:\n",
    "        return None \n",
    "variables = ['netusoft', 'nwsptot', 'rdtot', 'tvtot']\n",
    "for variable in variables:\n",
    "    # Categoriza la frecuencia\n",
    "    dfc[variable] = dfc[variable].apply(categorizar_frecuencia)\n",
    "    \n",
    "    # Mapea a valores numéricos (factorización)\n",
    "    dfc[f'{variable}_fc'] = dfc[variable].map(mapping)\n",
    "    \n",
    "    # Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "    ruta_archivo_json = f\"C:/GitHubRepos/ProyectoFinal/data/processed/{variable}_factors.json\"\n",
    "    with open(ruta_archivo_json, 'w') as f:\n",
    "        json.dump(mapping, f)\n",
    "dfc = dfc.drop(variables, axis=1)\n",
    "print(dfc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combinaremos las variables de media priorizando el valor de netusfot(0 si es rutinario se asignara al encuestado el valor factorizado\n",
    "# de 'Alternativo', si esta var tiene un valor diferente a rutinario se asignara el valor Oficial si alguna de las otras variables\n",
    "# tienee l valor de rutinario, si no se asignara el valor A ofimed = aleatorio.)\n",
    "def combinar_medios(row):\n",
    "    \"\"\"Combina las variables de uso de medios según el criterio especificado.\"\"\"\n",
    "\n",
    "    if row['netusoft_fc'] == 2:  # netusoft rutinariamente (2)\n",
    "        return 'Alternativo'\n",
    "    elif row['nwsptot_fc'] == 2 or row['rdtot_fc'] == 2 or row['tvtot_fc'] == 2:\n",
    "        return 'Oficial'\n",
    "    elif row['nwsptot_fc'] == 1 or row['rdtot_fc'] == 1 or row['tvtot_fc'] == 1:\n",
    "        return 'Ocasional'\n",
    "    else:\n",
    "        return 'Aleatorio'\n",
    "\n",
    "dfc['OfiMed'] = dfc.apply(combinar_medios, axis=1)\n",
    "\n",
    "# Mapea 'OfiMed' a valores numéricos (factorización)\n",
    "mapping_ofimed = {'Aleatorio': 0, 'Ocasional': 1, 'Alternativo': 3, 'Oficial': 4}  # Ajusta el mapeo\n",
    "dfc['OfiMed_fc'] = dfc['OfiMed'].map(mapping_ofimed)\n",
    "\n",
    "print(dfc[['netusoft_fc', 'nwsptot_fc', 'rdtot_fc', 'tvtot_fc', 'OfiMed', 'OfiMed_fc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfc['netusoft_fc'],dfc['nwsptot_fc'],dfc['rdtot_fc'],dfc['tvtot_fc'],dfc['OfiMed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "mappingtrust = {'Desconfiado': 0, 'Esceptico': 1, 'Selectivo': 2, 'Confiado': 3, 'Oficialista': 4}\n",
    "\n",
    "def categorizar_confianza(valor):\n",
    "    if 0 <= valor <= 1:\n",
    "        return 'Desconfiado'\n",
    "    elif 2 <= valor <= 4:\n",
    "        return 'Esceptico'\n",
    "    elif 5 <= valor <= 6:\n",
    "        return 'Selectivo'\n",
    "    elif 7 <= valor <= 8:\n",
    "        return 'Confiado'\n",
    "    else:  # La condición correcta para valores 9 y 10\n",
    "        return 'Oficialista'\n",
    "\n",
    "dfc['ppltrst'] = dfc['ppltrst'].apply(categorizar_confianza)\n",
    "dfc['ppltrst_fc'] = dfc['ppltrst'].map(mappingtrust)  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/ppltrst_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappingtrust, f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['ppltrst', 'ppltrst_fc']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfc['ppltrst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a factorizar todas las escalas de 0 a 10 del bloque 2 político.\n",
    "#BLOQUE 2: df_rd['actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv', 'dclmig', 'dclwlfr', \n",
    "# 'dmcntov', 'euftf', 'gincdif', 'ginveco', 'lawobey', 'lrscale', 'polintr','stfeco','stfedu',\n",
    "# 'stfgov', 'stfhlth', 'stflife', 'trstplc', 'trstplt', 'trstun', 'trstsci', 'imbgeco']\n",
    "#var 'dmcntov'\n",
    "mappingdemo = {'Dictadura': 0, 'PocoDem': 1, 'Democratico': 2}\n",
    "\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'Dictadura'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'PocoDem'\n",
    "    else: \n",
    "        return 'Democratico'\n",
    "\n",
    "dfc['dmcntov'] = dfc['dmcntov'].apply(demo)\n",
    "dfc['dmcntov_fc'] = dfc['dmcntov'].map(mappingdemo)  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/dmcntov_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappingdemo, f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['dmcntov', 'dmcntov_fc']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOQUE 2: df_rd['actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv', 'dclmig', 'dclwlfr', \n",
    "# 'dmcntov', 'euftf', 'gincdif', 'ginveco', 'lawobey', 'lrscale', 'polintr','stfeco','stfedu',\n",
    "# 'stfgov', 'stfhlth', 'stflife', 'trstplc', 'trstplt', 'trstun', 'trstsci', 'imbgeco']\n",
    "\n",
    "#var 'euftf' La unificacion de la zona euro ha ido demasiado lejos o necesita ir mas lejos.\n",
    "mappingeu = {'DemasiadoLejos': 0, 'EstaBien': 1, 'NecesitaIrMasLejos': 2}\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'DemasiadoLejos'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'EstaBien'\n",
    "    else: \n",
    "        return 'NecesitaIrMasLejos'\n",
    "\n",
    "dfc['euftf'] = dfc['euftf'].apply(demo)\n",
    "dfc['euftf_fc'] = dfc['euftf'].map(mappingeu)  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/euftf_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappingeu, f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['euftf', 'euftf_fc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOQUE 2: df_rd['actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv', 'dclmig', 'dclwlfr', \n",
    "# 'dmcntov', 'euftf', 'gincdif', 'ginveco', 'lawobey', 'lrscale', 'polintr','stfeco','stfedu',\n",
    "# 'stfgov', 'stfhlth', 'stflife', 'trstplc', 'trstplt', 'trstun', 'trstsci', 'imbgeco']\n",
    "\n",
    "#var 'stfeco' satisfaccion con la situacion economica del pais\n",
    "mappingeco = {'Insatisfecho': 0, 'SePuedeMejorar': 1, 'Satisfecho': 2}\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'Insatisfecho'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'SePuedeMejorar'\n",
    "    else: \n",
    "        return 'Satisfecho'\n",
    "\n",
    "dfc['stfeco'] = dfc['stfeco'].apply(demo)\n",
    "dfc['stfeco_fc'] = dfc['stfeco'].map(mappingeco)  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/stfeco_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappingeco, f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['stfeco', 'stfeco_fc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOQUE 2: df_rd['actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv', 'dclmig', 'dclwlfr', \n",
    "# 'dmcntov', 'euftf', 'gincdif', 'ginveco', 'lawobey', 'lrscale', 'polintr','stfeco','stfedu',\n",
    "# 'stfgov', 'stfhlth', 'stflife', 'trstplc', 'trstplt', 'trstun', 'trstsci', 'imbgeco']\n",
    "#Var  'stfedu'\n",
    "mappingedu = {'Insatisfecho': 0, 'SePuedeMejorar': 1, 'Satisfecho': 2}\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'Insatisfecho'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'SePuedeMejorar'\n",
    "    else: \n",
    "        return 'Satisfecho'\n",
    "\n",
    "dfc['stfedu'] = dfc['stfedu'].apply(demo)\n",
    "dfc['stfedu_fc'] = dfc['stfedu'].map(mappingedu)  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/stfedu_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappingedu, f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['stfedu', 'stfedu_fc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOQUE 2: df_rd['actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv', 'dclmig', 'dclwlfr', \n",
    "# 'dmcntov', 'euftf', 'gincdif', 'ginveco', 'lawobey', 'lrscale', 'polintr','stfeco','stfedu',\n",
    "# 'stfgov', 'stfhlth', 'stflife', 'trstplc', 'trstplt', 'trstun', 'trstsci', 'imbgeco']\n",
    "#Var 'stfgov' Cuan satisfecho con el govierno nacional.\n",
    "mappinggov = {'Insatisfecho': 0, 'SePuedeMejorar': 1, 'Satisfecho': 2}\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'Insatisfecho'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'SePuedeMejorar'\n",
    "    else: \n",
    "        return 'Satisfecho'\n",
    "\n",
    "dfc['stfgov'] = dfc['stfgov'].apply(demo)\n",
    "dfc['stfgov_fc'] = dfc['stfgov'].map(mappinggov)  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/stfgov_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappinggov, f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['stfgov', 'stfgov_fc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOQUE 2: df_rd['actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv', 'dclmig', 'dclwlfr', \n",
    "# 'dmcntov', 'euftf', 'gincdif', 'ginveco', 'lawobey', 'lrscale', 'polintr','stfeco','stfedu',\n",
    "# 'stfgov', 'stfhlth', 'stflife', 'trstplc', 'trstplt', 'trstun', 'trstsci', 'imbgeco']\n",
    "# Var 'stfhlth' cuan satisfecho con la sanidad publica.\n",
    "mappinghth = {'Insatisfecho': 0, 'SePuedeMejorar': 1, 'Satisfecho': 2}\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'Insatisfecho'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'SePuedeMejorar'\n",
    "    else: \n",
    "        return 'Satisfecho'\n",
    "\n",
    "dfc['stfhlth'] = dfc['stfhlth'].apply(demo)\n",
    "dfc['stfhlth_fc'] = dfc['stfhlth'].map(mappinghth)  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/stfhlth_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappinghth, f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['stfhlth', 'stfhlth_fc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOQUE 2: df_rd['actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv', 'dclmig', 'dclwlfr', \n",
    "# 'dmcntov', 'euftf', 'gincdif', 'ginveco', 'lawobey', 'lrscale', 'polintr','stfeco','stfedu',\n",
    "# 'stfgov', 'stfhlth', 'stflife', 'trstplc', 'trstplt', 'trstun', 'trstsci', 'imbgeco']\n",
    "#Var stflife - Cuan satisfecho de la calidad de vida.\n",
    "mappinglife = {'Insatisfecho': 0, 'SePuedeMejorar': 1, 'Satisfecho': 2}\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'Insatisfecho'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'SePuedeMejorar'\n",
    "    else: \n",
    "        return 'Satisfecho'\n",
    "\n",
    "dfc['stflife'] = dfc['stflife'].apply(demo)\n",
    "dfc['stflife_fc'] = dfc['stflife'].map(mappinglife)  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/stflife_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappinglife, f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['stflife', 'stflife_fc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOQUE 2: df_rd['actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv', 'dclmig', 'dclwlfr', \n",
    "# 'dmcntov', 'euftf', 'gincdif', 'ginveco', 'lawobey', 'lrscale', 'polintr','stfeco','stfedu',\n",
    "# 'stfgov', 'stfhlth', 'stflife', 'trstplc', 'trstplt', 'trstun', 'trstsci', 'imbgeco']\n",
    "#Var trstplc - Cuanta confianza en la policia\n",
    "mappingplc = {'Desconfiado': 0, 'Esceptico': 1, 'Confiado': 2}\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'Desconfiado'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'Esceptico'\n",
    "    else: \n",
    "        return 'Confiado'\n",
    "\n",
    "dfc['trstplc'] = dfc['trstplc'].apply(demo)\n",
    "dfc['trstplc_fc'] = dfc['trstplc'].map(mappingplc)  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/trstplc_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappingplc, f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['trstplc', 'trstplc_fc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOQUE 2: df_rd['actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv', 'dclmig', 'dclwlfr', \n",
    "# 'dmcntov', 'euftf', 'gincdif', 'ginveco', 'lawobey', 'lrscale', 'polintr','stfeco','stfedu',\n",
    "# 'stfgov', 'stfhlth', 'stflife', 'trstplc', 'trstplt', 'trstun', 'trstsci', 'imbgeco']\n",
    "#Var trstplt  - Cuanta confianza en los politicos\n",
    "mappingplc = {'Desconfiado': 0, 'Esceptico': 1, 'Confiado': 2}\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'Desconfiado'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'Esceptico'\n",
    "    else: \n",
    "        return 'Confiado'\n",
    "\n",
    "dfc['trstplt'] = dfc['trstplt'].apply(demo)\n",
    "dfc['trstplt_fc'] = dfc['trstplt'].map(mappingplc)  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/trstplt_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappingplc, f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['trstplt', 'trstplt_fc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOQUE 2: df_rd['actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv', 'dclmig', 'dclwlfr', \n",
    "# 'dmcntov', 'euftf', 'gincdif', 'ginveco', 'lawobey', 'lrscale', 'polintr','stfeco','stfedu',\n",
    "# 'stfgov', 'stfhlth', 'stflife', 'trstplc', 'trstplt', 'trstun', 'trstsci', 'imbgeco']\n",
    "#Var trstun  - Cuanta confianza en laa organizacion Naciones Unidas.\n",
    "mappingplc = {'Desconfiado': 0, 'Esceptico': 1, 'Confiado': 2}\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'Desconfiado'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'Esceptico'\n",
    "    else: \n",
    "        return 'Confiado'\n",
    "\n",
    "dfc['trstun'] = dfc['trstun'].apply(demo)\n",
    "dfc['trstun_fc'] = dfc['trstun'].map(mappingplc)  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/trstun_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappingplc, f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['trstun', 'trstun_fc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOQUE 2: df_rd['actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv', 'dclmig', 'dclwlfr', \n",
    "# 'dmcntov', 'euftf', 'gincdif', 'ginveco', 'lawobey', 'lrscale', 'polintr','stfeco','stfedu',\n",
    "# 'stfgov', 'stfhlth', 'stflife', 'trstplc', 'trstplt', 'trstun', 'trstsci', 'imbgeco']\n",
    "#Var trstsci  - Cuanta confianza en el lobby cientifico\n",
    "mappingplc = {'Desconfiado': 0, 'Esceptico': 1, 'Confiado': 2}\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'Desconfiado'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'Esceptico'\n",
    "    else: \n",
    "        return 'Confiado'\n",
    "\n",
    "dfc['trstsci'] = dfc['trstsci'].apply(demo)\n",
    "dfc['trstsci_fc'] = dfc['trstsci'].map(mappingplc)  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/trstsci_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappingplc, f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['trstsci', 'trstsci_fc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOQUE 2: df_rd['actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv', 'dclmig', 'dclwlfr', \n",
    "# 'dmcntov', 'euftf', 'gincdif', 'ginveco', 'lawobey', 'lrscale', 'polintr','stfeco','stfedu',\n",
    "# 'stfgov', 'stfhlth', 'stflife', 'trstplc', 'trstplt', 'trstun', 'trstsci', 'imbgeco']\n",
    "#Var imbgeco - Cuanto consideras que la inmigracion es buena para la economia.\n",
    "mappinginm = {'InfluyeNegativamente': 0, 'Indiferente': 1, 'InfluyePositivamente': 2}\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'InfluyeNegativamente'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'Indiferente'\n",
    "    else: \n",
    "        return 'InfluyePositivamente'\n",
    "\n",
    "dfc['imbgeco'] = dfc['imbgeco'].apply(demo)\n",
    "dfc['imbgeco_fc'] = dfc['imbgeco'].map(mappinginm )  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/imbgeco_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappinginm , f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['imbgeco', 'imbgeco_fc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos las columnas no factorizadas del bloque 2.\n",
    "dfc = dfc.drop(columns=['imbgeco', 'cntry', 'trstsci', 'trstun', 'trstplt', 'trstplc', \n",
    "                        'stflife', 'stfhlth', 'stfgov', 'stfedu', 'stfeco', 'euftf', 'dmcntov'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Factorizaremos las escalas de 0 a 10 del bloque 3.\n",
    "# #Bloque 3: df_rd['aesfdrk','atchctr','atcherp','blgetmg','brncntr','dscrgnd','dscrrlg',\n",
    "# 'facntr','happy','health','inprdsc','mocntr','rlgdgr','ccnthum','sclmeet','vteumbgb']\n",
    "#Var atchctr - Cuan patriota eres? [country]\n",
    "mappingemo = {'Desapegado': 0, 'ConCiertoApego': 1, 'MuyApegado': 2}\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'Desapegado'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'ConCiertoApego'\n",
    "    else: \n",
    "        return 'MuyApegado'\n",
    "\n",
    "dfc['atchctr'] = dfc['atchctr'].apply(demo)\n",
    "dfc['atchctr_fc'] = dfc['atchctr'].map(mappingemo)  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/atchctr_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappingemo , f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['atchctr', 'atchctr_fc']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Var atcherp - How emotionally attached to Europe\n",
    "mappingemo = {'Desapegado': 0, 'ConCiertoApego': 1, 'MuyApegado': 2}\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'Desapegado'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'ConCiertoApego'\n",
    "    else: \n",
    "        return 'MuyApegado'\n",
    "\n",
    "dfc['atcherp'] = dfc['atcherp'].apply(demo)\n",
    "dfc['atcherp_fc'] = dfc['atcherp'].map(mappingemo)  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/atcherp_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappingemo , f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['atcherp', 'atcherp_fc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Var happy - How happy are you\n",
    "mappinghppy = {'infeliz': 0, 'FelizPorMomentos': 1, 'Feliz': 2}\n",
    "\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'infeliz'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'FelizPorMomentos'\n",
    "    else: \n",
    "        return 'Feliz'\n",
    "\n",
    "dfc['happy'] = dfc['happy'].apply(demo)\n",
    "dfc['happy_fc'] = dfc['happy'].map(mappinghppy )  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/happy_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappinghppy, f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['happy', 'happy_fc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Var rlgdgr\n",
    "mappingrel = {'Ateo': 0, 'ConCiertaCreencia': 1, 'Agnostico/Practicante': 2}\n",
    "\n",
    "def demo(valor):\n",
    "    if 0 <= valor <= 3:\n",
    "        return 'Ateo'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'ConCiertaCreencia'\n",
    "    else: \n",
    "        return 'Agnostico/Practicante'\n",
    "\n",
    "dfc['rlgdgr'] = dfc['rlgdgr'].apply(demo)\n",
    "dfc['rlgdgr_fc'] = dfc['rlgdgr'].map(mappingrel)  # Usamos mappingtrust aquí\n",
    "\n",
    "# Guarda el mapeo en un archivo JSON (opcional, pero útil para referencia futura)\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/rlgdgr_fc.json\"\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mappingrel, f)  # Guardamos mappingtrust en el archivo JSON\n",
    "\n",
    "print(dfc[['rlgdgr', 'rlgdgr_fc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = dfc.drop(columns=['atchctr','atcherp','happy','rlgdgr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfc['ideo'], dfc['orig'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.to_csv('C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\dfc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bloque 4 no tiene escalas 0 10. \n",
    "Realizaremos un tanteo con el nuevo dataset, teniendo la posibilidad\n",
    "de escoger la variable target en escala 0 10 o factorizada y comparamos scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos todas las variables del notebook para optimizar recursos\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np, random\n",
    "from sklearn.model_selection import train_test_split\n",
    "random.seed(42)\n",
    "dtest = pd.read_csv('C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\dfc.csv')\n",
    "\n",
    "for col in dtest.select_dtypes(include=['float64']).columns:\n",
    "    \n",
    "    try:\n",
    "        dtest[col] = dtest[col].astype('int64')\n",
    "    except:\n",
    "        dtest[col] = pd.to_numeric(dtest[col], downcast='integer')\n",
    "\n",
    "print(dtest.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import os\n",
    "\n",
    "# 1. Carga de datos\n",
    "dtest = pd.read_csv(\"C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\dfc.csv\")\n",
    "\n",
    "# 2. División train/test inicial (CON MUESTREO ESTRATIFICADO GENERAL)\n",
    "X = dtest.drop(['ideo_fc', 'lrscale', 'cntryfcz', 'origfcz', 'cluster'], axis=1)\n",
    "y = dtest['ideo_fc']\n",
    "\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X, y, test_size=0.3,  random_state=42\n",
    ")#stratify=y,\n",
    "\n",
    "#Muestreo estratificado general\n",
    "sampled_data_general = []\n",
    "n_samples_general = min(4000, len(X_train_full))\n",
    "X_train_general = X_train_full.sample(n=n_samples_general, random_state=42)\n",
    "y_train_general = y_train_full.loc[X_train_general.index]\n",
    "\n",
    "X_train_general, X_test_general, y_train_general, y_test_general = train_test_split(\n",
    "    X_train_general, y_train_general, test_size=0.3, random_state=42\n",
    ")#stratify=y_train_general\n",
    "\n",
    "# 3. Guardado en CSV (opcional)\n",
    "X_train_general.to_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/X_train_general.csv', index=False)\n",
    "X_test_general.to_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/X_test_general.csv', index=False)\n",
    "pd.DataFrame(y_train_general).to_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/y_train_general.csv', index=False)\n",
    "pd.DataFrame(y_test_general).to_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/y_test_general.csv', index=False)\n",
    "\n",
    "\n",
    "# 4. Iteración por clusters (CON MUESTREO ESTRATIFICADO POR CLUSTER)\n",
    "for cluster in dtest['cluster'].unique():\n",
    "    # Filtrar *X_train_full e y_train_full* por cluster (USANDO X_train_full)\n",
    "    X_train_cluster = X_train_full[dtest.loc[X_train_full.index, 'cluster'] == cluster]\n",
    "    y_train_cluster = y_train_full[dtest.loc[X_train_full.index, 'cluster'] == cluster]\n",
    "\n",
    "    # Filtrar *X_test_full e y_test_full* por cluster (USANDO X_test_full)\n",
    "    X_test_cluster = X_test_full[dtest.loc[X_test_full.index, 'cluster'] == cluster]\n",
    "    y_test_cluster = y_test_full[dtest.loc[X_test_full.index, 'cluster'] == cluster]\n",
    "\n",
    "    #Muestreo estratificado por cluster\n",
    "    sampled_data_cluster_train = []\n",
    "    n_samples_cluster_train = min(700, len(X_train_cluster)) #375 train + 125 test = 500\n",
    "    X_train_cluster_sampled = X_train_cluster.sample(n=n_samples_cluster_train, random_state=42)\n",
    "    y_train_cluster_sampled = y_train_cluster.loc[X_train_cluster_sampled.index]\n",
    "\n",
    "    sampled_data_cluster_test = []\n",
    "    n_samples_cluster_test = min(300, len(X_test_cluster)) #375 train + 125 test = 500\n",
    "    X_test_cluster_sampled = X_test_cluster.sample(n=n_samples_cluster_test, random_state=42)\n",
    "    y_test_cluster_sampled = y_test_cluster.loc[X_test_cluster_sampled.index]\n",
    "\n",
    "    # 5. Submuestreo y Sobremuestreo (SMOTE después de RUS)\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = rus.fit_resample(X_train_cluster_sampled, y_train_cluster_sampled)\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_resampled, y_train_resampled)  # ¡CORREGIDO!\n",
    "\n",
    "    # 7. Creación de DataFrames para cada cluster\n",
    "    train_cluster_df = pd.DataFrame(X_train_resampled, columns=X_train_cluster.columns)\n",
    "    train_cluster_df['ideo_fc'] = y_train_resampled\n",
    "\n",
    "    test_cluster_df = pd.DataFrame(X_test_cluster_sampled, columns = X_test_cluster.columns) #Añadido los nombres de las columnas\n",
    "    test_cluster_df['ideo_fc'] = y_test_cluster_sampled\n",
    "\n",
    "    # 8. Guardado en CSVs para cada cluster (SOBREESCRIBIR)\n",
    "    train_cluster_df.to_csv(\n",
    "        f'C:/GitHubRepos/ProyectoFinal/data/models/Split/train52C{cluster}.csv', index=False\n",
    "    )\n",
    "    test_cluster_df.to_csv(\n",
    "        f'C:/GitHubRepos/ProyectoFinal/data/models/Split/test52C{cluster}.csv', index=False\n",
    "    )\n",
    "\n",
    "    print(f\"Archivos CSV para cluster {cluster} creados/sobreescritos exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from xgboost import XGBClassifier  # Import XGBoost\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import joblib\n",
    "import json\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder  # Import LabelEncoder\n",
    "\n",
    "# --- Función principal ---\n",
    "def entrenar_y_evaluar(X_train, y_train, X_test, y_test, target_variable, model_type, ruta_modelo, ruta_configuracion, ruta_dataset):\n",
    "    resultados = {}\n",
    "    mejor_modelo = None\n",
    "    mejor_k = None\n",
    "    mejores_resultados = None\n",
    "    mejor_auc = -1  # Inicializamos con el peor valor posible\n",
    "    mejor_target_variable = None\n",
    "\n",
    "    for k in range(9, 39):  # Range of k values to test\n",
    "        # Selección de características\n",
    "        selector = SelectKBest(score_func=chi2, k=k)\n",
    "        X_train_new = selector.fit_transform(X_train, y_train)\n",
    "        X_test_new = selector.transform(X_test)\n",
    "        mejores_features = X_train.columns[selector.get_support()]\n",
    "\n",
    "        # --- Tratamiento de clases minoritarias con SMOTE ---\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_new, y_train)\n",
    "\n",
    "        # Optimización de hiperparámetros con RandomizedSearchCV (XGBoost)\n",
    "\n",
    "        # 1. Label Encode your target variable *before* using it in XGBoost\n",
    "        le = LabelEncoder()\n",
    "        y_train_encoded = le.fit_transform(y_train)  # Fit and transform on training data\n",
    "        y_test_encoded = le.transform(y_test)  # Transform the test using the fitted encoder\n",
    "\n",
    "        model = XGBClassifier(random_state=42, eval_metric='logloss')  # XGBoost - use_label_encoder removed\n",
    "        param_grid = {\n",
    "            'n_estimators': range(50, 301, 50),  # Wider range\n",
    "            'max_depth': range(3, 11),  # Range for max_depth\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate\n",
    "            'gamma': [0, 0.1, 0.2],  # Gamma for regularization\n",
    "            'min_child_weight': range(1, 6),  # Min child weight\n",
    "            'subsample': [0.6, 0.8, 1.0],  # Subsample ratio\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0],  # Colsample by tree\n",
    "            'reg_alpha': [0, 0.1, 1],  # L1 regularization\n",
    "            'reg_lambda': [0, 0.1, 1]  # L2 regularization\n",
    "        }\n",
    "        scoring = 'f1_weighted'\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        X_train_opt = X_train_resampled\n",
    "        y_train_opt = y_train_resampled\n",
    "\n",
    "        random_search = RandomizedSearchCV(model, param_grid, n_iter=50, scoring=scoring, cv=cv, n_jobs=-1, random_state=42)  # Increased n_iter\n",
    "        random_search.fit(X_train_opt, y_train_opt)\n",
    "        best_model = random_search.best_estimator_\n",
    "\n",
    "        # Evaluación y almacenamiento de resultados\n",
    "        y_pred = best_model.predict(X_test_new)  # Predictions will be numerical\n",
    "        y_pred_decoded = le.inverse_transform(y_pred)  # Decode to original labels\n",
    "        accuracy = accuracy_score(y_test, y_pred_decoded)  # Compare with original labels\n",
    "        precision = precision_score(y_test, y_pred_decoded, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred_decoded, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred_decoded, average='weighted')\n",
    "        auc = roc_auc_score(y_test, best_model.predict_proba(X_test_new), multi_class='ovr')\n",
    "\n",
    "        resultados[(target_variable, k)] = {\n",
    "            'mejores_features': mejores_features.tolist(),\n",
    "            'best_params': random_search.best_params_,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc\n",
    "        }\n",
    "\n",
    "        print(f\"Resultados para {target_variable}, k={k}:\")\n",
    "        print(f\"Mejores características: {mejores_features}\")\n",
    "        print(f\"Mejores hiperparámetros: {random_search.best_params_}\")\n",
    "        print(f\"Exactitud: {accuracy}\")\n",
    "        print(f\"Precisión: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1-score: {f1}\")\n",
    "        print(f\"AUC: {auc}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        if auc > mejor_auc:\n",
    "            mejor_auc = auc\n",
    "            mejor_modelo = best_model\n",
    "            mejor_k = k\n",
    "            mejores_resultados = resultados[(target_variable, k)]\n",
    "            mejor_target_variable = target_variable\n",
    "\n",
    "    # Entrenar el modelo final con la mejor configuración (usando TODO el conjunto de entrenamiento)\n",
    "    selector_final = SelectKBest(score_func=chi2, k=mejor_k)\n",
    "    X_final = selector_final.fit_transform(X_train, y_train)  # Use X_train and y_train\n",
    "    rf_final = XGBClassifier(random_state=42, eval_metric='logloss', **mejores_resultados['best_params'])\n",
    "    rf_final.fit(X_final, le.transform(y_train))  # Encode y_train here as well\n",
    "\n",
    "    # Guardar el modelo (el MEJOR modelo)\n",
    "    joblib.dump(rf_final, ruta_modelo)\n",
    "\n",
    "    # Guardar la configuración (la MEJOR configuración)\n",
    "    with open(ruta_configuracion, 'w') as f:\n",
    "        json.dump({'k': mejor_k, 'resultados': mejores_resultados, 'target_variable': mejor_target_variable, 'best_params': mejores_resultados['best_params']}, f)\n",
    "\n",
    "    # Guardar el dataset final\n",
    "    df_final = pd.DataFrame(X_final, columns=X_train.columns[selector_final.get_support()])\n",
    "    df_final[mejor_target_variable] = y_train.loc[df_final.index] #Use y_train\n",
    "    df_final.to_csv(ruta_dataset, index=False)\n",
    "\n",
    "    print(f\"\\nMejor modelo guardado en: {ruta_modelo}\")\n",
    "    print(f\"Configuración guardada en: {ruta_configuracion}\")\n",
    "    print(f\"Dataset guardado en: {ruta_dataset}\")\n",
    "    \n",
    "pretrain = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data\\models\\Split/test52C2.csv',index_col=False)\n",
    "pretest = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/train52C2.csv',index_col=False)\n",
    "X_train = pretrain.drop('ideo_fc', axis=1)\n",
    "y_train = pretrain['ideo_fc'].squeeze()\n",
    "X_test = pretest.drop('ideo_fc', axis=1)\n",
    "y_test = pretest['ideo_fc'].squeeze()\n",
    "'''\n",
    "X_train = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/X_train_general.csv')\n",
    "y_train = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/y_train_general.csv')\n",
    "X_test = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/X_test_general.csv')\n",
    "y_test = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/y_test_general.csv')\n",
    "y_train = y_train.squeeze()  # Convierte y_train a Serie\n",
    "y_test = y_test.squeeze() #Convierte y_test a serie'''\n",
    "ruta_modelo= 'C:/GitHubRepos/ProyectoFinal/data/models/BRF_ideo_fc.pkl'\n",
    "ruta_configuracion= 'C:/GitHubRepos/ProyectoFinal/data/models/BRFsetup_ideo_fc.json'\n",
    "ruta_dataset = 'C:/GitHubRepos/ProyectoFinal/data/models/Datasets/dfRF_ideo_fc.csv'\n",
    "target_variable = 'ideo_fc'\n",
    "model_type = RandomForestClassifier\n",
    "entrenar_y_evaluar(X_train, y_train, X_test, y_test, target_variable, model_type, ruta_modelo, ruta_configuracion, ruta_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss\n",
    "import joblib\n",
    "import json\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- Función principal para Regresión Logística ---\n",
    "def entrenar_y_evaluar_regresion_logistica(X_train, y_train, X_test, y_test, target_variable, ruta_modelo, ruta_configuracion, ruta_dataset):\n",
    "    resultados = {}\n",
    "    mejor_modelo = None\n",
    "    mejor_k = None\n",
    "    mejores_resultados = None\n",
    "    mejor_auc = -1\n",
    "\n",
    "    for k in range(9, 39):  # Rango de valores de k a probar\n",
    "        # Selección de características\n",
    "        selector = SelectKBest(score_func=chi2, k=k) #o f_classif si hay negativos\n",
    "        X_train_new = selector.fit_transform(X_train, y_train)\n",
    "        X_test_new = selector.transform(X_test)\n",
    "        mejores_features = X_train.columns[selector.get_support()]\n",
    "\n",
    "        # --- Tratamiento de clases minoritarias con SMOTE ---\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_new, y_train)\n",
    "\n",
    "        # Escalado de características (¡MUY IMPORTANTE para Regresión Logística!)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_resampled)  # Ajustar y transformar en entrenamiento\n",
    "        X_test_scaled = scaler.transform(X_test_new)  # Transformar en prueba (usando el scaler ajustado en entrenamiento)\n",
    "\n",
    "\n",
    "        # Optimización de hiperparámetros con RandomizedSearchCV\n",
    "        model = LogisticRegression(random_state=42, solver='liblinear', max_iter=10000)  # solver 'liblinear' para penalización L1 y L2\n",
    "        param_grid = {\n",
    "            'C': np.logspace(-4, 4, 20),  # Valores de C para regularización\n",
    "            'penalty': ['l1', 'l2'],  # Penalización L1 o L2\n",
    "        }\n",
    "        scoring = 'f1_weighted' #Métrica a optimizar\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        random_search = RandomizedSearchCV(model, param_grid, n_iter=20, scoring=scoring, cv=cv, n_jobs=-1, random_state=42)\n",
    "        random_search.fit(X_train_scaled, y_train_resampled)\n",
    "        best_model = random_search.best_estimator_\n",
    "\n",
    "        # Evaluación y almacenamiento de resultados\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        try: #Manejo de error en caso de que la variable objetivo tenga solo una clase\n",
    "            auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled), multi_class='ovr')\n",
    "        except ValueError:\n",
    "            auc = 0\n",
    "\n",
    "        resultados[(target_variable, k)] = {\n",
    "            'mejores_features': mejores_features.tolist(),\n",
    "            'best_params': random_search.best_params_,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc\n",
    "        }\n",
    "\n",
    "        print(f\"Resultados para {target_variable}, k={k}:\")\n",
    "        print(f\"Mejores características: {mejores_features}\")\n",
    "        print(f\"Mejores hiperparámetros: {random_search.best_params_}\")\n",
    "        print(f\"Exactitud: {accuracy}\")\n",
    "        print(f\"Precisión: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1-score: {f1}\")\n",
    "        print(f\"AUC: {auc}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        if auc > mejor_auc:\n",
    "            mejor_auc = auc\n",
    "            mejor_modelo = best_model\n",
    "            mejor_k = k\n",
    "            mejores_resultados = resultados[(target_variable, k)]\n",
    "            mejor_target_variable = target_variable\n",
    "\n",
    "    # Entrenar el modelo final con la mejor configuración (usando TODO el conjunto de entrenamiento)\n",
    "    selector_final = SelectKBest(score_func=chi2, k=mejor_k) #o f_classif si hay negativos\n",
    "    X_final = selector_final.fit_transform(X_train, y_train)\n",
    "    scaler_final = StandardScaler()\n",
    "    X_final_scaled = scaler_final.fit_transform(X_final)\n",
    "    lr_final = LogisticRegression(solver='liblinear', max_iter=10000, **mejores_resultados['best_params'])\n",
    "    lr_final.fit(X_final_scaled, y_train)\n",
    "\n",
    "    # Guardar el modelo (el MEJOR modelo)\n",
    "    joblib.dump(lr_final, ruta_modelo)\n",
    "\n",
    "    # Guardar la configuración (la MEJOR configuración)\n",
    "    with open(ruta_configuracion, 'w') as f:\n",
    "        json.dump({'k': mejor_k, 'resultados': mejores_resultados, 'target_variable': mejor_target_variable, 'best_params': mejores_resultados['best_params']}, f)\n",
    "\n",
    "    # Guardar el dataset final\n",
    "    df_final = pd.DataFrame(X_final, columns=X_train.columns[selector_final.get_support()])\n",
    "    df_final[mejor_target_variable] = y_train.loc[df_final.index] #Use y_train\n",
    "    df_final.to_csv(ruta_dataset, index=False)\n",
    "\n",
    "    print(f\"\\nMejor modelo guardado en: {ruta_modelo}\")\n",
    "    print(f\"Configuración guardada en: {ruta_configuracion}\")\n",
    "    print(f\"Dataset guardado en: {ruta_dataset}\")\n",
    "\n",
    "# Ejemplo de uso (reemplaza con tus rutas y datos)\n",
    "pretrain = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data\\models\\Split/test52C2.csv',index_col=False)\n",
    "pretest = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/train52C2.csv',index_col=False)\n",
    "X_train = pretrain.drop('ideo_fc', axis=1)\n",
    "y_train = pretrain['ideo_fc'].squeeze()\n",
    "X_test = pretest.drop('ideo_fc', axis=1)\n",
    "y_test = pretest['ideo_fc'].squeeze()\n",
    "\n",
    "ruta_modelo = 'C:/GitHubRepos/ProyectoFinal/data/models/LR_ideo_fc.pkl'\n",
    "ruta_configuracion = 'C:/GitHubRepos/ProyectoFinal/data/models/LRsetup_ideo_fc.json'\n",
    "ruta_dataset = 'C:/GitHubRepos/ProyectoFinal/data/models/Datasets/dfLR_ideo_fc.csv'\n",
    "target_variable = 'ideo_fc'\n",
    "\n",
    "entrenar_y_evaluar_regresion_logistica(X_train, y_train, X_test, y_test, target_variable, ruta_modelo, ruta_configuracion, ruta_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB  # Modelo Bayesiano Gaussiano\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss\n",
    "import joblib\n",
    "import json\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- Función principal para Naive Bayes Gaussiano ---\n",
    "def entrenar_y_evaluar_naive_bayes(X_train, y_train, X_test, y_test, target_variable, ruta_modelo, ruta_configuracion, ruta_dataset):\n",
    "    resultados = {}\n",
    "    mejor_modelo = None\n",
    "    mejor_k = None\n",
    "    mejores_resultados = None\n",
    "    mejor_auc = -1\n",
    "\n",
    "    for k in range(9, 39):  # Rango de valores de k a probar\n",
    "        # Selección de características\n",
    "        selector = SelectKBest(score_func=chi2, k=k)  # o f_classif si hay negativos\n",
    "        X_train_new = selector.fit_transform(X_train, y_train)\n",
    "        X_test_new = selector.transform(X_test)\n",
    "        mejores_features = X_train.columns[selector.get_support()]\n",
    "\n",
    "        # --- Tratamiento de clases minoritarias con SMOTE ---\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_new, y_train)\n",
    "\n",
    "        # Escalado de características (Opcional para Naive Bayes, pero a veces ayuda)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "        X_test_scaled = scaler.transform(X_test_new)\n",
    "\n",
    "        # Entrenamiento del modelo Naive Bayes Gaussiano\n",
    "        model = GaussianNB()  # No hay hiperparámetros para ajustar en este caso\n",
    "        model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "        # Evaluación y almacenamiento de resultados\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, model.predict_proba(X_test_scaled), multi_class='ovr')\n",
    "        except ValueError:\n",
    "            auc = 0\n",
    "\n",
    "        resultados[(target_variable, k)] = {\n",
    "            'mejores_features': mejores_features.tolist(),\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc\n",
    "        }\n",
    "\n",
    "        print(f\"Resultados para {target_variable}, k={k}:\")\n",
    "        print(f\"Mejores características: {mejores_features}\")\n",
    "        print(f\"Exactitud: {accuracy}\")\n",
    "        print(f\"Precisión: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1-score: {f1}\")\n",
    "        print(f\"AUC: {auc}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        if auc > mejor_auc:\n",
    "            mejor_auc = auc\n",
    "            mejor_modelo = model\n",
    "            mejor_k = k\n",
    "            mejores_resultados = resultados[(target_variable, k)]\n",
    "            mejor_target_variable = target_variable\n",
    "\n",
    "    # Entrenar el modelo final con la mejor configuración (usando TODO el conjunto de entrenamiento)\n",
    "    selector_final = SelectKBest(score_func=chi2, k=mejor_k)  # o f_classif si hay negativos\n",
    "    X_final = selector_final.fit_transform(X_train, y_train)\n",
    "    scaler_final = StandardScaler()\n",
    "    X_final_scaled = scaler_final.fit_transform(X_final)\n",
    "    nb_final = GaussianNB()\n",
    "    nb_final.fit(X_final_scaled, y_train)\n",
    "\n",
    "    # Guardar el modelo (el MEJOR modelo)\n",
    "    joblib.dump(nb_final, ruta_modelo)\n",
    "\n",
    "    # Guardar la configuración (la MEJOR configuración)\n",
    "    with open(ruta_configuracion, 'w') as f:\n",
    "        json.dump({'k': mejor_k, 'resultados': mejores_resultados, 'target_variable': mejor_target_variable}, f)\n",
    "\n",
    "    # Guardar el dataset final\n",
    "    df_final = pd.DataFrame(X_final, columns=X_train.columns[selector_final.get_support()])\n",
    "    df_final[mejor_target_variable] = y_train.loc[df_final.index]  # Use y_train\n",
    "    df_final.to_csv(ruta_dataset, index=False)\n",
    "\n",
    "    print(f\"\\nMejor modelo guardado en: {ruta_modelo}\")\n",
    "    print(f\"Configuración guardada en: {ruta_configuracion}\")\n",
    "    print(f\"Dataset guardado en: {ruta_dataset}\")\n",
    "\n",
    "\n",
    "# Ejemplo de uso (reemplaza con tus rutas y datos)\n",
    "pretrain = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data\\models\\Split/test52C2.csv', index_col=False)\n",
    "pretest = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/train52C2.csv', index_col=False)\n",
    "X_train = pretrain.drop('ideo_fc', axis=1)\n",
    "y_train = pretrain['ideo_fc'].squeeze()\n",
    "X_test = pretest.drop('ideo_fc', axis=1)\n",
    "y_test = pretest['ideo_fc'].squeeze()\n",
    "\n",
    "ruta_modelo = 'C:/GitHubRepos/ProyectoFinal/data/models/NB_ideo_fc.pkl'\n",
    "ruta_configuracion = 'C:/GitHubRepos/ProyectoFinal/data/models/NBsetup_ideo_fc.json'\n",
    "ruta_dataset = 'C:/GitHubRepos/ProyectoFinal/data/models/Datasets/dfNB_ideo_fc.csv'\n",
    "target_variable = 'ideo_fc'\n",
    "\n",
    "entrenar_y_evaluar_naive_bayes(X_train, y_train, X_test, y_test, target_variable, ruta_modelo, ruta_configuracion, ruta_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import joblib\n",
    "import json\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# --- Función principal para Red Neuronal ---\n",
    "def entrenar_y_evaluar_red_neuronal(X_train, y_train, X_test, y_test, target_variable, ruta_modelo, ruta_configuracion, ruta_dataset):\n",
    "    resultados = {}\n",
    "    mejor_modelo = None\n",
    "    mejor_k = None\n",
    "    mejores_resultados = None\n",
    "    mejor_auc = -1\n",
    "\n",
    "    for k in range(9, 39):  # Rango de valores de k a probar\n",
    "        # Selección de características\n",
    "        selector = SelectKBest(score_func=chi2, k=k)\n",
    "        X_train_new = selector.fit_transform(X_train, y_train)\n",
    "        X_test_new = selector.transform(X_test)\n",
    "        mejores_features = X_train.columns[selector.get_support()]\n",
    "\n",
    "        # --- Tratamiento de clases minoritarias con SMOTE ---\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_new, y_train)\n",
    "\n",
    "        # Escalado de características\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "        X_test_scaled = scaler.transform(X_test_new)\n",
    "\n",
    "        # --- Diseño y compilación del modelo de red neuronal ---\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "            layers.Dropout(0.2),  # Capa de dropout para regularización\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(len(np.unique(y_train)), activation='softmax')  # Capa de salida con softmax\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',  # o 'categorical_crossentropy' si y_train está en one-hot encoding\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Entrenamiento del modelo\n",
    "        model.fit(X_train_scaled, y_train_resampled, epochs=50, batch_size=32, verbose=0)  # Ajusta epochs y batch_size\n",
    "\n",
    "        # Evaluación y almacenamiento de resultados\n",
    "        y_pred = np.argmax(model.predict(X_test_scaled), axis=1)  # Obtener las clases predichas\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, model.predict(X_test_scaled), multi_class='ovr', average='weighted')\n",
    "        except ValueError:\n",
    "            auc = 0\n",
    "\n",
    "        resultados[(target_variable, k)] = {\n",
    "            'mejores_features': mejores_features.tolist(),\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc\n",
    "        }\n",
    "\n",
    "        print(f\"Resultados para {target_variable}, k={k}:\")\n",
    "        print(f\"Mejores características: {mejores_features}\")\n",
    "        print(f\"Exactitud: {accuracy}\")\n",
    "        print(f\"Precisión: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1-score: {f1}\")\n",
    "        print(f\"AUC: {auc}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        if auc > mejor_auc:\n",
    "            mejor_auc = auc\n",
    "            mejor_modelo = model\n",
    "            mejor_k = k\n",
    "            mejores_resultados = resultados[(target_variable, k)]\n",
    "            mejor_target_variable = target_variable\n",
    "            mejor_scaler = scaler\n",
    "            mejor_selector = selector\n",
    "\n",
    "    # Entrenar el modelo final con la mejor configuración (usando TODO el conjunto de entrenamiento)\n",
    "    X_final = mejor_selector.transform(X_train)\n",
    "    X_final_scaled = mejor_scaler.transform(X_final)\n",
    "    nn_final = keras.models.clone_model(mejor_modelo)  # Crea un nuevo modelo con la misma arquitectura\n",
    "    nn_final.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    nn_final.fit(X_final_scaled, y_train, epochs=50, batch_size=32, verbose=0)  # Reentrena con todos los datos\n",
    "\n",
    "    # Guardar el modelo (el MEJOR modelo)\n",
    "    nn_final.save(ruta_modelo)  # Guarda en formato h5\n",
    "\n",
    "    # Guardar la configuración (la MEJOR configuración)\n",
    "    with open(ruta_configuracion, 'w') as f:\n",
    "        json.dump({'k': mejor_k, 'resultados': mejores_resultados, 'target_variable': mejor_target_variable}, f)\n",
    "\n",
    "    # Guardar el dataset final\n",
    "    df_final = pd.DataFrame(X_final, columns=X_train.columns[mejor_selector.get_support()])\n",
    "    df_final[mejor_target_variable] = y_train.loc[df_final.index]  # Use y_train\n",
    "    df_final.to_csv(ruta_dataset, index=False)\n",
    "\n",
    "    print(f\"\\nMejor modelo guardado en: {ruta_modelo}\")\n",
    "    print(f\"Configuración guardada en: {ruta_configuracion}\")\n",
    "    print(f\"Dataset guardado en: {ruta_dataset}\")\n",
    "\n",
    "# Ejemplo de uso (reemplaza con tus rutas y datos)\n",
    "# ... (tu código de carga de datos y definición de rutas)\n",
    "\n",
    "entrenar_y_evaluar_red_neuronal(X_train, y_train, X_test, y_test, target_variable, ruta_modelo, ruta_configuracion, ruta_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".FPJBGPCSobremesa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
