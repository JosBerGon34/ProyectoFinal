{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\G'\n",
      "C:\\Users\\Josue\\AppData\\Local\\Temp\\ipykernel_12712\\2811866231.py:4: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  conn = sqlite3.connect('C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\Dataset9k.db')\n"
     ]
    }
   ],
   "source": [
    "#Cargamos el df base del servidor sql.\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\Dataset9k.db')\n",
    "query = \"SELECT * FROM control\"\n",
    "df_encuestas = pd.read_sql_query(query, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pplfair  pplhlp  ppltrst  rdtot  tvtot  euftf  gincdif  lrscale  polintr  \\\n",
      "0        5       5        5      0      0      5        3        5        1   \n",
      "1        4       4        4      0      1      1        3       10        2   \n",
      "2        4       6        5      0      5      5        1        5        3   \n",
      "3        6       5        6      1      5      5        2        5        3   \n",
      "4        5       2        2      2      5      5        2        4        3   \n",
      "\n",
      "   stfeco  ...  ipadvnta  ipcrtiva  ipeqopta  iplylfra  ipmodsta  ipudrsta  \\\n",
      "0       5  ...         5         2         2         2         2         2   \n",
      "1       3  ...         5         2         2         2         2         2   \n",
      "2       2  ...         4         2         1         1         2         2   \n",
      "3       0  ...         4         4         3         2         2         2   \n",
      "4       5  ...         2         2         3         3         4         2   \n",
      "\n",
      "   P_fc  GroupCntry               ideo  ideo_fc  \n",
      "0     5           0  Demócrata(Centro)        2  \n",
      "1    20           0     ExtremaDerecha        4  \n",
      "2     5           0  Demócrata(Centro)        2  \n",
      "3     8           0  Demócrata(Centro)        2  \n",
      "4     8           0     ExtremaDerecha        4  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "#Factorizamos nuestra variable target ()\n",
    "import json\n",
    "import os\n",
    "mapping = {'SocialComunista': 0, 'Socialista': 1, 'Demócrata(Centro)': 2, 'Conservador': 3, 'ExtremaDerecha': 4}\n",
    "def agrupar_lrscale(valor):\n",
    "    if 0 <= valor <= 2:\n",
    "        return 'SocialComunista'\n",
    "    elif 3 == valor <= 4:\n",
    "        return 'Socialista'\n",
    "    elif 5 == valor <= 6:\n",
    "        return 'Demócrata(Centro)'\n",
    "    elif 7 == valor <= 8:\n",
    "        return 'Conservador'\n",
    "    else:\n",
    "        return 'ExtremaDerecha'\n",
    "\n",
    "\n",
    "df_encuestas['ideo'] = df_encuestas['lrscale'].apply(agrupar_lrscale)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_encuestas['ideo_fc'] = df_encuestas['ideo'].map(mapping)\n",
    "\n",
    "\n",
    "ruta_archivo_json = \"C:/GitHubRepos/ProyectoFinal/data/processed/ideo_factors.json\"\n",
    "\n",
    "\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mapping, f)\n",
    "df_encuestas = df_encuestas.apply(lambda col: col.astype(int) if col.dtype == 'float64' else col)\n",
    "print(df_encuestas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_encuestas['ideo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encuestas.to_csv('C:/GitHubRepos/ProyectoFinal/data/processed/presplit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\G'\n",
      "C:\\Users\\Josue\\AppData\\Local\\Temp\\ipykernel_12712\\905134761.py:6: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  dtest = pd.read_csv('C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\presplit.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pplfair       int64\n",
      "pplhlp        int64\n",
      "ppltrst       int64\n",
      "rdtot         int64\n",
      "tvtot         int64\n",
      "euftf         int64\n",
      "gincdif       int64\n",
      "lrscale       int64\n",
      "polintr       int64\n",
      "stfeco        int64\n",
      "stfedu        int64\n",
      "stfgov        int64\n",
      "stfhlth       int64\n",
      "stflife       int64\n",
      "trstplc       int64\n",
      "trstplt       int64\n",
      "trstun        int64\n",
      "imbgeco       int64\n",
      "aesfdrk       int64\n",
      "atcherp       int64\n",
      "happy         int64\n",
      "health        int64\n",
      "inprdsc       int64\n",
      "rlgdgr        int64\n",
      "sclmeet       int64\n",
      "ccnthum       int64\n",
      "impenva       int64\n",
      "impfreea      int64\n",
      "impfuna       int64\n",
      "impricha      int64\n",
      "impsafea      int64\n",
      "imptrada      int64\n",
      "ipadvnta      int64\n",
      "ipcrtiva      int64\n",
      "ipeqopta      int64\n",
      "iplylfra      int64\n",
      "ipmodsta      int64\n",
      "ipudrsta      int64\n",
      "P_fc          int64\n",
      "GroupCntry    int64\n",
      "ideo_fc       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np, random\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Pasamos el filtro de tipo de datos y obligamos a que esten en enteros\n",
    "random.seed(42)\n",
    "dtest = pd.read_csv('C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\presplit.csv')\n",
    "\n",
    "for col in dtest.select_dtypes(include=['float64']).columns:\n",
    "    \n",
    "    try:\n",
    "        dtest[col] = dtest[col].astype('int64')\n",
    "    except:\n",
    "        dtest[col] = pd.to_numeric(dtest[col], downcast='integer')\n",
    "\n",
    "print(dtest.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCION SPLIT TRAIN TEST PRINCIPAL.\n",
    "dtest = pd.read_csv(\"C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\presplit.csv\")\n",
    "iX = dtest.drop(['ideo_fc', 'lrscale'], axis=1)\n",
    "iy = dtest['ideo_fc']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import os\n",
    "\n",
    "# 1. Carga de datos\n",
    "dtest = pd.read_csv(\"C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\presplit.csv\")\n",
    "\n",
    "# 2. División train/test inicial (CON MUESTREO ESTRATIFICADO GENERAL)\n",
    "X = dtest.drop(['ideo_fc', 'lrscale', 'cntryfcz', 'origfcz', 'cluster'], axis=1)\n",
    "y = dtest['ideo_fc']\n",
    "\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X, y, test_size=0.3,  random_state=42\n",
    ")#stratify=y,\n",
    "\n",
    "#Muestreo estratificado general\n",
    "sampled_data_general = []\n",
    "n_samples_general = min(4000, len(X_train_full))\n",
    "X_train_general = X_train_full.sample(n=n_samples_general, random_state=42)\n",
    "y_train_general = y_train_full.loc[X_train_general.index]\n",
    "\n",
    "X_train_general, X_test_general, y_train_general, y_test_general = train_test_split(\n",
    "    X_train_general, y_train_general, test_size=0.3, random_state=42\n",
    ")#stratify=y_train_general\n",
    "\n",
    "# 3. Guardado en CSV (opcional)\n",
    "X_train_general.to_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/X_train_general.csv', index=False)\n",
    "X_test_general.to_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/X_test_general.csv', index=False)\n",
    "pd.DataFrame(y_train_general).to_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/y_train_general.csv', index=False)\n",
    "pd.DataFrame(y_test_general).to_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/y_test_general.csv', index=False)\n",
    "\n",
    "\n",
    "# 4. Iteración por clusters (CON MUESTREO ESTRATIFICADO POR CLUSTER)\n",
    "for cluster in dtest['cluster'].unique():\n",
    "    # Filtrar *X_train_full e y_train_full* por cluster (USANDO X_train_full)\n",
    "    X_train_cluster = X_train_full[dtest.loc[X_train_full.index, 'cluster'] == cluster]\n",
    "    y_train_cluster = y_train_full[dtest.loc[X_train_full.index, 'cluster'] == cluster]\n",
    "\n",
    "    # Filtrar *X_test_full e y_test_full* por cluster (USANDO X_test_full)\n",
    "    X_test_cluster = X_test_full[dtest.loc[X_test_full.index, 'cluster'] == cluster]\n",
    "    y_test_cluster = y_test_full[dtest.loc[X_test_full.index, 'cluster'] == cluster]\n",
    "\n",
    "    #Muestreo estratificado por cluster\n",
    "    sampled_data_cluster_train = []\n",
    "    n_samples_cluster_train = min(700, len(X_train_cluster)) #375 train + 125 test = 500\n",
    "    X_train_cluster_sampled = X_train_cluster.sample(n=n_samples_cluster_train, random_state=42)\n",
    "    y_train_cluster_sampled = y_train_cluster.loc[X_train_cluster_sampled.index]\n",
    "\n",
    "    sampled_data_cluster_test = []\n",
    "    n_samples_cluster_test = min(300, len(X_test_cluster)) #375 train + 125 test = 500\n",
    "    X_test_cluster_sampled = X_test_cluster.sample(n=n_samples_cluster_test, random_state=42)\n",
    "    y_test_cluster_sampled = y_test_cluster.loc[X_test_cluster_sampled.index]\n",
    "\n",
    "    # 5. Submuestreo y Sobremuestreo (SMOTE después de RUS)\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = rus.fit_resample(X_train_cluster_sampled, y_train_cluster_sampled)\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_resampled, y_train_resampled)  # ¡CORREGIDO!\n",
    "\n",
    "    # 7. Creación de DataFrames para cada cluster\n",
    "    train_cluster_df = pd.DataFrame(X_train_resampled, columns=X_train_cluster.columns)\n",
    "    train_cluster_df['ideo_fc'] = y_train_resampled\n",
    "\n",
    "    test_cluster_df = pd.DataFrame(X_test_cluster_sampled, columns = X_test_cluster.columns) #Añadido los nombres de las columnas\n",
    "    test_cluster_df['ideo_fc'] = y_test_cluster_sampled\n",
    "\n",
    "    # 8. Guardado en CSVs para cada cluster (SOBREESCRIBIR)\n",
    "    train_cluster_df.to_csv(\n",
    "        f'C:/GitHubRepos/ProyectoFinal/data/models/Split/train52C{cluster}.csv', index=False\n",
    "    )\n",
    "    test_cluster_df.to_csv(\n",
    "        f'C:/GitHubRepos/ProyectoFinal/data/models/Split/test52C{cluster}.csv', index=False\n",
    "    )\n",
    "\n",
    "    print(f\"Archivos CSV para cluster {cluster} creados/sobreescritos exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from xgboost import XGBClassifier  # Import XGBoost\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import joblib\n",
    "import json\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder  # Import LabelEncoder\n",
    "\n",
    "# --- Función principal ---\n",
    "def entrenar_y_evaluar(X_train, y_train, X_test, y_test, target_variable, model_type, ruta_modelo, ruta_configuracion, ruta_dataset):\n",
    "    resultados = {}\n",
    "    mejor_modelo = None\n",
    "    mejor_k = None\n",
    "    mejores_resultados = None\n",
    "    mejor_auc = -1  # Inicializamos con el peor valor posible\n",
    "    mejor_target_variable = None\n",
    "\n",
    "    for k in range(9, 39):  # Range of k values to test\n",
    "        # Selección de características\n",
    "        selector = SelectKBest(score_func=chi2, k=k)\n",
    "        X_train_new = selector.fit_transform(X_train, y_train)\n",
    "        X_test_new = selector.transform(X_test)\n",
    "        mejores_features = X_train.columns[selector.get_support()]\n",
    "\n",
    "        # --- Tratamiento de clases minoritarias con SMOTE ---\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_new, y_train)\n",
    "\n",
    "        # Optimización de hiperparámetros con RandomizedSearchCV (XGBoost)\n",
    "\n",
    "        # 1. Label Encode your target variable *before* using it in XGBoost\n",
    "        le = LabelEncoder()\n",
    "        y_train_encoded = le.fit_transform(y_train)  # Fit and transform on training data\n",
    "        y_test_encoded = le.transform(y_test)  # Transform the test using the fitted encoder\n",
    "\n",
    "        model = XGBClassifier(random_state=42, eval_metric='logloss')  # XGBoost - use_label_encoder removed\n",
    "        param_grid = {\n",
    "            'n_estimators': range(50, 301, 50),  # Wider range\n",
    "            'max_depth': range(3, 11),  # Range for max_depth\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate\n",
    "            'gamma': [0, 0.1, 0.2],  # Gamma for regularization\n",
    "            'min_child_weight': range(1, 6),  # Min child weight\n",
    "            'subsample': [0.6, 0.8, 1.0],  # Subsample ratio\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0],  # Colsample by tree\n",
    "            'reg_alpha': [0, 0.1, 1],  # L1 regularization\n",
    "            'reg_lambda': [0, 0.1, 1]  # L2 regularization\n",
    "        }\n",
    "        scoring = 'f1_weighted'\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        X_train_opt = X_train_resampled\n",
    "        y_train_opt = y_train_resampled\n",
    "\n",
    "        random_search = RandomizedSearchCV(model, param_grid, n_iter=50, scoring=scoring, cv=cv, n_jobs=-1, random_state=42)  # Increased n_iter\n",
    "        random_search.fit(X_train_opt, y_train_opt)\n",
    "        best_model = random_search.best_estimator_\n",
    "\n",
    "        # Evaluación y almacenamiento de resultados\n",
    "        y_pred = best_model.predict(X_test_new)  # Predictions will be numerical\n",
    "        y_pred_decoded = le.inverse_transform(y_pred)  # Decode to original labels\n",
    "        accuracy = accuracy_score(y_test, y_pred_decoded)  # Compare with original labels\n",
    "        precision = precision_score(y_test, y_pred_decoded, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred_decoded, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred_decoded, average='weighted')\n",
    "        auc = roc_auc_score(y_test, best_model.predict_proba(X_test_new), multi_class='ovr')\n",
    "\n",
    "        resultados[(target_variable, k)] = {\n",
    "            'mejores_features': mejores_features.tolist(),\n",
    "            'best_params': random_search.best_params_,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc\n",
    "        }\n",
    "\n",
    "        print(f\"Resultados para {target_variable}, k={k}:\")\n",
    "        print(f\"Mejores características: {mejores_features}\")\n",
    "        print(f\"Mejores hiperparámetros: {random_search.best_params_}\")\n",
    "        print(f\"Exactitud: {accuracy}\")\n",
    "        print(f\"Precisión: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1-score: {f1}\")\n",
    "        print(f\"AUC: {auc}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        if auc > mejor_auc:\n",
    "            mejor_auc = auc\n",
    "            mejor_modelo = best_model\n",
    "            mejor_k = k\n",
    "            mejores_resultados = resultados[(target_variable, k)]\n",
    "            mejor_target_variable = target_variable\n",
    "\n",
    "    # Entrenar el modelo final con la mejor configuración (usando TODO el conjunto de entrenamiento)\n",
    "    selector_final = SelectKBest(score_func=chi2, k=mejor_k)\n",
    "    X_final = selector_final.fit_transform(X_train, y_train)  # Use X_train and y_train\n",
    "    rf_final = XGBClassifier(random_state=42, eval_metric='logloss', **mejores_resultados['best_params'])\n",
    "    rf_final.fit(X_final, le.transform(y_train))  # Encode y_train here as well\n",
    "\n",
    "    # Guardar el modelo (el MEJOR modelo)\n",
    "    joblib.dump(rf_final, ruta_modelo)\n",
    "\n",
    "    # Guardar la configuración (la MEJOR configuración)\n",
    "    with open(ruta_configuracion, 'w') as f:\n",
    "        json.dump({'k': mejor_k, 'resultados': mejores_resultados, 'target_variable': mejor_target_variable, 'best_params': mejores_resultados['best_params']}, f)\n",
    "\n",
    "    # Guardar el dataset final\n",
    "    df_final = pd.DataFrame(X_final, columns=X_train.columns[selector_final.get_support()])\n",
    "    df_final[mejor_target_variable] = y_train.loc[df_final.index] #Use y_train\n",
    "    df_final.to_csv(ruta_dataset, index=False)\n",
    "\n",
    "    print(f\"\\nMejor modelo guardado en: {ruta_modelo}\")\n",
    "    print(f\"Configuración guardada en: {ruta_configuracion}\")\n",
    "    print(f\"Dataset guardado en: {ruta_dataset}\")\n",
    "    \n",
    "pretrain = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data\\models\\Split/test52C2.csv',index_col=False)\n",
    "pretest = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/train52C2.csv',index_col=False)\n",
    "X_train = pretrain.drop('ideo_fc', axis=1)\n",
    "y_train = pretrain['ideo_fc'].squeeze()\n",
    "X_test = pretest.drop('ideo_fc', axis=1)\n",
    "y_test = pretest['ideo_fc'].squeeze()\n",
    "'''\n",
    "X_train = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/X_train_general.csv')\n",
    "y_train = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/y_train_general.csv')\n",
    "X_test = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/X_test_general.csv')\n",
    "y_test = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/y_test_general.csv')\n",
    "y_train = y_train.squeeze()  # Convierte y_train a Serie\n",
    "y_test = y_test.squeeze() #Convierte y_test a serie'''\n",
    "ruta_modelo= 'C:/GitHubRepos/ProyectoFinal/data/models/BRF_ideo_fc.pkl'\n",
    "ruta_configuracion= 'C:/GitHubRepos/ProyectoFinal/data/models/BRFsetup_ideo_fc.json'\n",
    "ruta_dataset = 'C:/GitHubRepos/ProyectoFinal/data/models/Datasets/dfRF_ideo_fc.csv'\n",
    "target_variable = 'ideo_fc'\n",
    "model_type = RandomForestClassifier\n",
    "entrenar_y_evaluar(X_train, y_train, X_test, y_test, target_variable, model_type, ruta_modelo, ruta_configuracion, ruta_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss\n",
    "import joblib\n",
    "import json\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- Función principal para Regresión Logística ---\n",
    "def entrenar_y_evaluar_regresion_logistica(X_train, y_train, X_test, y_test, target_variable, ruta_modelo, ruta_configuracion, ruta_dataset):\n",
    "    resultados = {}\n",
    "    mejor_modelo = None\n",
    "    mejor_k = None\n",
    "    mejores_resultados = None\n",
    "    mejor_auc = -1\n",
    "\n",
    "    for k in range(9, 39):  # Rango de valores de k a probar\n",
    "        # Selección de características\n",
    "        selector = SelectKBest(score_func=chi2, k=k) #o f_classif si hay negativos\n",
    "        X_train_new = selector.fit_transform(X_train, y_train)\n",
    "        X_test_new = selector.transform(X_test)\n",
    "        mejores_features = X_train.columns[selector.get_support()]\n",
    "\n",
    "        # --- Tratamiento de clases minoritarias con SMOTE ---\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_new, y_train)\n",
    "\n",
    "        # Escalado de características (¡MUY IMPORTANTE para Regresión Logística!)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_resampled)  # Ajustar y transformar en entrenamiento\n",
    "        X_test_scaled = scaler.transform(X_test_new)  # Transformar en prueba (usando el scaler ajustado en entrenamiento)\n",
    "\n",
    "\n",
    "        # Optimización de hiperparámetros con RandomizedSearchCV\n",
    "        model = LogisticRegression(random_state=42, solver='liblinear', max_iter=10000)  # solver 'liblinear' para penalización L1 y L2\n",
    "        param_grid = {\n",
    "            'C': np.logspace(-4, 4, 20),  # Valores de C para regularización\n",
    "            'penalty': ['l1', 'l2'],  # Penalización L1 o L2\n",
    "        }\n",
    "        scoring = 'f1_weighted' #Métrica a optimizar\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        random_search = RandomizedSearchCV(model, param_grid, n_iter=20, scoring=scoring, cv=cv, n_jobs=-1, random_state=42)\n",
    "        random_search.fit(X_train_scaled, y_train_resampled)\n",
    "        best_model = random_search.best_estimator_\n",
    "\n",
    "        # Evaluación y almacenamiento de resultados\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        try: #Manejo de error en caso de que la variable objetivo tenga solo una clase\n",
    "            auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled), multi_class='ovr')\n",
    "        except ValueError:\n",
    "            auc = 0\n",
    "\n",
    "        resultados[(target_variable, k)] = {\n",
    "            'mejores_features': mejores_features.tolist(),\n",
    "            'best_params': random_search.best_params_,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc\n",
    "        }\n",
    "\n",
    "        print(f\"Resultados para {target_variable}, k={k}:\")\n",
    "        print(f\"Mejores características: {mejores_features}\")\n",
    "        print(f\"Mejores hiperparámetros: {random_search.best_params_}\")\n",
    "        print(f\"Exactitud: {accuracy}\")\n",
    "        print(f\"Precisión: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1-score: {f1}\")\n",
    "        print(f\"AUC: {auc}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        if auc > mejor_auc:\n",
    "            mejor_auc = auc\n",
    "            mejor_modelo = best_model\n",
    "            mejor_k = k\n",
    "            mejores_resultados = resultados[(target_variable, k)]\n",
    "            mejor_target_variable = target_variable\n",
    "\n",
    "    # Entrenar el modelo final con la mejor configuración (usando TODO el conjunto de entrenamiento)\n",
    "    selector_final = SelectKBest(score_func=chi2, k=mejor_k) #o f_classif si hay negativos\n",
    "    X_final = selector_final.fit_transform(X_train, y_train)\n",
    "    scaler_final = StandardScaler()\n",
    "    X_final_scaled = scaler_final.fit_transform(X_final)\n",
    "    lr_final = LogisticRegression(solver='liblinear', max_iter=10000, **mejores_resultados['best_params'])\n",
    "    lr_final.fit(X_final_scaled, y_train)\n",
    "\n",
    "    # Guardar el modelo (el MEJOR modelo)\n",
    "    joblib.dump(lr_final, ruta_modelo)\n",
    "\n",
    "    # Guardar la configuración (la MEJOR configuración)\n",
    "    with open(ruta_configuracion, 'w') as f:\n",
    "        json.dump({'k': mejor_k, 'resultados': mejores_resultados, 'target_variable': mejor_target_variable, 'best_params': mejores_resultados['best_params']}, f)\n",
    "\n",
    "    # Guardar el dataset final\n",
    "    df_final = pd.DataFrame(X_final, columns=X_train.columns[selector_final.get_support()])\n",
    "    df_final[mejor_target_variable] = y_train.loc[df_final.index] #Use y_train\n",
    "    df_final.to_csv(ruta_dataset, index=False)\n",
    "\n",
    "    print(f\"\\nMejor modelo guardado en: {ruta_modelo}\")\n",
    "    print(f\"Configuración guardada en: {ruta_configuracion}\")\n",
    "    print(f\"Dataset guardado en: {ruta_dataset}\")\n",
    "\n",
    "# Ejemplo de uso (reemplaza con tus rutas y datos)\n",
    "pretrain = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data\\models\\Split/test52C2.csv',index_col=False)\n",
    "pretest = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/train52C2.csv',index_col=False)\n",
    "X_train = pretrain.drop('ideo_fc', axis=1)\n",
    "y_train = pretrain['ideo_fc'].squeeze()\n",
    "X_test = pretest.drop('ideo_fc', axis=1)\n",
    "y_test = pretest['ideo_fc'].squeeze()\n",
    "\n",
    "ruta_modelo = 'C:/GitHubRepos/ProyectoFinal/data/models/LR_ideo_fc.pkl'\n",
    "ruta_configuracion = 'C:/GitHubRepos/ProyectoFinal/data/models/LRsetup_ideo_fc.json'\n",
    "ruta_dataset = 'C:/GitHubRepos/ProyectoFinal/data/models/Datasets/dfLR_ideo_fc.csv'\n",
    "target_variable = 'ideo_fc'\n",
    "\n",
    "entrenar_y_evaluar_regresion_logistica(X_train, y_train, X_test, y_test, target_variable, ruta_modelo, ruta_configuracion, ruta_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB  # Modelo Bayesiano Gaussiano\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss\n",
    "import joblib\n",
    "import json\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- Función principal para Naive Bayes Gaussiano ---\n",
    "def entrenar_y_evaluar_naive_bayes(X_train, y_train, X_test, y_test, target_variable, ruta_modelo, ruta_configuracion, ruta_dataset):\n",
    "    resultados = {}\n",
    "    mejor_modelo = None\n",
    "    mejor_k = None\n",
    "    mejores_resultados = None\n",
    "    mejor_auc = -1\n",
    "\n",
    "    for k in range(9, 39):  # Rango de valores de k a probar\n",
    "        # Selección de características\n",
    "        selector = SelectKBest(score_func=chi2, k=k)  # o f_classif si hay negativos\n",
    "        X_train_new = selector.fit_transform(X_train, y_train)\n",
    "        X_test_new = selector.transform(X_test)\n",
    "        mejores_features = X_train.columns[selector.get_support()]\n",
    "\n",
    "        # --- Tratamiento de clases minoritarias con SMOTE ---\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_new, y_train)\n",
    "\n",
    "        # Escalado de características (Opcional para Naive Bayes, pero a veces ayuda)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "        X_test_scaled = scaler.transform(X_test_new)\n",
    "\n",
    "        # Entrenamiento del modelo Naive Bayes Gaussiano\n",
    "        model = GaussianNB()  # No hay hiperparámetros para ajustar en este caso\n",
    "        model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "        # Evaluación y almacenamiento de resultados\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, model.predict_proba(X_test_scaled), multi_class='ovr')\n",
    "        except ValueError:\n",
    "            auc = 0\n",
    "\n",
    "        resultados[(target_variable, k)] = {\n",
    "            'mejores_features': mejores_features.tolist(),\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc\n",
    "        }\n",
    "\n",
    "        print(f\"Resultados para {target_variable}, k={k}:\")\n",
    "        print(f\"Mejores características: {mejores_features}\")\n",
    "        print(f\"Exactitud: {accuracy}\")\n",
    "        print(f\"Precisión: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1-score: {f1}\")\n",
    "        print(f\"AUC: {auc}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        if auc > mejor_auc:\n",
    "            mejor_auc = auc\n",
    "            mejor_modelo = model\n",
    "            mejor_k = k\n",
    "            mejores_resultados = resultados[(target_variable, k)]\n",
    "            mejor_target_variable = target_variable\n",
    "\n",
    "    # Entrenar el modelo final con la mejor configuración (usando TODO el conjunto de entrenamiento)\n",
    "    selector_final = SelectKBest(score_func=chi2, k=mejor_k)  # o f_classif si hay negativos\n",
    "    X_final = selector_final.fit_transform(X_train, y_train)\n",
    "    scaler_final = StandardScaler()\n",
    "    X_final_scaled = scaler_final.fit_transform(X_final)\n",
    "    nb_final = GaussianNB()\n",
    "    nb_final.fit(X_final_scaled, y_train)\n",
    "\n",
    "    # Guardar el modelo (el MEJOR modelo)\n",
    "    joblib.dump(nb_final, ruta_modelo)\n",
    "\n",
    "    # Guardar la configuración (la MEJOR configuración)\n",
    "    with open(ruta_configuracion, 'w') as f:\n",
    "        json.dump({'k': mejor_k, 'resultados': mejores_resultados, 'target_variable': mejor_target_variable}, f)\n",
    "\n",
    "    # Guardar el dataset final\n",
    "    df_final = pd.DataFrame(X_final, columns=X_train.columns[selector_final.get_support()])\n",
    "    df_final[mejor_target_variable] = y_train.loc[df_final.index]  # Use y_train\n",
    "    df_final.to_csv(ruta_dataset, index=False)\n",
    "\n",
    "    print(f\"\\nMejor modelo guardado en: {ruta_modelo}\")\n",
    "    print(f\"Configuración guardada en: {ruta_configuracion}\")\n",
    "    print(f\"Dataset guardado en: {ruta_dataset}\")\n",
    "\n",
    "\n",
    "# Ejemplo de uso (reemplaza con tus rutas y datos)\n",
    "pretrain = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data\\models\\Split/test52C2.csv', index_col=False)\n",
    "pretest = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/models/Split/train52C2.csv', index_col=False)\n",
    "X_train = pretrain.drop('ideo_fc', axis=1)\n",
    "y_train = pretrain['ideo_fc'].squeeze()\n",
    "X_test = pretest.drop('ideo_fc', axis=1)\n",
    "y_test = pretest['ideo_fc'].squeeze()\n",
    "\n",
    "ruta_modelo = 'C:/GitHubRepos/ProyectoFinal/data/models/NB_ideo_fc.pkl'\n",
    "ruta_configuracion = 'C:/GitHubRepos/ProyectoFinal/data/models/NBsetup_ideo_fc.json'\n",
    "ruta_dataset = 'C:/GitHubRepos/ProyectoFinal/data/models/Datasets/dfNB_ideo_fc.csv'\n",
    "target_variable = 'ideo_fc'\n",
    "\n",
    "entrenar_y_evaluar_naive_bayes(X_train, y_train, X_test, y_test, target_variable, ruta_modelo, ruta_configuracion, ruta_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import joblib\n",
    "import json\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# --- Función principal para Red Neuronal ---\n",
    "def entrenar_y_evaluar_red_neuronal(X_train, y_train, X_test, y_test, target_variable, ruta_modelo, ruta_configuracion, ruta_dataset):\n",
    "    resultados = {}\n",
    "    mejor_modelo = None\n",
    "    mejor_k = None\n",
    "    mejores_resultados = None\n",
    "    mejor_auc = -1\n",
    "\n",
    "    for k in range(9, 39):  # Rango de valores de k a probar\n",
    "        # Selección de características\n",
    "        selector = SelectKBest(score_func=chi2, k=k)\n",
    "        X_train_new = selector.fit_transform(X_train, y_train)\n",
    "        X_test_new = selector.transform(X_test)\n",
    "        mejores_features = X_train.columns[selector.get_support()]\n",
    "\n",
    "        # --- Tratamiento de clases minoritarias con SMOTE ---\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_new, y_train)\n",
    "\n",
    "        # Escalado de características\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "        X_test_scaled = scaler.transform(X_test_new)\n",
    "\n",
    "        # --- Diseño y compilación del modelo de red neuronal ---\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "            layers.Dropout(0.2),  # Capa de dropout para regularización\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(len(np.unique(y_train)), activation='softmax')  # Capa de salida con softmax\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',  # o 'categorical_crossentropy' si y_train está en one-hot encoding\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Entrenamiento del modelo\n",
    "        model.fit(X_train_scaled, y_train_resampled, epochs=50, batch_size=32, verbose=0)  # Ajusta epochs y batch_size\n",
    "\n",
    "        # Evaluación y almacenamiento de resultados\n",
    "        y_pred = np.argmax(model.predict(X_test_scaled), axis=1)  # Obtener las clases predichas\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, model.predict(X_test_scaled), multi_class='ovr', average='weighted')\n",
    "        except ValueError:\n",
    "            auc = 0\n",
    "\n",
    "        resultados[(target_variable, k)] = {\n",
    "            'mejores_features': mejores_features.tolist(),\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc\n",
    "        }\n",
    "\n",
    "        print(f\"Resultados para {target_variable}, k={k}:\")\n",
    "        print(f\"Mejores características: {mejores_features}\")\n",
    "        print(f\"Exactitud: {accuracy}\")\n",
    "        print(f\"Precisión: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1-score: {f1}\")\n",
    "        print(f\"AUC: {auc}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        if auc > mejor_auc:\n",
    "            mejor_auc = auc\n",
    "            mejor_modelo = model\n",
    "            mejor_k = k\n",
    "            mejores_resultados = resultados[(target_variable, k)]\n",
    "            mejor_target_variable = target_variable\n",
    "            mejor_scaler = scaler\n",
    "            mejor_selector = selector\n",
    "\n",
    "    # Entrenar el modelo final con la mejor configuración (usando TODO el conjunto de entrenamiento)\n",
    "    X_final = mejor_selector.transform(X_train)\n",
    "    X_final_scaled = mejor_scaler.transform(X_final)\n",
    "    nn_final = keras.models.clone_model(mejor_modelo)  # Crea un nuevo modelo con la misma arquitectura\n",
    "    nn_final.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    nn_final.fit(X_final_scaled, y_train, epochs=50, batch_size=32, verbose=0)  # Reentrena con todos los datos\n",
    "\n",
    "    # Guardar el modelo (el MEJOR modelo)\n",
    "    nn_final.save(ruta_modelo)  # Guarda en formato h5\n",
    "\n",
    "    # Guardar la configuración (la MEJOR configuración)\n",
    "    with open(ruta_configuracion, 'w') as f:\n",
    "        json.dump({'k': mejor_k, 'resultados': mejores_resultados, 'target_variable': mejor_target_variable}, f)\n",
    "\n",
    "    # Guardar el dataset final\n",
    "    df_final = pd.DataFrame(X_final, columns=X_train.columns[mejor_selector.get_support()])\n",
    "    df_final[mejor_target_variable] = y_train.loc[df_final.index]  # Use y_train\n",
    "    df_final.to_csv(ruta_dataset, index=False)\n",
    "\n",
    "    print(f\"\\nMejor modelo guardado en: {ruta_modelo}\")\n",
    "    print(f\"Configuración guardada en: {ruta_configuracion}\")\n",
    "    print(f\"Dataset guardado en: {ruta_dataset}\")\n",
    "\n",
    "# Ejemplo de uso (reemplaza con tus rutas y datos)\n",
    "# ... (tu código de carga de datos y definición de rutas)\n",
    "\n",
    "entrenar_y_evaluar_red_neuronal(X_train, y_train, X_test, y_test, target_variable, ruta_modelo, ruta_configuracion, ruta_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".FPJBGPCSobremesa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
