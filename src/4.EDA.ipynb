{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HIPOTESIS EDA:\n",
    "\n",
    "EN ESTE EDA SE QUIERE DEMOSTRAR QUE LAS POLITICAS DE IZQUIERDAS Y GLOBALISTAS, COMO LAS POLITICAS DE LA UNION EUROPEA, QUE SE REFLEJAN EN INDICADORES, QUE SON CONSECUENCIA DE LA EMISION DESENFRENADA DE DEUDA Y EL AUMENTO FISCAL, TIENEN UN EFECTO NEGATIVO EN LOS VALORES ÉTICOS, SALUD TANTO FÍSICA Y MENTAL,\n",
    "ADEMAS DE REPERCUTIR EN EL PRESTIGIO DADO POR LA CIUDADANIA ACTIVA AL LOBBY POLÍTICO Y ORGANISMOS QUE REPRESENTAN AL SISTEMA ,(Cuerpo policial, organismos internacionales, Eurodiputados, etc.)\n",
    "EN ESTE CASO EL ESTUDIO SOLO SE CENTRARA EN LA EUROZONA. Y MAS DETALLADAMENTE EN SUBGRUPOS DE PAISES QUE TIENEN PARECIDOS EN LAS CARACTERISTICAS ECONOMICAS, CULTURALES Y CON CIERTA CERCANIA GEOGRAFICA.\n",
    "\n",
    "\n",
    "EN RESUMEN SE DEMOSTRARA LA RELACION POSITIVA ENTRE LA INFLACION, EL BAJO SUELDO BRUTO MENSUAL MEDIO DEL PAIS, EL PIB PER CAPITA Y CONDUCTAS NEGATIVAS PARA EL SISTEMA DE LA CIUDADANIA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLANTEAMIENTO:\n",
    "Para ello haremos una división de nuestro dataset basandonos en la inflación, sueldo bruto mensual medio y pib per capita, para crear subgrupos que tengan similitudes socioeconomicas, compararemos los datos respecto a la Eurozona en general. Haremos un trabajo de etiquetación ademas de proponer varias variables de diferente campo semantico como targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "#Configuramos el directorio raiz a traves de la variable proyecto raiz de .env\n",
    "\n",
    "load_dotenv()  # Lee el archivo .env\n",
    "ruta_raiz = os.environ.get('RAIZ_PROYECTO')  \n",
    "ruta_carpeta = os.path.join(ruta_raiz, 'data', 'src')\n",
    "os.makedirs(ruta_carpeta, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionaremos 2 paises por grupo:\n",
    "GRUPO A(Economicamente mas rico.):  Alemania, Paises Bajos.\n",
    "GRUPO B(Economicamente similar a la media, tomaremos como referencia España e Italia): UE, España, Italia.\n",
    "Nuestro dataset principal no tiene muestras especificas de la Media de la UE.\n",
    "GRUPO C(Economicamente inferior a la media.): Hungría, Bulgaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) EXPLORACION DE DATOS Y ANÁLISIS:\n",
    "AL SER DATOS CATEGÓRICOS FACTORIZADOS NO TENDREMOS PROBLEMAS DE OUTLIERS. CADA PARTE DEL EDA TENDRA SUBDIVISIONES.\n",
    "ACORDEMONOS QUE NUESTRO DATASET PRINCIPAL, SUS VARIABLES CATEGORICAS ESTAN AGRUPADAS POR 4 GRANDES BLOQUES\n",
    "[Bloque 1(Medios de informacion),Bloque 2(Político),Bloque 3(Bienestar Social), Bloque 4(Valores personales)] CON UN TOTAL DE 57 VARIABLES. CON MUESTRAS DE TODOS LOS PAISES DE LA ZONA EURO, UNAS 100MIL FILAS DE MUESTRAS.\n",
    "ALGO MAS DE 9K MUESTRAS POR PAIS.\n",
    "Ademas hay muestras de paises fuera de la UNion Europea pero estrechamente relacionados con Europa por proximidad geografica y relaciones socioeconomicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Almacenamos dataset principal, y datasets de GRUPO A,B,C en un csv para mejor manejo.(dfeu,dfA,dfB,dfC)\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "rng = random.seed(42)\n",
    "database_path = os.path.abspath(\"C:/GitHubRepos/ProyectoFinal/data/processed/Dataset9k.db\")\n",
    "print(f\"Database path: {database_path}\")\n",
    "conn = create_engine(f'sqlite:///{database_path}')\n",
    "query = \"SELECT * FROM ZonaEuroESS\"\n",
    "dfZES = pd.read_sql(query, conn)\n",
    "conn.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factorizaremos la columna paises y la aañadiremos al dataset\n",
    "import json\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def JsonFCZRename(df, categorical_cols, output_dir, suffix=\"_fc\"):\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col + suffix] = le.fit_transform(df[col]) \n",
    "        mapping_dict = {value: str(index) for value, index in zip(le.classes_, le.transform(le.classes_))}\n",
    "        print(f\"Column: {col}\")\n",
    "        print(mapping_dict)\n",
    "        filename = f\"{col}_factors.json\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(mapping_dict, f, indent=4)\n",
    "\n",
    "    return df\n",
    "categorical_cols = ['cntry']\n",
    "JsonFCZRename(dfZES, categorical_cols, '\\GitHubRepos\\ProyectoFinal\\data\\processed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "def agrupar_lrscale(valor):\n",
    "    if 1 <= valor <= 3:\n",
    "        return 'SocialComunista'\n",
    "    elif 4 <= valor <= 7:\n",
    "        return 'Socialdemócrata'\n",
    "    else:\n",
    "        return 'ExtremaDerecha'\n",
    "\n",
    "\n",
    "dfZES['ideo'] = dfZES['lrscale'].apply(agrupar_lrscale)\n",
    "\n",
    "\n",
    "mapping = {'SocialComunista': 1, 'Socialdemócrata': 2, 'ExtremaDerecha': 3}\n",
    "\n",
    "\n",
    "dfZES['ideo_fcz'] = dfZES['ideo'].map(mapping)\n",
    "\n",
    "\n",
    "ruta_archivo_json = \"C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\ideo_factors.json\"\n",
    "\n",
    "\n",
    "with open(ruta_archivo_json, 'w') as f:\n",
    "    json.dump(mapping, f)\n",
    "\n",
    "print(dfZES.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df's A,B y C\n",
    "import os\n",
    "import pandas as pd\n",
    "ruta_carpeta = 'C:/GitHubRepos/ProyectoFinal/data/raw'\n",
    "os.makedirs(ruta_carpeta, exist_ok=True)\n",
    "dfeuraw = dfZES.sample(n=2700,random_state=rng)\n",
    "ruta_archivo = os.path.join(ruta_carpeta, 'dfeuraw.csv')\n",
    "dfeuraw.to_csv(ruta_archivo, index=False)\n",
    "\n",
    "dffa = dfZES[dfZES['cntry'].isin(['Alemania', 'Holanda'])]\n",
    "dfAraw = dffa.groupby('cntry').sample(n=1350, random_state=rng, replace=True)\n",
    "ruta_archivo = os.path.join(ruta_carpeta, 'dfAraw.csv')\n",
    "dfeuraw.to_csv(ruta_archivo, index=False)\n",
    "\n",
    "dffb = dfZES[dfZES['cntry'].isin(['España', 'Italia'])]\n",
    "dfBraw = dffb.groupby('cntry').sample(n=1350, random_state=rng, replace=True)\n",
    "ruta_archivo = os.path.join(ruta_carpeta, 'dfBraw.csv')\n",
    "dfeuraw.to_csv(ruta_archivo, index=False)\n",
    "\n",
    "dffc = dfZES[dfZES['cntry'].isin(['Hungria', 'Bulgaria'])]\n",
    "dfCraw = dffc.groupby('cntry').sample(n=1350, random_state=rng, replace=True)\n",
    "ruta_archivo = os.path.join(ruta_carpeta, 'dfCraw.csv')\n",
    "dfeuraw.to_csv(ruta_archivo, index=False)\n",
    "\n",
    "del dffa, dffb, dffc, ruta_archivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Primero de todo, vamos a hacer una preseleccion de variables de las que tenemos en el dataset en funcion de nuestros intereses, y el\n",
    "enfoque que queremos dar a la etiquetacion del usuario final a traves de nuestra aplicacion. Y escoger una posible Var target por bloque.\n",
    "Para ello miraremos el despliegue de variables del documento DataResearch.\n",
    "Bloque 1(Media Trust):\n",
    "dfeu['netusoft','nwsptot','ppltrst','rdtot','tvtot'] - var target local Bloque 1(media trust) ['ppltrst'] (Grado de confianza en la opinion de la  gente y medios de comunicacion, 0 no confiar nada y 10 plena confianza)\n",
    "Bloque 2(Político):\n",
    "dfeu['actrolga','dclcrm','dclmig','dmcntov','euftf','gincdif','ideo_fcz','imbgeco','stfgov']                                               - var target global Bloque 2 (Politica) ['ideo_fcz'] (1=socialcomunista, 2=socialdemocrata, 3=extrema derecha)\n",
    "Bloque 3(Bienestar Social):\n",
    "dfeu['aesfdrk','atchctr','atcherp','dscrgnd','happy','rlgdgr','sclmeet','origfcz'] - var target local Bloque 3(bienestar social) ['sclmeet'] - How often socially meet with friends, relatives or colleagues (Cuan antisocial el individuo se ha hecho como resultado, 0 antisocial, 7 social cada dia)\n",
    "Bloque 4(Valores personales):\n",
    "dfeu['iplylfra','impenva','impfreea','impricha','imptrada','ipmodsta','ipudrsta']      - var target local Bloque 4  ['iplylfra'] - Important to be loyal to friends and devote to people close (Como de leal eres con los que consideras amigos y con tu familia, 1 Individuo perfecto en este aspecto, 6 individuo interesado y sin palabra)\n",
    "\n",
    "dfeu['netusoft','nwsptot','ppltrst','rdtot','tvtot','actrolga','dclcrm','dclmig','dmcntov','euftf','gincdif','ideo_fcz','imbgeco','stfgov','aesfdrk','atchctr','atcherp','dscrgnd','happy','rlgdgr','sclmeet','origfcz','iplylfra','impenva','impfreea','impricha','imptrada','ipmodsta','ipudrsta','cntry_fc']\n",
    "\n",
    "Casi hemos reducido a la mitad el numero de variables de 57 a 30 variables.\n",
    "antes de aplicar el feature engineering, gracias a la matriz de correlacion y el selectkbest podremos reducir significativamente el numero de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora especificare la lista de variables con las que a prior voy a trabajar en el EDA antes de el selectKbest, y almacenare los csv en carpeta interim.\n",
    "import os\n",
    "from os import path\n",
    "intcols=['netusoft','nwsptot','ppltrst','rdtot','tvtot','actrolga','dclcrm','dclmig','dmcntov','euftf','gincdif','ideo_fcz','ideo','imbgeco','stfgov','aesfdrk','atchctr','atcherp','dscrgnd','happy','rlgdgr','sclmeet','origfcz','iplylfra','impenva','impfreea','impricha','imptrada','ipmodsta','ipudrsta']\n",
    "dfeu = dfeuraw[intcols]\n",
    "dfA = dfAraw[intcols]\n",
    "dfB = dfBraw[intcols]\n",
    "dfC= dfCraw[intcols]\n",
    "dfeu.to_csv('C:\\GitHubRepos\\ProyectoFinal\\data\\interim\\dfCeu.csv', index=False)\n",
    "dfeu.to_csv('C:\\GitHubRepos\\ProyectoFinal\\data\\interim\\dfA.csv', index=False)\n",
    "dfeu.to_csv('C:\\GitHubRepos\\ProyectoFinal\\data\\interim\\dfB.csv', index=False)\n",
    "dfeu.to_csv('C:\\GitHubRepos\\ProyectoFinal\\data\\interim\\dfC.csv', index=False)\n",
    "del dfeuraw, dfAraw, dfBraw, dfCraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2)Distribucion del dataset por paises. El dataset dfZES de 100k muestras.\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(28, 10)) \n",
    "sns.countplot(\n",
    "    data=dfZES,\n",
    "    x='cntry',\n",
    "    hue='cntry', \n",
    "    order=dfZES['cntry'].value_counts().index, \n",
    "    ax=ax  \n",
    ")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "plt.ylabel('Num Muestras')\n",
    "plt.xlabel('Paises Eurozona')\n",
    "plt.title('Cantidad de muestras por Pais Euro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1) Analisis univariable categoricas factorizadas.\n",
    "#Univariable numericas\n",
    "#Aplicamos una funcion para representar las distribuciones de las variables representadas en un grafico de barras.\n",
    "#2.1.1)Bloque1 (Media Trust) - var target ['ppltrst'] (Grado de confianza en la opinion de la  gente y medios de comunicacion, 0 no confiar nada y 10 plena confianza)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def uninummultidf(df_list, columns, titles, figsize=(20, 20), rotation=90):\n",
    "    num_dfs = len(df_list)\n",
    "    num_cols = len(columns)\n",
    "    num_rows = (num_dfs * num_cols - 1) // 4 + 1\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, 4, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        for j, (df, title) in enumerate(zip(df_list, titles)):\n",
    "            ax = axes[i * num_dfs + j]\n",
    "            sns.countplot(\n",
    "                data=df,\n",
    "                x=col,\n",
    "                hue=col,\n",
    "                order=df[col].value_counts().index,\n",
    "                ax=ax\n",
    "            )\n",
    "            ax.set_title(f'{col} - {title}')\n",
    "            ax.set_ylabel('Muestras bloque 1 (medios de inf)')\n",
    "            ax.set_xlabel('Min(Uso Nulo en tiempo),Max')\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=rotation)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "df_list = [dfeu, dfA, dfB, dfC]\n",
    "columns = ['netusoft','nwsptot','ppltrst','rdtot','tvtot']\n",
    "titles = ['Eu', 'Grupo A(Ale y Hol)', 'Grupo B(Esp e Ita)', 'Grupo C(Hung y Bul)']\n",
    "uninummultidf(df_list, columns, titles)\n",
    "del df_list, columns, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def uninummultidfbox(df_list, columns, titles, figsize=(20, 20), rotation=90):\n",
    "  num_dfs = len(df_list)\n",
    "  num_cols = len(columns)\n",
    "  num_rows = (num_dfs * num_cols - 1) // 4 + 1\n",
    "\n",
    "  fig, axes = plt.subplots(num_rows, 4, figsize=figsize)\n",
    "  axes = axes.flatten()\n",
    "\n",
    "  for i, col in enumerate(columns):\n",
    "    for j, (df, title) in enumerate(zip(df_list, titles)):\n",
    "      ax = axes[i * num_dfs + j]\n",
    "      sns.boxplot(\n",
    "          data=df,\n",
    "          x=col,\n",
    "          showmeans=True, \n",
    "          ax=ax\n",
    "      )\n",
    "      ax.set_title(f'{col} - {title}')\n",
    "      ax.set_ylabel('Valor')\n",
    "      ax.set_xlabel(col)\n",
    "      ax.set_xticklabels(ax.get_xticklabels(), rotation=rotation)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "df_list = [dfeu, dfA, dfB, dfC]\n",
    "columns = ['netusoft', 'nwsptot', 'ppltrst', 'rdtot', 'tvtot']\n",
    "titles = ['Eu', 'Grupo A(Ale y Hol)', 'Grupo B(Esp e Ita)', 'Grupo C(Hung y Bul)']\n",
    "uninummultidfbox(df_list, columns, titles)\n",
    "del df_list, columns, titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones Bloque 1, Eu, Grupo A, B, C.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1.2)Bloque2 (Politica) \n",
    "'''VAR  lrscale - Placement on left right scale target global(Cuan de izquierdas o derechas eres)\n",
    "\n",
    "0\tLeft\n",
    "1\t1- 1,2,3 Izquierdas(Comunista)\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4- 4,5,6,7 mediocentro(Socialdemocrata)\n",
    "5\t5-\n",
    "6\t6-\n",
    "7\t7-\n",
    "8\t8\n",
    "9\t9\n",
    "10\tRight- 8,9,10 Extrema derecha conservador\n",
    "'''\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def uninummultidf(df_list, columns, titles, figsize=(20, 40), rotation=90):\n",
    "    num_dfs = len(df_list)\n",
    "    num_cols = len(columns)\n",
    "    num_rows = (num_dfs * num_cols - 1) // 4 + 1\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, 4, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        for j, (df, title) in enumerate(zip(df_list, titles)):\n",
    "            ax = axes[i * num_dfs + j]\n",
    "            sns.countplot(\n",
    "                data=df,\n",
    "                x=col,\n",
    "                hue=col,\n",
    "                order=df[col].value_counts().index,\n",
    "                ax=ax\n",
    "            )\n",
    "            ax.set_title(f'{col} - {title}')\n",
    "            ax.set_ylabel('Muestras bloque 2 (Política)')\n",
    "            ax.set_xlabel('Min(Poco Interes o Int. internacional),Max')\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=rotation)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "df_list = [dfeu, dfA, dfB, dfC]\n",
    "columns = ['actrolga','dclcrm','dclmig','dmcntov','euftf','gincdif','ideo_fcz','imbgeco','stfgov']\n",
    "titles = ['Eu', 'Grupo A(Ale y Hol)', 'Grupo B(Esp e Ita)', 'Grupo C(Hung y Bul)']\n",
    "uninummultidf(df_list, columns, titles)\n",
    "del df_list, columns, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def uninummultidfbox(df_list, columns, titles, figsize=(20, 35), rotation=90):\n",
    "  num_dfs = len(df_list)\n",
    "  num_cols = len(columns)\n",
    "  num_rows = (num_dfs * num_cols - 1) // 4 + 1\n",
    "\n",
    "  fig, axes = plt.subplots(num_rows, 4, figsize=figsize)\n",
    "  axes = axes.flatten()\n",
    "\n",
    "  for i, col in enumerate(columns):\n",
    "    for j, (df, title) in enumerate(zip(df_list, titles)):\n",
    "      ax = axes[i * num_dfs + j]\n",
    "      sns.boxplot(\n",
    "          data=df,\n",
    "          x=col,\n",
    "          showmeans=True, \n",
    "          ax=ax\n",
    "      )\n",
    "      ax.set_title(f'{col} - {title}')\n",
    "      ax.set_ylabel('Valor')\n",
    "      ax.set_xlabel(col)\n",
    "      ax.set_xticklabels(ax.get_xticklabels(), rotation=rotation)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "df_list = [dfeu, dfA, dfB, dfC]\n",
    "columns = ['actrolga','dclcrm','dclmig','dmcntov','euftf','gincdif','ideo_fcz','imbgeco','stfgov']\n",
    "titles = ['Eu', 'Grupo A(Ale y Hol)', 'Grupo B(Esp e Ita)', 'Grupo C(Hung y Bul)']\n",
    "uninummultidfbox(df_list, columns, titles)\n",
    "del df_list, columns, titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones Bloque 2, Eu, Grupo A, B, C.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1.3)Bloque 3 (Bienestar Social) - var target ['sclmeet'] - How often socially meet with friends, relatives or colleagues (Cuan antisocial el individuo se ha hecho como resultado, 0 antisocial, 7 Necesitado de Socializar)\n",
    "#                                  \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def uninummultidf(df_list, columns, titles, figsize=(20, 35), rotation=90):\n",
    "    num_dfs = len(df_list)\n",
    "    num_cols = len(columns)\n",
    "    num_rows = (num_dfs * num_cols - 1) // 4 + 1\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, 4, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        for j, (df, title) in enumerate(zip(df_list, titles)):\n",
    "            ax = axes[i * num_dfs + j]\n",
    "            sns.countplot(\n",
    "                data=df,\n",
    "                x=col,\n",
    "                hue=col,\n",
    "                order=df[col].value_counts().index,\n",
    "                ax=ax\n",
    "            )\n",
    "            ax.set_title(f'{col} - {title}')\n",
    "            ax.set_ylabel('Muestras bloque 3 (Bienestar Social)')\n",
    "            ax.set_xlabel('Min(Miedo, Vinculado Emoc. o Discriminado),Max')\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=rotation)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "df_list = [dfeu, dfA, dfB, dfC]\n",
    "columns = ['aesfdrk','atchctr','atcherp','dscrgnd','happy','rlgdgr','sclmeet','origfcz']\n",
    "titles = ['Eu', 'Grupo A(Ale y Hol)', 'Grupo B(Esp e Ita)', 'Grupo C(Hung y Bul)']\n",
    "uninummultidf(df_list, columns, titles)\n",
    "del df_list, columns, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uninummultidfbox(df_list, columns, titles, figsize=(20, 35), rotation=90):\n",
    "  num_dfs = len(df_list)\n",
    "  num_cols = len(columns)\n",
    "  num_rows = (num_dfs * num_cols - 1) // 4 + 1\n",
    "\n",
    "  fig, axes = plt.subplots(num_rows, 4, figsize=figsize)\n",
    "  axes = axes.flatten()\n",
    "\n",
    "  for i, col in enumerate(columns):\n",
    "    for j, (df, title) in enumerate(zip(df_list, titles)):\n",
    "      ax = axes[i * num_dfs + j]\n",
    "      sns.boxplot(\n",
    "          data=df,\n",
    "          x=col,\n",
    "          showmeans=True, \n",
    "          ax=ax\n",
    "      )\n",
    "      ax.set_title(f'{col} - {title}')\n",
    "      ax.set_ylabel('Valor')\n",
    "      ax.set_xlabel(col)\n",
    "      ax.set_xticklabels(ax.get_xticklabels(), rotation=rotation)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "df_list = [dfeu, dfA, dfB, dfC]\n",
    "columns = ['aesfdrk','atchctr','atcherp','dscrgnd','happy','rlgdgr','sclmeet','origfcz']\n",
    "titles = ['Eu', 'Grupo A(Ale y Hol)', 'Grupo B(Esp e Ita)', 'Grupo C(Hung y Bul)']\n",
    "uninummultidfbox(df_list, columns, titles)\n",
    "del df_list, columns, titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones Bloque 3 Eu, Grupo A, B y C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1.4)Bloque 4 (Valores personales) - var target ['iplylfra'] - Important to be loyal to friends and devote to people close \n",
    "#                                      (Como de leal eres con los que consideras amigos y con tu familia, 1 Individuo perfecto en este aspecto, 6 individuo interesado y sin palabra)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def uninummultidf(df_list, columns, titles, figsize=(20, 35), rotation=90):\n",
    "    num_dfs = len(df_list)\n",
    "    num_cols = len(columns)\n",
    "    num_rows = (num_dfs * num_cols - 1) // 4 + 1\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, 4, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        for j, (df, title) in enumerate(zip(df_list, titles)):\n",
    "            ax = axes[i * num_dfs + j]\n",
    "            sns.countplot(\n",
    "                data=df,\n",
    "                x=col,\n",
    "                hue=col,\n",
    "                order=df[col].value_counts().index,\n",
    "                ax=ax\n",
    "            )\n",
    "            ax.set_title(f'{col} - {title}')\n",
    "            ax.set_ylabel('Muestras bloque 4 (Valores personales)')\n",
    "            ax.set_xlabel('Min(Ejemplo Persona Perfecta),Max')\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=rotation)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "df_list = [dfeu, dfA, dfB, dfC]\n",
    "columns = ['iplylfra','impenva','impfreea','impricha','imptrada','ipmodsta','ipudrsta']      \n",
    "titles = ['Eu', 'Grupo A(Ale y Hol)', 'Grupo B(Esp e Ita)', 'Grupo C(Hung y Bul)']\n",
    "uninummultidf(df_list, columns, titles)\n",
    "del df_list, columns, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uninummultidfbox(df_list, columns, titles, figsize=(20, 35), rotation=90):\n",
    "  num_dfs = len(df_list)\n",
    "  num_cols = len(columns)\n",
    "  num_rows = (num_dfs * num_cols - 1) // 4 + 1\n",
    "\n",
    "  fig, axes = plt.subplots(num_rows, 4, figsize=figsize)\n",
    "  axes = axes.flatten()\n",
    "\n",
    "  for i, col in enumerate(columns):\n",
    "    for j, (df, title) in enumerate(zip(df_list, titles)):\n",
    "      ax = axes[i * num_dfs + j]\n",
    "      sns.boxplot(\n",
    "          data=df,\n",
    "          x=col,\n",
    "          showmeans=True, \n",
    "          ax=ax\n",
    "      )\n",
    "      ax.set_title(f'{col} - {title}')\n",
    "      ax.set_ylabel('Valor')\n",
    "      ax.set_xlabel(col)\n",
    "      ax.set_xticklabels(ax.get_xticklabels(), rotation=rotation)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "df_list = [dfeu, dfA, dfB, dfC]\n",
    "columns = ['iplylfra','impenva','impfreea','impricha','imptrada','ipmodsta','ipudrsta'] \n",
    "titles = ['Eu', 'Grupo A(Ale y Hol)', 'Grupo B(Esp e Ita)', 'Grupo C(Hung y Bul)']\n",
    "uninummultidfbox(df_list, columns, titles)\n",
    "del df_list, columns, titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones Bloque 4, Eu, Grupo A, B y C:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2) ANÁLISIS MULTIVARIABLE, PARA EL EDA SOLO TRABAJAREMOS CON LA VARIABLE TARGET GLOBAL ['origfcz'] (1-Nacional, 2-Mixto(Nacional pero con almenos un padre extranjero), 3-Extranjero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2) MultiVariable Numerica - var target global ['ideo'] o ['ideo_fcz']\n",
    "#Vamos a visualizar como influye nuestra variable target en la cantidad de muestras y el valor de las muestras. En un diagrama de barras apilado.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def multivarnum(df_list, target_col, numeric_cols, titles, figsize=(20, 6), hue_order=None):\n",
    "    num_dfs = len(df_list)\n",
    "    num_cols = len(numeric_cols)\n",
    "    num_rows = (num_dfs * num_cols - 1) // 4 + 1\n",
    "    fig, axes = plt.subplots(num_rows, 4, figsize=(20, 6 * num_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        for j, (df, title) in enumerate(zip(df_list, titles)):\n",
    "            ax = axes[i * num_dfs + j]\n",
    "            sns.countplot(\n",
    "                data=df,\n",
    "                x=col,\n",
    "                hue=target_col,\n",
    "                order=df[col].value_counts().index,\n",
    "                ax=ax,\n",
    "            \n",
    "            )\n",
    "            ax.set_title(f'{col} - {title}')\n",
    "            ax.set_ylabel('Cantidad de muestras')\n",
    "            ax.set_xlabel('Valores de {}'.format(col))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "df_list = [dfeu, dfA, dfB, dfC]\n",
    "target_col = 'ideo'\n",
    "numeric_cols = ['netusoft','nwsptot','ppltrst','rdtot','tvtot','actrolga','dclcrm','dclmig','dmcntov','euftf','gincdif','imbgeco','stfgov','aesfdrk','atchctr','atcherp','dscrgnd','happy','rlgdgr','sclmeet','iplylfra','impenva','impfreea','impricha','imptrada','ipmodsta','ipudrsta', 'origfcz']\n",
    "\n",
    "titles = ['Eur', 'Grupo A(Ale y Hol)', 'Grupo B(Esp e Ita)', 'Grupo C(Hun y Bul)']\n",
    "multivarnum(df_list, target_col, numeric_cols, titles)\n",
    "del df_list, target_col, numeric_cols, titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2.2) MultiVariable Numerica - var target ['ideo_fcz']\n",
    "#Vamos a visualizar como influye nuestra variable target al desplazamiento de las distribuciones cuartilicas de cada una de las variables observadas, \n",
    "#Por razones ideologicas(izquierda-centro-derecha).(La cantidad de muestras se representara en escala termica)\n",
    "def multivarnumbox(df, target_col, numeric_cols, title, figsize=(10, 6)):\n",
    "  num_cols = len(numeric_cols)\n",
    "  fig, axes = plt.subplots(1, num_cols, figsize=figsize)\n",
    "\n",
    "  for i, col in enumerate(numeric_cols):\n",
    "      ax = axes[i]\n",
    "      sns.boxplot(\n",
    "          x=target_col,\n",
    "          y=col,\n",
    "          showmeans=True,\n",
    "          data=df,\n",
    "          palette='RdBu'\n",
    "      )\n",
    "      ax.set_title(col)\n",
    "      ax.set_xlabel(target_col)\n",
    "      ax.set_ylabel(col)\n",
    "\n",
    "  fig.suptitle(title)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "df = dfeu  \n",
    "target_col = 'ideo'\n",
    "numeric_cols = ['netusoft', 'nwsptot', 'ppltrst', 'rdtot', 'tvtot']\n",
    "title = 'Distribuciones Eu variables respecto [orig]'\n",
    "multivarnumbox(df, target_col, numeric_cols, title)\n",
    "Necesito depurar la función, no me itera correctamente, solo grafica el ultimo subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No es un conjunto de graficos facil de interpetar y sea muy visual. Solo lo aplicare para el dataframe de Europa 'dfeu'. Se debera ampliar en una pagina a parte.\n",
    "#sns.pairplot(dfeu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfeu.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a realizar una matriz de correlacion, para ver las relaciones, positivas y negativas entre variables.\n",
    "#Matriz de correlacion EU.\n",
    "\n",
    "target_var = \"ideo_fcz\"\n",
    "dfeu1 = dfeu.drop(columns=['ideo']) \n",
    "df_heatmap = dfeu1.reindex(columns=[target_var] + list(dfeu1.columns.difference([target_var])))\n",
    "plt.figure(figsize=(30, 30))\n",
    "sns.heatmap(df_heatmap.corr(),annot=True, fmt=\".2f\")\n",
    "plt.show()\n",
    "'''mask=np.triu(df_heatmap.corr())'''\n",
    "del df_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones Analisis multivariable Eu, Grupo A, B y C:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Feature engineering:\n",
    "3.1) Reduccion de variables a 16 + la variable target global = 17.\n",
    "De esas 16, 13 seran con el selektkbest, tres seran a mi eleccion por criterios propios, para poder trabajar con ellas con otros modelos.\n",
    "Variable target global =  ['ideo_fcz']\n",
    "Variables a mi eleccion = ['ppltrst'],['sclmeet'],['iplylfra']\n",
    "3.2) Modelo de etiquetacion sin variable target.\n",
    "3.3) Modelo de clasificacion para mi variable target global categórica factorizada(como todo el conjunto de datos.)\n",
    "3.4) Especificaciones por agrupaciones de paises EU, Grupo A, Grupo B y Grupo C\n",
    "3.5) Implementacion de mis modelos a una app de Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1) Feature engineering. Split train test dataset 29 variables. Compararemos a posteriori los scores con el dataset a 16 variables.\n",
    "# Divide los datos en conjunto de entrenamiento y prueba.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_29= dfeu1.copy().drop('ideo_fcz', axis=1)\n",
    "y_29 = dfeu1.copy()['ideo_fcz']\n",
    "X_29tr, X_29t, y_29tr, y_29t = train_test_split(X_29, y_29, test_size=0.2, random_state=rng)\n",
    "\n",
    "# Guardo los splits del dataset europeo a 30 variables\n",
    "X_29tr.to_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/X_29tr.csv\", index=False)\n",
    "X_29t.to_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/X_29t.csv\", index=False) \n",
    "y_29tr.to_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/y_29tr.csv\", index=False)\n",
    "y_29t.to_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/y_29t.csv\", index=False)\n",
    "dfeu1.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# --- Carga de datos ---\n",
    "X_29tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/X_29tr.csv\")\n",
    "y_29tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/y_29tr.csv\").squeeze()\n",
    "X_29t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/X_29t.csv\")\n",
    "y_29t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/y_29t.csv\").squeeze()\n",
    "\n",
    "# --- Depuración y preprocesamiento de X_29tr ---\n",
    "print(\"Depuración y preprocesamiento de X_29tr:\")\n",
    "\n",
    "# 1. Revisión de tipos de datos y conversión a numérico\n",
    "print(\"\\nTipos de datos de X_29tr (antes de la conversión):\")\n",
    "print(X_29tr.dtypes)\n",
    "\n",
    "for col in X_29tr.columns:\n",
    "    if X_29tr[col].dtype == 'object':\n",
    "        try:\n",
    "            X_29tr[col] = pd.to_numeric(X_29tr[col])\n",
    "            print(f\"Columna '{col}' convertida a numérica.\")\n",
    "        except ValueError:\n",
    "            print(f\"Columna '{col}' es categórica. Aplicando one-hot encoding...\")\n",
    "            X_29tr = pd.get_dummies(X_29tr, columns=[col], drop_first=True)\n",
    "\n",
    "print(\"\\nTipos de datos de X_29tr (después de la conversión):\")\n",
    "print(X_29tr.dtypes)\n",
    "\n",
    "# 2. Revisión de valores únicos (inspección visual)\n",
    "print(\"\\nValores únicos en X_29tr:\")\n",
    "for col in X_29tr.columns:\n",
    "    print(f\"Columna '{col}': {X_29tr[col].unique()}\")\n",
    "\n",
    "# 3. Imputación de NaN (si es necesario)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_29tr_imputado = imputer.fit_transform(X_29tr)\n",
    "X_29tr_imputado = pd.DataFrame(X_29tr_imputado, columns=X_29tr.columns)\n",
    "\n",
    "# 4. Escalado de características (opcional)\n",
    "scaler = StandardScaler() #Si quieres escalar\n",
    "X_29tr_escalado = scaler.fit_transform(X_29tr_imputado) #Si quieres escalar\n",
    "X_29tr_escalado = pd.DataFrame(X_29tr_escalado, columns=X_29tr_imputado.columns) #Si quieres escalar\n",
    "\n",
    "\n",
    "# --- Depuración y preprocesamiento de y_29tr ---\n",
    "print(\"\\nDepuración y preprocesamiento de y_29tr:\")\n",
    "\n",
    "# 1. Revisión de tipos de datos y conversión a numérico\n",
    "print(\"\\nTipo de dato de y_29tr (antes de la conversión):\")\n",
    "print(y_29tr.dtype)\n",
    "\n",
    "y_29tr = pd.to_numeric(y_29tr)\n",
    "\n",
    "print(\"\\nTipo de dato de y_29tr (después de la conversión):\")\n",
    "print(y_29tr.dtype)\n",
    "\n",
    "# 2. Revisión de valores únicos\n",
    "print(\"\\nValores únicos en y_29tr:\")\n",
    "print(y_29tr.unique())\n",
    "\n",
    "# --- Depuración y preprocesamiento de X_29t ---\n",
    "print(\"\\nDepuración y preprocesamiento de X_29t:\")\n",
    "\n",
    "# 1. Conversión de tipos (usando las mismas columnas que en X_29tr)\n",
    "for col in X_29t.columns:\n",
    "    if X_29t[col].dtype == 'object':\n",
    "        try:\n",
    "            X_29t[col] = pd.to_numeric(X_29t[col])\n",
    "        except ValueError:\n",
    "            print(f\"Columna '{col}' es categórica. Aplicando one-hot encoding (igual que en X_29tr)...\")\n",
    "            X_29t = pd.get_dummies(X_29t, columns=[col], drop_first=True)\n",
    "\n",
    "# Alinear las columnas de X_29t con X_29tr después del one-hot encoding\n",
    "X_29t = X_29t.reindex(columns=X_29tr.columns, fill_value=0)\n",
    "\n",
    "# 2. Imputación (usando el mismo imputer que para X_29tr)\n",
    "X_29t_imputado = imputer.transform(X_29t)\n",
    "X_29t_imputado = pd.DataFrame(X_29t_imputado, columns=X_29t.columns)\n",
    "\n",
    "# 3. Escalado (usando el mismo scaler que para X_29tr, opcional)\n",
    "X_29t_escalado = scaler.transform(X_29t_imputado) #Si quieres escalar\n",
    "X_29t_escalado = pd.DataFrame(X_29t_escalado, columns=X_29t_imputado.columns) #Si quieres escalar\n",
    "\n",
    "# --- Depuración y preprocesamiento de y_29t ---\n",
    "print(\"\\nDepuración y preprocesamiento de y_29t:\")\n",
    "\n",
    "# 1. Revisión de tipos de datos y conversión a numérico\n",
    "print(\"\\nTipo de dato de y_29t (antes de la conversión):\")\n",
    "print(y_29t.dtype)\n",
    "\n",
    "y_29t = pd.to_numeric(y_29t)\n",
    "\n",
    "print(\"\\nTipo de dato de y_29t (después de la conversión):\")\n",
    "print(y_29t.dtype)\n",
    "\n",
    "# 2. Revisión de valores únicos\n",
    "print(\"\\nValores únicos en y_29t:\")\n",
    "print(y_29t.unique())\n",
    "\n",
    "# --- Sobreescritura segura de archivos CSV ---\n",
    "\n",
    "# Define las rutas de los nuevos archivos\n",
    "ruta_X_29tr_limpio = \"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/X_29trstd.csv\"\n",
    "ruta_y_29tr_limpio = \"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/y_29trstd.csv\"\n",
    "ruta_X_29t_limpio = \"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/X_29tstd.csv\"\n",
    "ruta_y_29t_limpio = \"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/y_29tstd.csv\"\n",
    "\n",
    "# Guarda los DataFrames limpios en los nuevos archivos\n",
    "X_29tr_escalado.to_csv(ruta_X_29tr_limpio, index=False) #Si quieres escalar\n",
    "y_29tr.to_csv(ruta_y_29tr_limpio, index=False)\n",
    "X_29t_escalado.to_csv(ruta_X_29t_limpio, index=False) #Si quieres escalar\n",
    "y_29t.to_csv(ruta_y_29t_limpio, index=False)\n",
    "\n",
    "print(f\"\\nArchivo X_29tr limpio guardado en: {ruta_X_29tr_limpio}\")\n",
    "print(f\"Archivo y_29tr limpio guardado en: {ruta_y_29tr_limpio}\")\n",
    "print(f\"Archivo X_29t limpio guardado en: {ruta_X_29t_limpio}\")\n",
    "print(f\"Archivo y_29t limpio guardado en: {ruta_y_29t_limpio}\")\n",
    "\n",
    "# --- En scripts futuros, carga los datos limpios de la siguiente manera: ---\n",
    "# X_29tr = pd.read_csv(ruta_X_29tr_limpio)\n",
    "# y_29tr = pd.read_csv(ruta_y_29tr_limpio, squeeze=True)\n",
    "# X_29t = pd.read_csv(ruta_X_29t_limpio)\n",
    "# y_29t = pd.read_csv(ruta_y_29t_limpio, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "X_29tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/X_29tr.csv\")\n",
    "X_29t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/X_29t.csv\")\n",
    "y_29tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/y_29tr.csv\").squeeze() #se reduce la dimensionalidad a array 1D, para evitar errores de compatibilidad\n",
    "y_29t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/y_29t.csv\").squeeze()\n",
    "\n",
    "#Características (solo en X_29tr, en el conjunto de entrenamiento.)\n",
    "selector = SelectKBest(chi2, k=12)  # Ajusta 'k' a tu necesidad\n",
    "X_29tr_selected = selector.fit_transform(X_29tr, y_29tr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Crear los DataFrames finales (CON LAS 16 VARIABLES)\n",
    "X_16tr = pd.DataFrame(X_29tr_selected, columns=selected_features_kbest)\n",
    "y_16tr = y_29tr\n",
    "X_16t = X_29t_selected\n",
    "y_16t = y_29t\n",
    "#Guardo en csv los split train test a 16\n",
    "X_16tr.to_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/X_16tr.csv\", index=False)\n",
    "X_16t.to_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/X_16t.csv\", index=False) \n",
    "y_16tr.to_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/y_16tr.csv\", index=False)\n",
    "y_16t.to_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/y_16t.csv\", index=False)\n",
    "# --- Carga de datos ---\n",
    "X_16tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/X_16tr.csv\")\n",
    "y_16tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/y_16tr.csv\").squeeze()\n",
    "X_16t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/X_16t.csv\")\n",
    "y_16t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/y_16t.csv\").squeeze()\n",
    "\n",
    "# --- Depuración y preprocesamiento de X_16tr ---\n",
    "print(\"Depuración y preprocesamiento de X_16tr:\")\n",
    "\n",
    "# 1. Revisión de tipos de datos y conversión a numérico\n",
    "print(\"\\nTipos de datos de X_16tr (antes de la conversión):\")\n",
    "print(X_16tr.dtypes)\n",
    "\n",
    "for col in X_16tr.columns:\n",
    "    if X_16tr[col].dtype == 'object':\n",
    "        try:\n",
    "            X_16tr[col] = pd.to_numeric(X_16tr[col])\n",
    "            print(f\"Columna '{col}' convertida a numérica.\")\n",
    "        except ValueError:\n",
    "            print(f\"Columna '{col}' es categórica. Aplicando one-hot encoding...\")\n",
    "            X_16tr = pd.get_dummies(X_16tr, columns=[col], drop_first=True)\n",
    "\n",
    "print(\"\\nTipos de datos de X_16tr (después de la conversión):\")\n",
    "print(X_16tr.dtypes)\n",
    "\n",
    "# 2. Revisión de valores únicos (inspección visual)\n",
    "print(\"\\nValores únicos en X_16tr:\")\n",
    "for col in X_16tr.columns:\n",
    "    print(f\"Columna '{col}': {X_16tr[col].unique()}\")\n",
    "\n",
    "# 3. Imputación de NaN (si es necesario)\n",
    "imputer_16 = SimpleImputer(strategy='mean')\n",
    "X_16tr_imputado = imputer_16.fit_transform(X_16tr)\n",
    "X_16tr_imputado = pd.DataFrame(X_16tr_imputado, columns=X_16tr.columns)\n",
    "\n",
    "# 4. Escalado de características (opcional)\n",
    "scaler_16 = StandardScaler()\n",
    "X_16tr_escalado = scaler_16.fit_transform(X_16tr_imputado)\n",
    "X_16tr_escalado = pd.DataFrame(X_16tr_escalado, columns=X_16tr_imputado.columns)\n",
    "\n",
    "# --- Depuración y preprocesamiento de y_16tr ---\n",
    "print(\"\\nDepuración y preprocesamiento de y_16tr:\")\n",
    "\n",
    "# 1. Revisión de tipos de datos y conversión a numérico\n",
    "print(\"\\nTipo de dato de y_16tr (antes de la conversión):\")\n",
    "print(y_16tr.dtype)\n",
    "\n",
    "y_16tr = pd.to_numeric(y_16tr)\n",
    "\n",
    "print(\"\\nTipo de dato de y_16tr (después de la conversión):\")\n",
    "print(y_16tr.dtype)\n",
    "\n",
    "# 2. Revisión de valores únicos\n",
    "print(\"\\nValores únicos en y_16tr:\")\n",
    "print(y_16tr.unique())\n",
    "\n",
    "# --- Depuración y preprocesamiento de X_16t ---\n",
    "print(\"\\nDepuración y preprocesamiento de X_16t:\")\n",
    "\n",
    "# 1. Conversión de tipos (usando las mismas columnas que en X_16tr)\n",
    "for col in X_16t.columns:\n",
    "    if X_16t[col].dtype == 'object':\n",
    "        try:\n",
    "            X_16t[col] = pd.to_numeric(X_16t[col])\n",
    "        except ValueError:\n",
    "            print(f\"Columna '{col}' es categórica. Aplicando one-hot encoding (igual que en X_16tr)...\")\n",
    "            X_16t = pd.get_dummies(X_16t, columns=[col], drop_first=True)\n",
    "\n",
    "# Alinear las columnas de X_16t con X_16tr después del one-hot encoding\n",
    "X_16t = X_16t.reindex(columns=X_16tr.columns, fill_value=0)\n",
    "\n",
    "# 2. Imputación (usando el mismo imputer que para X_16tr)\n",
    "X_16t_imputado = imputer_16.transform(X_16t)\n",
    "X_16t_imputado = pd.DataFrame(X_16t_imputado, columns=X_16t.columns)\n",
    "\n",
    "# 3. Escalado (usando el mismo scaler que para X_16tr, opcional)\n",
    "X_16t_escalado = scaler_16.transform(X_16t_imputado)\n",
    "X_16t_escalado = pd.DataFrame(X_16t_escalado, columns=X_16t_imputado.columns)\n",
    "\n",
    "# --- Depuración y preprocesamiento de y_16t ---\n",
    "print(\"\\nDepuración y preprocesamiento de y_16t:\")\n",
    "\n",
    "# 1. Revisión de tipos de datos y conversión a numérico\n",
    "print(\"\\nTipo de dato de y_16t (antes de la conversión):\")\n",
    "print(y_16t.dtype)\n",
    "\n",
    "y_16t = pd.to_numeric(y_16t)\n",
    "\n",
    "print(\"\\nTipo de dato de y_16t (después de la conversión):\")\n",
    "print(y_16t.dtype)\n",
    "\n",
    "# 2. Revisión de valores únicos\n",
    "print(\"\\nValores únicos en y_16t:\")\n",
    "print(y_16t.unique())\n",
    "\n",
    "# --- Sobreescritura segura de archivos CSV ---\n",
    "\n",
    "# Define las rutas de los nuevos archivos\n",
    "ruta_X_16tr_limpio = \"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/X_16trstd.csv\"\n",
    "ruta_y_16tr_limpio = \"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/y_16trstd.csv\"\n",
    "ruta_X_16t_limpio = \"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/X_16tstd.csv\"\n",
    "ruta_y_16t_limpio = \"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/y_16tstd.csv\"\n",
    "\n",
    "# Guarda los DataFrames limpios en los nuevos archivos\n",
    "X_16tr_escalado.to_csv(ruta_X_16tr_limpio, index=False)\n",
    "y_16tr.to_csv(ruta_y_16tr_limpio, index=False)\n",
    "X_16t_escalado.to_csv(ruta_X_16t_limpio, index=False)\n",
    "y_16t.to_csv(ruta_y_16t_limpio, index=False)\n",
    "\n",
    "print(f\"\\nArchivo X_16tr limpio guardado en: {ruta_X_16tr_limpio}\")\n",
    "print(f\"Archivo y_16tr limpio guardado en: {ruta_y_16tr_limpio}\")\n",
    "print(f\"Archivo X_16t limpio guardado en: {ruta_X_16t_limpio}\")\n",
    "print(f\"\\nArchivo y_16t limpio guardado en: {ruta_y_16t_limpio}\")\n",
    "\n",
    "# --- En scripts futuros, carga los datos limpios de la siguiente manera: ---\n",
    "# X_16tr = pd.read_csv(ruta_X_16tr_limpio)\n",
    "# y_16tr = pd.read_csv(ruta_y_16tr_limpio, squeeze=True)\n",
    "# X_16t = pd.read_csv(ruta_X_16t_limpio)\n",
    "# y_16t = pd.read_csv(ruta_y_16t_limpio, squeeze=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ya que tenemos todo lo necesario para entrenar Borramos todo lo que hay en la cache.\n",
    "%reset -f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomforest con 29 variables por defecto:\n",
    "from sklearn.feature_selection import SelectKBest, chi2  # O tu método preferido\n",
    "from sklearn.ensemble import RandomForestClassifier  # O tu modelo preferido\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import os\n",
    "print(\"Entrenamiento y evaluación con 29 variables:\")\n",
    "X_29tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/X_29trstd.csv\")\n",
    "X_29t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/X_29tstd.csv\")\n",
    "y_29tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/y_29trstd.csv\").squeeze() #se reduce la dimensionalidad a array 1D, para evitar errores de compatibilidad\n",
    "y_29t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/y_29tstd.csv\").squeeze()\n",
    "model_29 = RandomForestClassifier(random_state=42)  # O tu modelo preferido\n",
    "model_29.fit(X_29tr, y_29tr)\n",
    "\n",
    "y_pred_29 = model_29.predict(X_29t)\n",
    "\n",
    "average = 'weighted'\n",
    "y_prob_29 = model_29.predict_proba(X_29t)\n",
    "auc_roc_29 = roc_auc_score(y_29t, y_prob_29, multi_class=\"ovr\", average=\"weighted\")\n",
    "print(\"AUC-ROC (29 variables):\", auc_roc_29) #No difiere mucho de un modelo al azar si este es cercano a 0.5\n",
    "print(\"Precisión (29 variables):\", precision_score(y_29t, y_pred_29, average=average)) #Si es inferior al 0.5 indica que el modelo tiene buena capacidad para identificar falsos positivos.\n",
    "print(\"Recall (29 variables):\", recall_score(y_29t, y_pred_29, average=average)) # Si es superior al 0.5 indica que el modelo tiene buena capacidad para identificar falsos negativos\n",
    "print(\"F1-score (29 variables):\", f1_score(y_29t, y_pred_29, average=average)) #Superior al 0.5 sugiere que el modelo entre su precision y recall hay correlatividad positva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo randomforest por defecto 16 variables:\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier  # O tu modelo preferido\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import os\n",
    "\n",
    "print(\"Entrenamiento y evaluación con 16 variables:\")\n",
    "X_29tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/X_16trstd.csv\")\n",
    "X_29t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/X_16tstd.csv\")\n",
    "y_29tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/y_16trstd.csv\").squeeze() #se reduce la dimensionalidad a array 1D, para evitar errores de compatibilidad\n",
    "y_29t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/y_16tstd.csv\").squeeze()\n",
    "#Entrenamiento del modelo\n",
    "model = RandomForestClassifier(random_state=42)  # O tu modelo preferido\n",
    "model.fit(X_16tr, y_16tr)\n",
    "\n",
    "#Predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_16t)\n",
    "\n",
    "#Evaluación del modelo\n",
    "\n",
    "average = 'weighted'  # O 'macro', 'micro', según tu necesidad\n",
    "y_prob_16 = model.predict_proba(X_16t)\n",
    "auc_roc_16 = roc_auc_score(y_16t, y_prob_16, multi_class=\"ovr\", average=\"weighted\")\n",
    "print(\"AUC-ROC (29 variables):\", auc_roc_16) #No difiere mucho de un modelo al azar si este es cercano a 0.5\n",
    "print(\"Precisión:\", precision_score(y_16t, y_pred, average=average))\n",
    "print(\"Recall:\", recall_score(y_16t, y_pred, average=average))\n",
    "print(\"F1-score:\", f1_score(y_16t, y_pred, average=average))\n",
    "del X_29tr, X_29t, y_29tr, y_29t, X_29tr_to16, X_29t_to16, y_29tr_to16, y_29t_to16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1) Vemos que nuestro RandomForest consigue algo en los scores con variables reducidas a 16. No mucho pero algo gana, ahora debemos parametrizar el modelo y guardar el mejor para su uso de clasificacion. Tambien podemos jugar con el numero de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperparametrizacion XGBOOST. 29 variables. usando como metrica en el motor del modelo f1-score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import os\n",
    "import pickle\n",
    "mapping = {1: 0, 2: 1, 3: 2}\n",
    "X_29tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/X_29tr.csv\")\n",
    "X_29t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/X_29t.csv\")\n",
    "y_29tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/y_29tr.csv\").squeeze()\n",
    "y_29t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/y_29t.csv\").squeeze()\n",
    "\n",
    "\n",
    "y_29tr = y_29tr.map(mapping)\n",
    "y_29t = y_29t.map(mapping)\n",
    "\n",
    "\n",
    "model_xgb_29 = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "# 3. Definir la grilla de hiperparámetros a probar\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'gamma': [0, 0.1, 0.5],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# 4. Realizar la búsqueda de hiperparámetros con GridSearchCV\n",
    "grid_search = GridSearchCV(model_xgb_29, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "grid_search.fit(X_29tr, y_29tr)\n",
    "\n",
    "# 5. Obtener el mejor modelo\n",
    "best_model_xgb_29 = grid_search.best_estimator_\n",
    "\n",
    "# 6. Realizar predicciones en el conjunto de prueba\n",
    "y_pred_xgb_29 = best_model_xgb_29.predict(X_29t)\n",
    "\n",
    "# 7. Evaluar el mejor modelo\n",
    "average = 'weighted'\n",
    "\n",
    "print(\"Precisión (29 variables, XGBoost):\", precision_score(y_29t, y_pred_xgb_29, average=average))\n",
    "print(\"Recall (29 variables, XGBoost):\", recall_score(y_29t, y_pred_xgb_29, average=average))\n",
    "print(\"F1-score (29 variables, XGBoost):\", f1_score(y_29t, y_pred_xgb_29, average=average))\n",
    "\n",
    "# AUC-ROC para multiclase (One-vs-Rest)\n",
    "y_prob_xgb_29 = best_model_xgb_29.predict_proba(X_29t)\n",
    "auc_roc_xgb_29 = roc_auc_score(y_29t, y_prob_xgb_29, multi_class=\"ovr\", average=\"weighted\")\n",
    "print(\"AUC-ROC (29 variables, XGBoost):\", auc_roc_xgb_29)\n",
    "\n",
    "# 8. Guardar el modelo con 29 variables (con directorio y nombre personalizables)\n",
    "\n",
    "# Solicitar al usuario el directorio y nombre del archivo (con valores por defecto)\n",
    "directorio_29 = \"C:/GitHubRepos/ProyectoFinal/data/models\"\n",
    "nombre_archivo_29 = 'F1GSRF29.pkl'\n",
    "\n",
    "# Crear el directorio si no existe\n",
    "if not os.path.exists(directorio_29):\n",
    "    os.makedirs(directorio_29)\n",
    "\n",
    "ruta_completa_29 = os.path.join(directorio_29, nombre_archivo_29)\n",
    "pickle.dump(best_model_xgb_29, open(ruta_completa_29, 'wb'))\n",
    "\n",
    "print(f\"Modelo guardado en: {ruta_completa_29}\")\n",
    "del X_29tr, X_29t, y_29tr, y_29t,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperparametrizacion XGBOOST. 16 variables. usando como metrica en el motor del modelo f1-score\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, chi2  # If you're still using feature selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Load the 16-variable data\n",
    "X_16tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/X_16tr.csv\")\n",
    "X_16t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/X_16t.csv\")\n",
    "y_16tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/y_16tr.csv\").squeeze()\n",
    "y_16t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/y_16t.csv\").squeeze()\n",
    "\n",
    "# Remap the target variable (important!)\n",
    "mapping = {1: 0, 2: 1, 3: 2}\n",
    "y_16tr = y_16tr.map(mapping)\n",
    "y_16t = y_16t.map(mapping)\n",
    "\n",
    "\n",
    "model_xgb_to16 = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [150, 250, 400],\n",
    "    'max_depth': [4,8],\n",
    "    'learning_rate': [0.01, 0.05, 0.15],\n",
    "    'gamma': [0.01, 0.1, 0.5],\n",
    "    'subsample': [0.4, 0.6, 0.8],\n",
    "    'colsample_bytree': [0.5,0.7, 0.8, 0.9]\n",
    "}\n",
    "#Modificamos el grid search hasta encontrar mejores scores.\n",
    "# 7. Perform hyperparameter search\n",
    "grid_search_to16 = GridSearchCV(model_xgb_to16, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "\n",
    "grid_search_to16.fit(X_16tr, y_16tr)  # Use the 16-variable data here!\n",
    "\n",
    "# 8. Get the best model\n",
    "best_model_xgb_to16 = grid_search_to16.best_estimator_\n",
    "\n",
    "# 9. Make predictions\n",
    "y_pred_xgb_to16 = best_model_xgb_to16.predict(X_16t)  # Predict on the 16-variable test set\n",
    "\n",
    "# 10. Evaluate the model\n",
    "average = 'weighted'\n",
    "print(\"Precisión (16 variables, XGBoost):\", precision_score(y_16t, y_pred_xgb_to16, average=average))\n",
    "print(\"Recall (16 variables, XGBoost):\", recall_score(y_16t, y_pred_xgb_to16, average=average))\n",
    "print(\"F1-score (16 variables, XGBoost):\", f1_score(y_16t, y_pred_xgb_to16, average=average))\n",
    "\n",
    "# AUC-ROC\n",
    "y_prob_xgb_to16 = best_model_xgb_to16.predict_proba(X_16t)\n",
    "auc_roc_xgb_to16 = roc_auc_score(y_16t, y_prob_xgb_to16, multi_class=\"ovr\", average=\"weighted\")\n",
    "print(\"AUC-ROC (16 variables, XGBoost):\", auc_roc_xgb_to16)\n",
    "\n",
    "# 11. Save the model\n",
    "directorio_16 = \"C:/GitHubRepos/ProyectoFinal/data/models\"\n",
    "nombre_archivo_16 = \"F1GSRF16.pkl\"\n",
    "\n",
    "if not os.path.exists(directorio_16):\n",
    "    os.makedirs(directorio_16)\n",
    "\n",
    "ruta_completa_16 = os.path.join(directorio_16, nombre_archivo_16)\n",
    "pickle.dump(best_model_xgb_to16, open(ruta_completa_16, 'wb'))\n",
    "\n",
    "print(f\"Modelo guardado en: {ruta_completa_16}\")\n",
    "del X_16tr, X_16t, y_16tr, y_16t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_29tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/X_29tr.csv\")\n",
    "print(X_29tr.isnull().sum())  # Suma de NaN por columna\n",
    "print(X_29tr.isna().any().any()) #Verifica si hay algún NaN en el dataframe\n",
    "for col in X_29tr.columns:\n",
    "    if X_29tr[col].dtype == 'object':\n",
    "        print(f\"Columna '{col}' es de tipo 'object'. Convirtiendo...\")\n",
    "        X_29tr = pd.get_dummies(X_29tr, columns=[col], drop_first=True) #One-hot encoding y drop_first para evitar multicolinealidad\n",
    "\n",
    "print(X_29tr.dtypes) #Verifica que se hayan convertido\n",
    "for col in X_29tr.columns:\n",
    "    if X_29tr[col].dtype == 'object': #Si aun hay columnas object\n",
    "        try:\n",
    "            X_29tr[col] = pd.to_numeric(X_29tr[col])\n",
    "        except ValueError: #Si no se puede convertir a numérico, es probablemente categórica.\n",
    "            X_29tr = pd.get_dummies(X_29tr, columns=[col], drop_first=True)  # One-hot encoding\n",
    "\n",
    "print(X_29tr.dtypes) #Verifica que se hayan convertido\n",
    "X_29tr.to_csv('C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\dfeu29\\X_29tr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizaremos otra metrica AUC-SCore, para el rendimiento del modelo de clasificacion , ademaas utilizaremos randomsearch com punto de partida.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, make_scorer  # Import make_scorer\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "X_29tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/X_29tr.csv\")\n",
    "X_29t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/X_29t.csv\")\n",
    "y_29tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/y_29tr.csv\").squeeze()\n",
    "y_29t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu29/y_29t.csv\").squeeze()\n",
    "\n",
    "mapping = {1: 0, 2: 1, 3: 2}\n",
    "y_29tr = y_29tr.map(mapping)\n",
    "y_29t = y_29t.map(mapping)\n",
    "\n",
    "\n",
    "\n",
    "model_xgb_29 = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "param_grid_29 = {  \n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01,0.05, 0.1, 0.15],\n",
    "    'gamma': [0, 0.05, 0.1, 0.5],\n",
    "    'subsample': [0.4,0.6, 0.8],\n",
    "    'colsample_bytree': [0.5,0.7, 0.9],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1],\n",
    "    'reg_lambda': [0, 0.1,0.5, 1]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring29 = {\n",
    "    'auc': make_scorer(roc_auc_score, multi_class=\"ovr\", average=\"weighted\"),\n",
    "    'f1': make_scorer(f1_score, average='weighted'),\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted')\n",
    "}\n",
    "random_search_29 = RandomizedSearchCV(\n",
    "    model_xgb_29,\n",
    "    param_grid_29,\n",
    "    n_iter=20,  # Ajusta el número de iteraciones\n",
    "    scoring=scoring29,  # Usar el diccionario de métricas\n",
    "    cv=cv,\n",
    "    refit='auc',  # Refinar el modelo usando AUC-ROC\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    return_train_score=True, #Incluir el score de entrenamiento\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search_29.fit(X_29tr, y_29tr)\n",
    "\n",
    "best_model_xgb_29 = random_search_29.best_estimator_\n",
    "\n",
    "directorio_29 = \"C:/GitHubRepos/ProyectoFinal/data/models\"\n",
    "nombre_archivo_29 = \"ARSRF29.pkl\"\n",
    "\n",
    "if not os.path.exists(directorio_29):\n",
    "    os.makedirs(directorio_29)\n",
    "\n",
    "ruta_completa_29 = os.path.join(directorio_29, nombre_archivo_29)\n",
    "pickle.dump(best_model_xgb_29, open(ruta_completa_29, 'wb'))\n",
    "\n",
    "print(f\"Modelo guardado en: {ruta_completa_29}\")\n",
    "random_search_29.fit(X_29tr, y_29tr)\n",
    "best_model_xgb_29 = random_search_29.best_estimator_\n",
    "resultados_29 = pd.DataFrame(random_search_29.cv_results_)\n",
    "mejor_indice_29 = resultados_29['mean_test_auc'].idxmax()\n",
    "mejores_parametros_29 = resultados_29.loc[mejor_indice_29, 'params']\n",
    "mejor_auc_29 = resultados_29.loc[mejor_indice_29, 'mean_test_auc']\n",
    "mejor_f1_29 = resultados_29.loc[mejor_indice_29, 'mean_test_f1']\n",
    "mejor_precision_29 = resultados_29.loc[mejor_indice_29, 'mean_test_precision']\n",
    "mejor_recall_29 = resultados_29.loc[mejor_indice_29, 'mean_test_recall']\n",
    "print(\"Modelo con 29 variables:\")\n",
    "print(f\"Mejores parámetros: {mejores_parametros_29}\")\n",
    "print(f\"Mejor AUC: {mejor_auc_29}\")\n",
    "print(f\"Mejor F1: {mejor_f1_29}\")\n",
    "print(f\"Mejor Precision: {mejor_precision_29}\")\n",
    "print(f\"Mejor Recall: {mejor_recall_29}\")\n",
    "del X_29tr, X_29t, y_29tr, y_29t,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo 16 variables, metrica auc roc\n",
    "X_16tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/X_16tr.csv\")\n",
    "X_16t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/X_16t.csv\")\n",
    "y_16tr = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/y_16tr.csv\").squeeze()\n",
    "y_16t = pd.read_csv(\"C:/GitHubRepos/ProyectoFinal/data/processed/dfeu16/y_16t.csv\").squeeze()\n",
    "y_16tr = y_16tr.map(mapping)\n",
    "y_16t = y_16t.map(mapping)\n",
    "\n",
    "model_xgb_16 = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "scoring16 = {\n",
    "    'auc': make_scorer(roc_auc_score, multi_class=\"ovr\", average=\"weighted\"),\n",
    "    'f1': make_scorer(f1_score, average='weighted'),\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted')\n",
    "}\n",
    "param_grid_16 = { \n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01,0.05, 0.1, 0.15],\n",
    "    'gamma': [0, 0.05, 0.1, 0.5],\n",
    "    'subsample': [0.4,0.6, 0.8],\n",
    "    'colsample_bytree': [0.5,0.7, 0.9],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1],\n",
    "    'reg_lambda': [0, 0.1,0.5, 1]\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "random_search_16 = RandomizedSearchCV(\n",
    "    model_xgb_29,\n",
    "    param_grid_29,\n",
    "    n_iter=20,  # Ajusta el número de iteraciones\n",
    "    scoring=scoring16,  # Usar el diccionario de métricas\n",
    "    cv=cv,\n",
    "    refit='auc',  # Refinar el modelo usando AUC-ROC (CORREGIDO)\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    return_train_score=True, #Incluir el score de entrenamiento\n",
    "    random_state=42\n",
    ")\n",
    "random_search_16.fit(X_16tr, y_16tr)\n",
    "best_model_xgb_16 = random_search_16.best_estimator_\n",
    "directorio_16 = \"C:/GitHubRepos/ProyectoFinal/data/models\"\n",
    "nombre_archivo_16 = \"ARSRF16.pkl\"\n",
    "if not os.path.exists(directorio_16):\n",
    "    os.makedirs(directorio_16)\n",
    "ruta_completa_16 = os.path.join(directorio_16, nombre_archivo_16)\n",
    "pickle.dump(best_model_xgb_to16, open(ruta_completa_16, 'wb'))\n",
    "print(f\"Modelo guardado en: {ruta_completa_16}\")\n",
    "random_search_16.fit(X_16tr, y_16tr)\n",
    "best_model_xgb_16 = random_search_16.best_estimator_\n",
    "resultados_16 = pd.DataFrame(random_search_16.cv_results_)\n",
    "mejor_indice_16 = resultados_16['mean_test_auc'].idxmax()\n",
    "mejores_parametros_16 = resultados_16.loc[mejor_indice_16, 'params']\n",
    "mejor_auc_16 = resultados_16.loc[mejor_indice_16, 'mean_test_auc']\n",
    "mejor_f1_16 = resultados_16.loc[mejor_indice_16, 'mean_test_f1']\n",
    "mejor_precision_16 = resultados_16.loc[mejor_indice_16, 'mean_test_precision']\n",
    "mejor_recall_16 = resultados_16.loc[mejor_indice_16, 'mean_test_recall']\n",
    "print(\"\\nModelo con 16 variables:\")\n",
    "print(f\"Mejores parámetros: {mejores_parametros_16}\")\n",
    "print(f\"Mejor AUC: {mejor_auc_16}\")\n",
    "print(f\"Mejor F1: {mejor_f1_16}\")\n",
    "print(f\"Mejor Precision: {mejor_precision_16}\")\n",
    "print(f\"Mejor Recall: {mejor_recall_16}\")\n",
    "del X_16tr, X_16t, y_16tr, y_16t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".FPJBGPCSobremesa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
