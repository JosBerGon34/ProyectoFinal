{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECOLECCION Y FILTRADO DE DATOS: \n",
    "\n",
    "OBTENEMOS LOS DATOS EN FORMATO CSV, ADEMAS DEL INDICE DE VARIABLES SELECCIONADAS EN ARCHIVO HTML.\n",
    "LA PAGINA ES: ESS DATA PORTAL, ORGANISMO EUROPEO DE ENCUESTAS POR PAISES.\n",
    "RECOLECTAN DATOS SOCIODEMOGRAFICOS , POLÍTICOS, BIENESTAR SOCIAL , ETC.\n",
    "\n",
    "NOS CENTRAREMOS EN LOS DATOS RECOLECTADOS EN DOS GRUPOS DELIMITADOS POR EL GRADO DE INFLACION(DEFLACION DEL PIB) DE LA ZONA EURO.\n",
    "LA FUENTE DE DATOS PARA LA CLASIFICACION POR PIB PER CAPITA E INFLACION LA OBTENEMOS DE LA WEB OFICIAL DEL BANCO MUNDIAL.\n",
    "''https://datos.bancomundial.org/indicator/NY.GDP.DEFL.KD.ZG?end=2023&locations=EU&most_recent_value_desc=true&start=1960&view=chart''\n",
    "\n",
    "EN EL GRUPO 1 ESTARAN LOS TRES PAISES CON MAYOR INFLACION(RANGO COMPRENDIDO ENTRE 14,7-11,7) DE LA EUROZONA Y DE LOS CUALES SE TIENEN DATOS SOCIOECONOMICOS ACTUALIZADOS: HUNGRIA, SLOVAKIA Y CROACIA.\n",
    "EN EL GRUPO 2 ESTARAN LOS TRES PAISES CON MENOR INFLACION DE LA EUROZONA: FINLANDIA, BÉLGICA E IRLANDA.\n",
    "\n",
    "EN EL CRITERIO DE SELECCION DE PAISES PARA LOS GRUPOS HEMOS TENIDO EN CUENTA OTROS FACTORES IMPORTANTES:\n",
    "Los paises de los grupos han de tener datos oficiales socioeconomicos actualizados (en el grupo 1 hemos omitido Rumania por este motivo), una cercania geografica relativamente alta, las caracteristicas como país además han de ser parecidas(país sin ventajas fiscales, en el grupo 2 hemos omitido por este motivo Luxemburgo y Chipre, trato politico ecuanime en la eurozona, cultura similar.)\n",
    "Dentro del estudio hemos obviado dinamarca, dado que es el unico pais de la zona euro con una inflacion negativa, por lo que el grado de satisfaccion  y mentalidad ciudadana va a estar influenciado y se va a reflejar en los datos obtenidos de las encuestas.\n",
    "\n",
    "https://ess.sikt.no/en/\n",
    "Hacemos una seleccion de variables dentro de las posibilidades que nos ofrece el selector de la web:\n",
    "Escogemos los apartados de mass media, políticas, bienestar social, y valores humanos.\n",
    "Haremos una seleccion de datos y las tablas se almacenaran en SQL para utilizarlas en otro notebook de exploracion de datos\n",
    "y realizar asi el feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRIMERO DE TODO VAMOS A HACER UN CLUSTER DE PAISES BASANDONOS EN CRITERIOS ECONOMICOS BASICOS.\n",
    "#1) Creacion de Subgrupos de paises:\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "import sqlite3\n",
    "#1.1 Nos conectamos a la base de datos.\n",
    "database_path = os.path.abspath(\"C:/GitHubRepos/ProyectoFinal/data/processed/Dataset9k.db\")\n",
    "print(f\"Database path: {database_path}\")\n",
    "conn = create_engine(f'sqlite:///{database_path}')\n",
    "# Execute the SQL query, ya hemos visto en el visualizador de sql que los datos que deberian ser numericos hay algunas columnas que son formato texto, \n",
    "# hacemos un query para transformarlos justo antes de almacenarlos en un dataframe\n",
    "\n",
    "#DF inflacion\n",
    "query = \"\"\"\n",
    "SELECT País, REPLACE(Inflación, ',', '.') AS Inflación_Float\n",
    "FROM EuroInfl\n",
    "\"\"\"\n",
    "#Creamos un diccionario para almacenar la conversion\n",
    "dtype_dict = {'Inflación_Float': float}\n",
    "# Load the DataFrame with the dtype dictionary\n",
    "df_inf = pd.read_sql_query(query, conn, dtype=dtype_dict)\n",
    "df_inf.drop_duplicates(subset='País', keep='first', inplace=True)\n",
    "\n",
    "#DF PPC\n",
    "query = f\"\"\"\n",
    "SELECT País, ROUND(PPC) AS PPC_INTEGER\n",
    "FROM EuroPPC\n",
    "\"\"\"\n",
    "df_PPC = pd.read_sql_query(query, conn)\n",
    "df_PPC.drop_duplicates(subset='País', keep='first', inplace=True)\n",
    "print(df_PPC)\n",
    "\n",
    "#DF SBMM\n",
    "query = \"\"\"\n",
    "SELECT País, REPLACE(SBMM, ',', '.') AS SBMM_Float\n",
    "FROM EuroSBMM\n",
    "\"\"\"\n",
    "dtype_dict = {'SBMM': float}\n",
    "df_SBMM = pd.read_sql_query(query, conn)\n",
    "df_SBMM.drop_duplicates(subset='País', keep='first', inplace=True)\n",
    "df_SBMM['País'] = df_SBMM['País'].str.strip().str.lower()\n",
    "\n",
    "conn.dispose()\n",
    "print(df_SBMM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No hay forma de cambiar la columna paises de la tabla SBMM, por lo que habra que realizar un diccionario de equivalencias para hacer coincidir la columna paises en los tres\n",
    "#Dataframes de graficos, realizaremos posteriormente una clusterizacion para elegir 3 subgrupos de paises con economias similares y poder compararlos\n",
    "#A nivel general con la Union Europea.\n",
    "país_equivalencias = {\n",
    "    'bulgaria :bg:': 'Bulgaria',\n",
    "    'rumanía :ro:': 'Rumania',\n",
    "    'hungría :hu:': 'Hungría',\n",
    "    'croacia :hr:': 'Croacia',\n",
    "    'polonia :pl:': 'Polonia',\n",
    "    'letonia :lv:': 'Letonia',\n",
    "    'eslovaquia :sk:': 'República Eslovaca',\n",
    "    'grecia :gr:': 'Grecia',\n",
    "    'lituania :lt:': 'Lituania',\n",
    "    'estonia :ee:': 'Estonia',\n",
    "    'portugal :pt:': 'Portugal',\n",
    "    'república checa :cz:': 'República Checa',\n",
    "    'malta :mt:': 'Malta',\n",
    "    'chipre :cy:': 'Chipre',\n",
    "    'eslovenia :si:': 'Eslovenia',\n",
    "    'españa :es:': 'España',\n",
    "    'italia :it:': 'Italia',\n",
    "    'media ue :eu:': 'Unión Europea',\n",
    "    'francia :fr:': 'Francia',\n",
    "    'suecia :se:': 'Suecia',\n",
    "    'finlandia :fi:': 'Finlandia',\n",
    "    'austria :at:': 'Austria',\n",
    "    'países bajos :nl:': 'Países Bajos',\n",
    "    'bélgica :be:': 'Bélgica',\n",
    "    'irlanda :ie:': 'Irlanda',\n",
    "    'alemania :de:': 'Alemania',\n",
    "    'dinamarca :dk:': 'Dinamarca',\n",
    "    'luxemburgo :lu:': 'Luxemburgo',\n",
    "}\n",
    "df_SBMM['País'] = df_SBMM['País'].map(país_equivalencias)\n",
    "print(df_SBMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Antes de la concatenacion de los dataframe hemos de hacer coincidir el tipo de datos, coincidir el index \n",
    "\n",
    "df_inf.reset_index(drop=True, inplace=True)\n",
    "print(df_inf['País'].dtype)\n",
    "print(df_inf['Inflación_Float'].dtype)\n",
    "print(df_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_PPC dtypes\n",
    "df_PPC.reset_index(drop=True, inplace=True)\n",
    "print(df_PPC['País'].dtype)\n",
    "print(df_PPC['PPC_INTEGER'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_SBMM\n",
    "df_SBMM.reset_index(drop=True, inplace=True)\n",
    "print(df_SBMM['País'].dtype)\n",
    "print(df_SBMM['SBMM_Float'].dtype)\n",
    "#Aqui tenemos el problema, hemos de pasar la columna SBMM_FLoat a float64\n",
    "df_SBMM['SBMM_Float'] = df_SBMM['SBMM_Float'].astype('float64')\n",
    "print(df_SBMM['SBMM_Float'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparamos el dataframe concatenado filtrando por valores unicos de paises y sus valores numericos en cada uno de los dataframes.\n",
    "# Encontrar países comunes\n",
    "paises_comunes = set(df_inf['País']).intersection(set(df_PPC['País']), set(df_SBMM['País']))\n",
    "\n",
    "# Filtrar DataFrames para países comunes y establecer 'País' como índice\n",
    "df_i1 = df_inf[df_inf['País'].isin(paises_comunes)].set_index('País')\n",
    "df_P1 = df_PPC[df_PPC['País'].isin(paises_comunes)].set_index('País')\n",
    "df_S1 = df_SBMM[df_SBMM['País'].isin(paises_comunes)].set_index('País')\n",
    "# Concatenar los DataFrames por columnas\n",
    "df_IPS = pd.concat([df_i1, df_P1, df_S1], axis=1)\n",
    "#Eliminamos variables usadas que no son inutiles.\n",
    "df_IPS.to_csv('C:/GitHubRepos/ProyectoFinal/data/processed/EuEco.csv', index=True)\n",
    "del df_i1, df_P1, df_S1, df_inf, df_SBMM, df_PPC, país_equivalencias, dtype_dict, paises_comunes, df_IPS\n",
    "df1= pd.read_csv('C:\\GitHubRepos\\ProyectoFinal\\data\\processed\\EuEco.csv')\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#1.5 Representacion Graficos Economicos.\n",
    "# Crear una grupo de graficos lineales con 3 subplots en una fila\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 8))\n",
    "# Graficar cada DataFrame en un subplot diferente\n",
    "axes[0].plot(df1['País'], df1['Inflación_Float'])\n",
    "axes[0].set_title('Inflacion Europa 2023')\n",
    "axes[0].set_xlabel('Paises Euro')\n",
    "axes[0].set_ylabel('Porcentaje')\n",
    "axes[0].tick_params(axis='x', rotation=90)  \n",
    "\n",
    "axes[1].plot(df1['País'], df1['SBMM_Float'], color='Green')\n",
    "axes[1].set_title('Sueldo Bruto Mensual Medio')\n",
    "axes[1].set_xlabel('Paises Euro')\n",
    "axes[1].set_ylabel('Euros')\n",
    "axes[1].set_ylim(ymin=0) \n",
    "axes[1].tick_params(axis='x', rotation=90)  \n",
    "\n",
    "axes[2].plot(df1['País'], df1['PPC_INTEGER'],  color='Orange')\n",
    "axes[2].set_title('PIB Per Capita Anual')\n",
    "axes[2].set_xlabel('Paises Euro')\n",
    "axes[2].set_ylabel('Miles de Euros')\n",
    "axes[2].set_ylim(ymin=0)  \n",
    "axes[2].tick_params(axis='x', rotation=90) \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "df1.set_index('País', inplace=True)\n",
    "for column in ['Inflación_Float', 'PPC_INTEGER', 'SBMM_Float']:\n",
    "    sns.lineplot(data=df1, x=df1.index, y=column, label=column)\n",
    "plt.title('Evolución de indicadores económicos por país')\n",
    "plt.xlabel('País')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend(title='Indicador')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()\n",
    "del axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora vamos a crear 3 subgrupos de paises: \n",
    "#Los haremos automaticamente con KMeans, podemos predecir los grupos\n",
    "#por las caracteristicas economicas.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "\n",
    "X = df1[['Inflación_Float', 'PPC_INTEGER', 'SBMM_Float']]\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(X)\n",
    "df1['cluster'] = kmeans.labels_\n",
    "fig = px.scatter_3d(df1, x='Inflación_Float', y='PPC_INTEGER', z='SBMM_Float',\n",
    "              color='cluster',\n",
    "              opacity=0.7,\n",
    "              color_continuous_scale='viridis',\n",
    "              symbol='cluster',\n",
    "              size_max=18)\n",
    "fig.update_layout(\n",
    "    title='Clustering de países en 3D',\n",
    "    scene = dict(\n",
    "        xaxis_title='Inflación',\n",
    "        yaxis_title='PPC',\n",
    "        zaxis_title='SBMM'\n",
    "    ),\n",
    "    legend=dict(\n",
    "        title='Cluster',\n",
    "        yanchor=\"top\",\n",
    "        xanchor=\"right\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora llamamos a los valores unicos del indice default del df1 que se agrupan en cada cluster para seleccionar ademas \n",
    "#del criterio matematico, razones geograficas y culturales.\n",
    "\n",
    "for i in range(kmeans.n_clusters):\n",
    "    cluster_i = df1[df1['cluster'] == i].index\n",
    "    print(f\"Países en el cluster {i}:\")\n",
    "    print(cluster_i.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECCION DE VARIABLES DE LOS DATOS DE ESS PORTAL:\n",
    "Como ya he comentado, para nuestro proyecto tomaremos en cuenta solo 4 grandes bloques de todos los datos recolectados en ESS, añadiremos un factor bayesiano y aleatorio en el bloque 5 para reducir la interpretabilidad de los datos.\n",
    "\n",
    "Variables Bloque 1(Media), Media use and Trust (Uso, tipo de los medios de informacion, y grado de confianza).De todas las variables explicadas y recolectadas nos quedamos:\n",
    "\n",
    "Variables Bloque 2(Politicas),  Políticas(perfil politico del individuo, participacion an nivel social y político, etc):\n",
    "\n",
    "Variables Bloque 3(Bienestar),  Bienestar social(Situaci;on social del individuo, etnia, etc.):\n",
    "\n",
    "Varibales Bloque 4(Valores e Inquietudes),  Valores Humanos(Objetivos de vida, inquietudes, enfoque de la vida, etc.):\n",
    "\n",
    "\n",
    "\n",
    "TODAS LAS VARIABLES SON CATEGÓRICAS FACTORIZADAS Y ALGUNAS EN ESCALA ARBITRARIA DE 0 A 1 CON VALORES INTERMEDIOS.\n",
    "SE DESCARTARAN VALORES DE VARIABLES INCONCLUSOS.(//value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECCION VARIABLES BLOQUE 1 (Media):\n",
    "VAR netusoft - Internet use, how often\n",
    "Value\tCategory\n",
    "1\tNever\n",
    "2\tOnly occasionally\n",
    "3\tA few times a week\n",
    "4\tMost days\n",
    "5\tEvery day\n",
    "//7\tRefusal*-DISCARDED\n",
    "//8\tDon't know*-DISCARDED\n",
    "//9\tNo answer*-DISCARDED\n",
    "\n",
    "VAR nwsptot - Newspaper reading, total time on average weekday\n",
    "Value\tCategory\n",
    "0\tNo time at all\n",
    "1\tLess than 0,5 hour\n",
    "2\t0,5 hour to 1 hour\n",
    "3\tMore than 1 hour, up to 1,5 hours\n",
    "4\tMore than 1,5 hours, up to 2 hours\n",
    "5\tMore than 2 hours, up to 2,5 hours\n",
    "6\tMore than 2,5 hours, up to 3 hours\n",
    "7\tMore than 3 hours\n",
    "//77\tRefusal*-DISCARDED\n",
    "//88\tDon't know*-DISCARDED\n",
    "//99\tNo answer* -DISCARDED\n",
    "\n",
    "VAR ppltrst - Most people of the media can be trusted or you can't be too careful (VAR TARGET DEL BLOQUE 1)\n",
    "Value\tCategory\n",
    "0\tYou can't be too careful\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tMost people can be trusted\n",
    "//77\tRefusal* -DISCARDED\n",
    "//88\tDon't know* -DISCARDED\n",
    "//99\tNo answer*-DISCARDED\n",
    "\n",
    "VAR rdtot - Radio listening, total time on average weekday.\n",
    "\n",
    "Value\tCategory\n",
    "0\tNo time at all\n",
    "1\tLess than 0,5 hour\n",
    "2\t0,5 hour to 1 hour\n",
    "3\tMore than 1 hour, up to 1,5 hours\n",
    "4\tMore than 1,5 hours, up to 2 hours\n",
    "5\tMore than 2 hours, up to 2,5 hours\n",
    "6\tMore than 2,5 hours, up to 3 hours\n",
    "7\tMore than 3 hours\n",
    "//77\tRefusal*-DISCARDED\n",
    "//88\tDon't know*-DISCARDED\n",
    "//99\tNo answer*-DISCARDED\n",
    "\n",
    "VAR tvtot - TV watching, total time on average weekday.\n",
    "Value\tCategory\n",
    "0\tNo time at all\n",
    "1\tLess than 0,5 hour\n",
    "2\t0,5 hour to 1 hour\n",
    "3\tMore than 1 hour, up to 1,5 hours\n",
    "4\tMore than 1,5 hours, up to 2 hours\n",
    "5\tMore than 2 hours, up to 2,5 hours\n",
    "6\tMore than 2,5 hours, up to 3 hours\n",
    "7\tMore than 3 hours\n",
    "//77\tRefusal*-DISCARDED\n",
    "//88\tDon't know*-DISCARDED\n",
    "//99\tNo answer*-DISCARDED\n",
    "\n",
    "VAR pplfair - Most people try to take advantage of you, or try to be fair\n",
    "ESCALA DE 0 A 10\n",
    "//77\tRefusal*-DISCARDED\n",
    "//88\tDon't know*-DISCARDED\n",
    "//99\tNo answer*-DISCARDED\n",
    "\n",
    "Resumen columnas/variables 5, BLOQUE 1: df_rd['netusoft','nwsptot','ppltrst','rdtot','tvtot','pplfair'], eliminaremos los valores 7,8,9 y 77,88,99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECCION VARIABLES BLOQUE 2 (Político):\n",
    "\n",
    "Las siguientes columnas faltan en el DataFrame: {'dclaid B1', 'ccnthum', 'rdtot', 'nwsptot', 'dclagr', 'tvtot', 'dmcntov', 'dclwlfr', 'blgetmg', 'ginveco', 'dclenv', 'lawobey', 'dclcrm', 'dclmig', 'vteumbgb'}\n",
    "\n",
    "VAR actrolga - Able to take active role in political group(Variable Target BLOQUE 2)\n",
    "Value\tCategory\n",
    "1\tNot at all able\n",
    "2\tA little able\n",
    "3\tQuite able\n",
    "4\tVery able\n",
    "5\tCompletely able\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "\n",
    "VAR bctprd - Boycotted certain products last 12 months\n",
    "Value\tCategory\n",
    "1\tYes\n",
    "2\tNo\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "\n",
    "VAR dclagr - Preferred decision level of agricultural policies\n",
    "Value\tCategory\n",
    "1\tInternational level\n",
    "2\tEuropean level\n",
    "3\tNational level\n",
    "4\tRegional or local level\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "\n",
    "VAR dclaid - Preferred decision level of policies about aid to developing countries\n",
    "Value\tCategory\n",
    "1\tInternational level\n",
    "2\tEuropean level\n",
    "3\tNational level\n",
    "4\tRegional or local level\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer\n",
    "\n",
    "VAR dclcrm - Preferred decision level of fighting against organised crime policies\n",
    "Value\tCategory\n",
    "1\tInternational level\n",
    "2\tEuropean level\n",
    "3\tNational level\n",
    "4\tRegional or local level\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "\n",
    "VAR dclenv - Preferred decision level of environmental protection policies\n",
    "Value\tCategory\n",
    "1\tInternational level\n",
    "2\tEuropean level\n",
    "3\tNational level\n",
    "4\tRegional or local level\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "\n",
    "VAR dclmig - Preferred decision level of immigration and refugees policies\n",
    "Value\tCategory\n",
    "1\tInternational level\n",
    "2\tEuropean level\n",
    "3\tNational level\n",
    "4\tRegional or local level\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "\n",
    "VAR dclwlfr - Preferred decision level of social welfare policies\n",
    "Value\tCategory\n",
    "1\tInternational level\n",
    "2\tEuropean level\n",
    "3\tNational level\n",
    "4\tRegional or local level\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "\n",
    "VAR dmcntov - How democratic [country] is overall(Target Var num 2 Bloque 2.)\n",
    "Value\tCategory\n",
    "0\tNot at all democratic\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tCompletely democratic\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "VAR euftf - European Union: European unification go further or gone too far\n",
    "Value\tCategory\n",
    "0\tUnification already gone too far\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tUnification go further\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "VAR  gincdif - Government should reduce differences in income levels\n",
    "Value\tCategory\n",
    "1\tAgree strongly\n",
    "2\tAgree\n",
    "3\tNeither agree nor disagree\n",
    "4\tDisagree\n",
    "5\tDisagree strongly\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "VAR  ginveco - The less government intervenes in economy, the better for country\n",
    "Value\tCategory\n",
    "1\tAgree strongly\n",
    "2\tAgree\n",
    "3\tNeither agree nor disagree\n",
    "4\tDisagree\n",
    "5\tDisagree strongly\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "VAR lawobey - The law should always be obeyed\n",
    "Value\tCategory\n",
    "1\tAgree strongly\n",
    "2\tAgree\n",
    "3\tNeither agree nor disagree\n",
    "4\tDisagree\n",
    "5\tDisagree strongly\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "\n",
    "VAR  lrscale - Placement on left right scale(Posible var target numero 3 del Bloque 2)\n",
    "Value\tCategory\n",
    "0\tLeft\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tRight\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "VAR polintr - How interested in politics\n",
    "Value\tCategory\n",
    "1\tVery interested\n",
    "2\tQuite interested\n",
    "3\tHardly interested\n",
    "4\tNot at all interested\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "\n",
    "Var stfeco - How satisfied with present state of economy in country\n",
    "Value\tCategory\n",
    "0\tExtremely dissatisfied\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tExtremely satisfied\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var  stfedu - State of education in country nowadays\n",
    "Value\tCategory\n",
    "0\tExtremely bad\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tExtremely good\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var stfgov - How satisfied with the national government\n",
    "Value\tCategory\n",
    "0\tExtremely dissatisfied\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tExtremely satisfied\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var stfhlth - State of health services in country nowadays\n",
    "0\tExtremely bad\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tExtremely good\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var stflife - How satisfied with life as a whole\n",
    "Value\tCategory\n",
    "0\tExtremely dissatisfied\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tExtremely satisfied\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var trstplc - Trust in the police\n",
    "Value\tCategory\n",
    "0\tNo trust at all\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tComplete trust\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var trstplt - Trust in politicians\n",
    "Value\tCategory\n",
    "0\tNo trust at all\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tComplete trust\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "\n",
    "Var trstun - Trust in the United Nations\n",
    "Value\tCategory\n",
    "0\tNo trust at all\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tComplete trust\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var trstsci - Trust in scientists\n",
    "Value\tCategory\n",
    "0\tNo trust at all\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tComplete trust\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var imbgeco - Immigration bad or good for country's economy\n",
    "Value\tCategory\n",
    "0\tBad for the economy\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tGood for the economy\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "resumen columnas/variables 25, BLOQUE 2: df_rd['actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv', 'dclmig', 'dclwlfr', 'dmcntov', 'euftf', 'gincdif', 'ginveco', 'lawobey', 'lrscale', 'polintr','stfeco','stfedu', 'stfgov', 'stfhlth', 'stflife', 'trstplc', 'trstplt', 'trstun', 'trstsci', 'imbgeco']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECCION VARIABLES BLOQUE 3(Bienestar),  Bienestar social(Situación social del individuo, etnia, etc.):\n",
    "\n",
    "\n",
    "Var pplhlp - Most of the time people helpful or mostly looking out for themselves\n",
    "Would you say that most of the time people try to be helpful or that they are mostly looking out for themselves?\n",
    "\n",
    "Escala de 0 a 10.\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var aesfdrk - Feeling of safety of walking alone in local area after dark\n",
    "Value\tCategory\n",
    "1\tVery safe\n",
    "2\tSafe\n",
    "3\tUnsafe\n",
    "4\tVery unsafe\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "\n",
    "Var atchctr - How emotionally attached to [country]\n",
    "Value\tCategory\n",
    "0\tNot at all emotionally attached\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tVery emotionally attached\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var atcherp - How emotionally attached to Europe\n",
    "Value\tCategory\n",
    "0\tNot at all emotionally attached\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tVery emotionally attached\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var blgetmg - Belong to minority ethnic group in country\n",
    "Value\tCategory\n",
    "1\tYes\n",
    "2\tNo\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "\n",
    "Var brncntr - Born in country //\"IMPORTANTISIMO ESTE DATO, SERA EL PRINCIPAL FILTRO DEL DATASET, SOLO ESTAMOS INTERESADOS EN LOS DATOS DE INDIVIDUOS NACIDOS EN EL MISMO ESTADO DE LA ZONA EURO.\"\"\n",
    "Value\tCategory\n",
    "1\tYes\n",
    "//2\tNo\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "\n",
    "Var dscrgnd - Discrimination of respondent's group: gender\n",
    "Value\tCategory\n",
    "0\tNot marked\n",
    "1\tMarked\n",
    "\n",
    "Var dscrrlg - Discrimination of respondent's group: religion\n",
    "Value\tCategory\n",
    "0\tNot marked\n",
    "1\tMarked\n",
    "\n",
    "Var facntr - Father born in country , VARIABLE CRUCIAL E INFLUYENTE EN LAS OTRAS(Posible variable target num 1 bloque 3)\n",
    "Value\tCategory\n",
    "1\tYes\n",
    "2\tNo\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "\n",
    "Var happy - How happy are you (Posible variable target num 2 Bloque 3)\n",
    "Value\tCategory\n",
    "0\tExtremely unhappy\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tExtremely happy\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var health - Subjective general health\n",
    "Value\tCategory\n",
    "1\tVery good\n",
    "2\tGood\n",
    "3\tFair\n",
    "4\tBad\n",
    "5\tVery bad\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "\n",
    "Var inprdsc - How many people with whom you can discuss intimate and personal matters\n",
    "Value\tCategory\n",
    "0\tNone\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4-6\n",
    "5\t7-9\n",
    "6\t10 or more\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var mocntr - Mother born in country VARIABLE CRUCIAL E INFLUYENTE EN LAS OTRAS\n",
    "Value\tCategory\n",
    "1\tYes\n",
    "2\tNo\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "\n",
    "Var rlgdgr - How religious are you\n",
    "Value\tCategory\n",
    "0\tNot at all religious\n",
    "1\t1\n",
    "2\t2\n",
    "3\t3\n",
    "4\t4\n",
    "5\t5\n",
    "6\t6\n",
    "7\t7\n",
    "8\t8\n",
    "9\t9\n",
    "10\tVery religious\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var ccnthum - Climate change caused by natural processes, human activity, or both\n",
    "Value\tCategory\n",
    "1\tEntirely by natural processes\n",
    "2\tMainly by natural processes\n",
    "3\tAbout equally by natural processes and human activity\n",
    "4\tMainly by human activity\n",
    "5\tEntirely by human activity\n",
    "55\tI don't think climate change is happening\n",
    "//66\tNot applicable*\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var sclmeet - How often socially meet with friends, relatives or colleagues\n",
    "Value\tCategory\n",
    "1\tNever\n",
    "2\tLess than once a month\n",
    "3\tOnce a month\n",
    "4\tSeveral times a month\n",
    "5\tOnce a week\n",
    "6\tSeveral times a week\n",
    "7\tEvery day\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var vteumbgb - Would vote for [country] to remain member of European Union or leave, United Kingdom (Posible Var target num 3 del bloque 3)\n",
    "Value\tCategory\n",
    "1\tRemain a member of the European Union\n",
    "2\tLeave the European Union\n",
    "33\tWould submit a blank ballot paper\n",
    "44\tWould spoil the ballot paper\n",
    "55\tWould not vote in EU referendum\n",
    "65\tNot eligible/registered to vote\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var orig - determinar_origen Variable creada como combinacion de 'brncntr' 'facntr' 'mocntr'\n",
    "Value\tCategory\n",
    "1 Nacional ambos padres\n",
    "2 Mixto, almenos un padre extranjero'\n",
    "3 'Extranjero'\n",
    "\n",
    "Resumen columnas/variables 16, del Bloque 3: df_rd['aesfdrk','atchctr','atcherp','blgetmg','brncntr','dscrgnd','dscrrlg','facntr','happy','health','inprdsc','mocntr','rlgdgr','ccnthum','sclmeet','vteumbgb']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VARIABLES BLOQUE 4(VALORES HUMANOS, INQUIETUDES):\n",
    "EL VALOR 1 DE CADA VARIABLE ES EL MODELO DE INDIVIDUO PERFECTO A SEGUIR, moral y eticamente correcto.\n",
    "\n",
    "Var impenv/impenva - Important to care for nature and environment\n",
    "Value\tCategory\n",
    "1\tVery much like me\n",
    "2\tLike me\n",
    "3\tSomewhat like me\n",
    "4\tA little like me\n",
    "5\tNot like me\n",
    "6\tNot like me at all\n",
    "//66\tNot applicable*\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var impfree/impfreea - Important to make own decisions and be free (Posible var target num 1 Bloque 4)\n",
    "Value\tCategory\n",
    "1\tVery much like me\n",
    "2\tLike me\n",
    "3\tSomewhat like me\n",
    "4\tA little like me\n",
    "5\tNot like me\n",
    "6\tNot like me at all\n",
    "//66\tNot applicable*\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var impfun/impfuna - Important to seek fun and things that give pleasure\n",
    "Value\tCategory\n",
    "1\tVery much like me\n",
    "2\tLike me\n",
    "3\tSomewhat like me\n",
    "4\tA little like me\n",
    "5\tNot like me\n",
    "6\tNot like me at all\n",
    "//66\tNot applicable*\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var imprich/impricha - Important to be rich, have money and expensive things\n",
    "Value\tCategory\n",
    "1\tVery much like me\n",
    "2\tLike me\n",
    "3\tSomewhat like me\n",
    "4\tA little like me\n",
    "5\tNot like me\n",
    "6\tNot like me at all\n",
    "66\tNot applicable*\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var impsafe/impsafea - Important to live in secure and safe surroundings\n",
    "Value\tCategory\n",
    "1\tVery much like me\n",
    "2\tLike me\n",
    "3\tSomewhat like me\n",
    "4\tA little like me\n",
    "5\tNot like me\n",
    "6\tNot like me at all\n",
    "//66\tNot applicable*\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var imptrad/imptrada - Important to follow traditions and customs\n",
    "Value\tCategory\n",
    "1\tVery much like me\n",
    "2\tLike me\n",
    "3\tSomewhat like me\n",
    "4\tA little like me\n",
    "5\tNot like me\n",
    "6\tNot like me at all\n",
    "//66\tNot applicable*\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var ipadvnt/ipadvnta - Important to seek adventures and have an exciting life\n",
    "Value\tCategory\n",
    "1\tVery much like me\n",
    "2\tLike me\n",
    "3\tSomewhat like me\n",
    "4\tA little like me\n",
    "5\tNot like me\n",
    "6\tNot like me at all\n",
    "//66\tNot applicable*\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var ipcrtiv/ipcrtiva - Important to think new ideas and being creative\n",
    "Value\tCategory\n",
    "1\tVery much like me\n",
    "2\tLike me\n",
    "3\tSomewhat like me\n",
    "4\tA little like me\n",
    "5\tNot like me\n",
    "6\tNot like me at all\n",
    "//66\tNot applicable*\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var ipeqop/ipeqopta - Important that people are treated equally and have equal opportunities\n",
    "Value\tCategory\n",
    "1\tVery much like me\n",
    "2\tLike me\n",
    "3\tSomewhat like me\n",
    "4\tA little like me\n",
    "5\tNot like me\n",
    "6\tNot like me at all\n",
    "//66\tNot applicable*\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var iplylfr/iplylfra - Important to be loyal to friends and devote to people close (Posible var target num 2 Bloque 4)\n",
    "Value\tCategory\n",
    "1\tVery much like me\n",
    "2\tLike me\n",
    "3\tSomewhat like me\n",
    "4\tA little like me\n",
    "5\tNot like me\n",
    "6\tNot like me at all\n",
    "//66\tNot applicable*\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var ipmodst/ipmodsta - Important to be humble and modest, not draw attention\n",
    "Value\tCategory\n",
    "1\tVery much like me\n",
    "2\tLike me\n",
    "3\tSomewhat like me\n",
    "4\tA little like me\n",
    "5\tNot like me\n",
    "6\tNot like me at all\n",
    "//66\tNot applicable*\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Var ipudrst/ipudrsta - Important to understand different people\n",
    "Value\tCategory\n",
    "1\tVery much like me\n",
    "2\tLike me\n",
    "3\tSomewhat like me\n",
    "4\tA little like me\n",
    "5\tNot like me\n",
    "6\tNot like me at all\n",
    "//66\tNot applicable*\n",
    "//77\tRefusal*\n",
    "//88\tDon't know*\n",
    "//99\tNo answer*\n",
    "\n",
    "Resumen columnas/variables 11, del Bloque 4: df_rd['impenva','impfreea','impfuna','impricha','impsafe','imptrada','ipadvnta','ipcrtiva','iplylfra','ipmodsta','ipudrsta']\n",
    "Se eliminaran muestras con los valores 66,77,88,99 en el bloque 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumen columnas/variables 5, BLOQUE 1: df_rd['netusoft','nwsptot','ppltrst','rdtot','tvtot']\n",
    "Resumen columnas/variables 25, BLOQUE 2: df_rd['actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv','dclmig','dclwlfr','dmcntov','euftf','gincdif','ginveco','lawobey','lrscale','polintr','stfeco','stfedu','stfgov','stfhlth','stflife','trstplc','trstplt','trstun','trstsci','imbgeco']\n",
    "Resumen columnas/variables 16, del Bloque 3: df_rd['aesfdrk','atchctr','atcherp','blgetmg','brncntr','dscrgnd','dscrrlg','facntr','happy','health','inprdsc','mocntr','rlgdgr','ccnthum','sclmeet','vteumbgb']\n",
    "Resumen columnas/variables 11, del Bloque 4: df_rd['impenva','impfreea','impfuna','impricha','impsafea','imptrada','ipadvnta','ipcrtiva','iplylfra','ipmodsta','ipudrsta']\n",
    "Total variables del Dataset 57.\n",
    "Limites y filtros del Dataset a tener en cuenta:\n",
    "Var brncntr - Born in country \"IMPORTANTISIMO ESTE DATO, SERA EL PRINCIPAL FILTRO DEL DATASET, SOLO ESTAMOS INTERESADOS EN LOS DATOS DE INDIVIDUOS NACIDOS EN EL MISMO ESTADO DE LA ZONA EURO.\"\"\n",
    "Var facntr - Father born in country , VARIABLE CRUCIAL E INFLUYENTE EN LAS OTRAS(Posible variable target num 1 bloque 3)\n",
    "Var mocntr - Mother born in country VARIABLE CRUCIAL E INFLUYENTE EN LAS OTRAS, se observara por separado\n",
    "ELIMINAREMOS MUESTRAS CON DATOS INCONCLUSOS.\n",
    "CUANDO ESTUDIEMOS Y COMPAREMOS DATOS EN FUNCION DEL PAIS DE LA ZONA EURO, SE LE DARA EL MISMO FORMATO AL SAMPLE, ANTES DEL FEATURE ENGINEERING.\n",
    "Lista de columnas totales:\n",
    "['netusoft','nwsptot','ppltrst','rdtot','tvtot','actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv','dclmig','dclwlfr','dmcntov','euftf','gincdif','ginveco','lawobey','lrscale','polintr','stfeco','stfedu','stfgov','stfhlth','stflife','trstplc','trstplt','trstun','trstsci','imbgeco','aesfdrk','atchctr','atcherp','blgetmg','brncntr','dscrgnd','dscrrlg','facntr','happy','health','inprdsc','mocntr','rlgdgr','ccnthum','sclmeet','vteumbgb','impenva','impfreea','impfuna','impricha','impsafea','imptrada','ipadvnta','ipcrtiva','iplylfra','ipmodsta','ipudrsta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "import zipfile\n",
    "\n",
    "def descomprimir_zip(ruta_zip, ruta_destino):\n",
    "    \"\"\"Descomprime un archivo ZIP en la ruta especificada.\n",
    "\n",
    "    Args:\n",
    "        ruta_zip: La ruta al archivo ZIP.\n",
    "        ruta_destino: La ruta al directorio donde se extraerán los archivos.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(ruta_zip, 'r') as archivo_zip:\n",
    "        archivo_zip.extractall(ruta_destino)\n",
    "\n",
    "# Ejemplo de uso\n",
    "ruta_zip = 'C:/GitHubRepos/ProyectoFinal/data/Deprecated/ZonaEuroV2.zip'\n",
    "ruta_destino = 'C:/GitHubRepos/ProyectoFinal/data/raw/'\n",
    "descomprimir_zip(ruta_zip, ruta_destino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rd= pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/raw/ZonaEuroV2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAMOS A COMPROBAR SI FALTA ALGUNA COLUMNA O SI TIENE COLUMNAS DE MÁS:\n",
    "\n",
    "\n",
    "# Lista de columnas esperadas\n",
    "columnas_esperadas = ['cntry','netusoft','nwsptot','ppltrst','rdtot','tvtot','actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv','dclmig','dclwlfr','dmcntov',\n",
    "                      'euftf','gincdif','ginveco','lawobey','lrscale','polintr','stfeco','stfedu','stfgov','stfhlth','stflife','trstplc','trstplt',\n",
    "                      'trstun','trstsci','imbgeco','aesfdrk','atchctr','atcherp','blgetmg','brncntr','dscrgnd','dscrrlg','facntr','happy','health','inprdsc',\n",
    "                      'mocntr','rlgdgr','ccnthum','sclmeet','vteumbgb','impenva','impfreea','impfuna','impricha','impsafea','imptrada','ipadvnta','ipcrtiva',\n",
    "                      'iplylfra','ipmodsta','ipudrsta']\n",
    "\n",
    "# Convertir la lista de columnas del DataFrame a una lista para facilitar la comparación\n",
    "columnas_actuales = df_rd.columns.tolist()\n",
    "\n",
    "# Identificar columnas faltantes\n",
    "columnas_faltantes = set(columnas_esperadas) - set(columnas_actuales)\n",
    "\n",
    "# Identificar columnas adicionales\n",
    "columnas_adicionales = set(columnas_actuales) - set(columnas_esperadas)\n",
    "\n",
    "# Imprimir resultados\n",
    "if columnas_faltantes:\n",
    "    print(\"Las siguientes columnas faltan en el DataFrame:\", columnas_faltantes)\n",
    "else:\n",
    "    print(\"Todas las columnas esperadas están presentes.\")\n",
    "\n",
    "if columnas_adicionales:\n",
    "    print(\"El DataFrame contiene las siguientes columnas adicionales:\", columnas_adicionales)\n",
    "else:\n",
    "    print(\"El DataFrame no tiene columnas adicionales.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHEQUEAMOS MANUALMENTE EL DATAFRAME CON DATA WRANGLER Y ELIMINAMOS COLUMNAS QUE NO INTERESEN. las demas las añadimos a la lista.\n",
    "del df_rd['name'], df_rd['essround'], df_rd['edition'], df_rd['proddate'], df_rd['idno'], df_rd['dweight'], df_rd['pspwght'], df_rd['anweight']\n",
    "del df_rd['prob'], df_rd['stratum'], df_rd['psu'], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#volvemos a pasar el filtro para ver las variables que hay demás.\n",
    "#VAMOS A COMPROBAR SI FALTA ALGUNA COLUMNA O SI TIENE COLUMNAS DE MÁS:\n",
    "\n",
    "\n",
    "# Lista de columnas esperadas\n",
    "columnas_esperadas = ['cntry','netusoft','nwsptot','ppltrst','rdtot','tvtot','actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv','dclmig','dclwlfr','dmcntov',\n",
    "                      'euftf','gincdif','ginveco','lawobey','lrscale','polintr','stfeco','stfedu','stfgov','stfhlth','stflife','trstplc','trstplt',\n",
    "                      'trstun','trstsci','imbgeco','aesfdrk','atchctr','atcherp','blgetmg','brncntr','dscrgnd','dscrrlg','facntr','happy','health','inprdsc',\n",
    "                      'mocntr','rlgdgr','ccnthum','sclmeet','vteumbgb','impenva','impfreea','impfuna','impricha','impsafea','imptrada','ipadvnta','ipcrtiva',\n",
    "                      'iplylfra','ipmodsta','ipudrsta','pplhlp', 'impsafe' ]\n",
    "\n",
    "# Convertir la lista de columnas del DataFrame a una lista para facilitar la comparación\n",
    "columnas_actuales = df_rd.columns.tolist()\n",
    "\n",
    "# Identificar columnas faltantes\n",
    "columnas_faltantes = set(columnas_esperadas) - set(columnas_actuales)\n",
    "\n",
    "# Identificar columnas adicionales\n",
    "columnas_adicionales = set(columnas_actuales) - set(columnas_esperadas)\n",
    "\n",
    "# Imprimir resultados\n",
    "if columnas_faltantes:\n",
    "    print(\"Las siguientes columnas faltan en el DataFrame:\", columnas_faltantes)\n",
    "else:\n",
    "    print(\"Todas las columnas esperadas están presentes.\")\n",
    "\n",
    "if columnas_adicionales:\n",
    "    print(\"El DataFrame contiene las siguientes columnas adicionales:\", columnas_adicionales)\n",
    "else:\n",
    "    print(\"El DataFrame no tiene columnas adicionales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columnas para combinar, masculino y femenino: \n",
    "impsafe/impsafea. ipcrtiv/ipcrtiva. iplylfr/iplylfra,  impenv/impenva, imprich/impricha, \n",
    "imptrad/imptrada,  impenv/impenva, impfree/impfreea, impfun/impfuna, 'ipeqopt':'ipeqopta',\n",
    "'ipmodst':'ipmodsta', 'ipudrst':'ipudrsta',\n",
    "\n",
    "Predominara el valor del femenino(Encuestas mas recientes.)\n",
    "\n",
    "Actualizaremos la lista de Columnas esperadas conforme vayamos comprobandolas.\n",
    "# Lista de columnas esperadas V2\n",
    "columnas_esperadas = ['cntry','netusoft','nwsptot','ppltrst','rdtot','tvtot','actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv','dclmig','dclwlfr','dmcntov',\n",
    "                      'euftf','gincdif','ginveco','lawobey','lrscale','polintr','stfeco','stfedu','stfgov','stfhlth','stflife','trstplc','trstplt',\n",
    "                      'trstun','trstsci','imbgeco','aesfdrk','atchctr','atcherp','blgetmg','brncntr','dscrgnd','dscrrlg','facntr','happy','health','inprdsc',\n",
    "                      'mocntr','rlgdgr','ccnthum','sclmeet','vteumbgb','impenva','impfreea','impfuna','impricha','impsafea','imptrada','ipadvnta','ipcrtiva',\n",
    "                      'iplylfra','ipmodsta','ipudrsta','pplhlp', 'impsafe','pplfair' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del df_rd['nwsppol'],df_rd['pweight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rellenamos los datos faltantes teniendo en cuenta los pares de variables que son la misma categoria pero de tiradas de encuestas\n",
    "#De diferente fecha.\n",
    "pares_variables = {\n",
    "    'impsafe': 'impsafea',\n",
    "    'ipcrtiv': 'ipcrtiva',\n",
    "    'iplylfr': 'iplylfra',\n",
    "    'impenv': 'impenva',\n",
    "    'imprich': 'impricha',\n",
    "    'imptrad': 'imptrada',\n",
    "    'impfree': 'impfreea',\n",
    "    'impfun':'impfuna',\n",
    "    'ipadvnt':'ipadvnta',\n",
    "    'ipeqopt':'ipeqopta',\n",
    "    'ipmodst':'ipmodsta',\n",
    "    'ipudrst':'ipudrsta',\n",
    "}\n",
    "def actualizar_variable(row, masculina, femenina):\n",
    "    if pd.notna(row[femenina]):\n",
    "        return row[femenina]\n",
    "    elif pd.notna(row[masculina]):\n",
    "        return row[masculina]\n",
    "    else:\n",
    "        return\n",
    "for masculina, femenina in pares_variables.items():\n",
    "    df_rd[femenina] = df_rd.apply(lambda row: actualizar_variable(row, masculina, femenina), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora que hemos aplicado la actualizacion de datos antiguos en variables actualizadas borramos todas las columnas obsoletas.\n",
    "columnas_a_eliminar = ['impsafe', 'ipcrtiv', 'iplylfr', 'impenv', 'imprich', 'imptrad', 'impfree', 'impfun', 'ipadvnt', 'ipeqopt', 'ipmodst', 'ipudrst','nwspol']\n",
    "df_rd = df_rd.drop(columnas_a_eliminar, axis=1) # axis=1 indica que se eliminan columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Volvemos a pasar el comparador de columnas, actualizado.\n",
    "columnas_esperadas = ['cntry','netusoft','nwsptot','ppltrst','rdtot','tvtot','actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv','dclmig','dclwlfr','dmcntov',\n",
    "                      'euftf','gincdif','ginveco','lawobey','lrscale','polintr','stfeco','stfedu','stfgov','stfhlth','stflife','trstplc','trstplt',\n",
    "                      'trstun','trstsci','imbgeco','aesfdrk','atchctr','atcherp','blgetmg','brncntr','dscrgnd','dscrrlg','facntr','happy','health','inprdsc',\n",
    "                      'mocntr','rlgdgr','ccnthum','sclmeet','vteumbgb','impenva','impfreea','impfuna','impricha','imptrada','ipadvnta','ipcrtiva',\n",
    "                      'iplylfra','ipmodsta','ipudrsta','pplhlp', 'impsafea','pplfair','ipeqopta']\n",
    "# Convertir la lista de columnas del DataFrame a una lista para facilitar la comparación\n",
    "columnas_actuales = df_rd.columns.tolist()\n",
    "\n",
    "# Identificar columnas faltantes\n",
    "columnas_faltantes = set(columnas_esperadas) - set(columnas_actuales)\n",
    "\n",
    "# Identificar columnas adicionales\n",
    "columnas_adicionales = set(columnas_actuales) - set(columnas_esperadas)\n",
    "\n",
    "# Imprimir resultados\n",
    "if columnas_faltantes:\n",
    "    print(\"Las siguientes columnas faltan en el DataFrame:\", columnas_faltantes)\n",
    "else:\n",
    "    print(\"Todas las columnas esperadas están presentes.\")\n",
    "\n",
    "if columnas_adicionales:\n",
    "    print(\"El DataFrame contiene las siguientes columnas adicionales:\", columnas_adicionales)\n",
    "else:\n",
    "    print(\"El DataFrame no tiene columnas adicionales.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a comprobar los valores unicos de la columna 'cntry' para pasar el diccionario de nuestro dataset economico y agrupar muestras de paises por clusters\n",
    "#de razones economicas y geograficas, asi podremos rellenar datos faltantes por la moda de cada cluster.\n",
    "paises_unicos = df_rd['cntry'].unique()\n",
    "print(paises_unicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora vamos a renombrar nuestra columna cntry con los nombres de los paises completos creando una nueva columna 'País'\n",
    "\n",
    "PaisesEuro = {\n",
    "    'AT': 'Austria','BE': 'Bélgica','CH': 'Suiza','CZ': 'Chequia','DE': 'Alemania','ES': 'España','FI': 'Finlandia','FR': 'Francia',\n",
    "    'GB': 'Inglaterra','GR': 'Grecia','HU': 'Hungría','IE': 'Irlanda','IL': 'Israel','IT': 'Italia','LU': 'Luxemburgo','NL': 'Países Bajos',\n",
    "    'NO': 'Noruega','PL': 'Polonia','PT': 'Portugal','SE': 'Suecia','SI': 'Eslovenia','EE': 'Estonia','IS': 'Islandia','SK': 'República Eslovaca',\n",
    "    'TR': 'Turquia','UA': 'Ucrania','BG': 'Bulgaria','CY': 'Chipre','RU': 'Rusia','HR': 'Croacia','LV': 'Letonia','RO': 'Rumania','LT': 'Lituania',\n",
    "    'AL': 'Albania','XK': 'Kosovo','ME': 'Montenegro','RS': 'Serbia','MK': 'Macedonia'\n",
    "}\n",
    "\n",
    "PaisesEuro2 = {\n",
    "    'AT': 1,  'BE': 2,  'CH': 3,  'CZ': 4,  'DE': 5,  'ES': 6, 'FI': 7,  'FR': 8, 'GB': 9,'GR': 10, 'HU': 11, \n",
    "    'IE': 12, 'IL': 13, 'IT': 14, 'LU': 15, 'NL': 16, 'NO': 17, 'PL': 18, 'PT': 19,'SE': 20, 'SI': 21, 'EE': 22, 'IS': 23, \n",
    "    'SK': 24, 'TR': 25, 'UA': 26, 'BG': 27, 'CY': 28, 'RU': 29, 'HR': 30, 'LV': 31, 'RO': 32, 'LT': 33, 'AL': 34, 'XK': 35, \n",
    "    'ME': 36, 'RS': 37, 'MK': 38  \n",
    "}\n",
    "df_rd['País'] = df_rd['cntry'].map(PaisesEuro)\n",
    "df_rd['P_fc'] = df_rd['cntry'].map(PaisesEuro2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agrupamos los paises del dataframe df_rd por los clusters de nuestro dataset economico df1\n",
    "paises_cluster = {\n",
    "    'Dinamarca': 0,'Luxemburgo': 0,'Irlanda': 0,'Finlandia': 0,'Bélgica': 0,'Francia': 0,'Alemania': 0, 'Suecia': 0,'Austria': 0,'Países Bajos': 0,\n",
    "    'Chipre': 1,'Malta': 1,'Grecia': 1,'Letonia': 1,'Portugal': 1,'Bulgaria': 1,'Estonia': 1,'República Checa': 1,'Lituania': 1,'Polonia': 1,'Eslovenia': 1,'República Eslovaca': 1,\n",
    "    'Croacia': 1,'Rumania': 1,'Hungría': 1,\n",
    "    'Unión Europea': 2,'Italia': 2,'España': 2\n",
    "}\n",
    "df_rd['GroupCntry'] = df_rd['País'].map(lambda pais: paises_cluster.get(pais, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a checkear los valores unicos de pais cuando el valor unico de GroupCntry es 3\n",
    "df_cluster_3 = df_rd[df_rd['GroupCntry'] == 3]\n",
    "paises_cluster_3 = df_cluster_3['País'].unique()\n",
    "print(list(paises_cluster_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_rd['cntry']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LO MAS IMPORTANTE DE ESTE TRABAJO , SABER LOS FACTORES LIMITANTES DE EL MATERIAL QUE DISPONEMOS, ESTE DATASET TIENE DOS CONTRAS, TIENE MUCHAS VARIABLES\n",
    "CON DATOS FALTANTES EN VALOR CLAVE DEL PAIS DE DONDE SE HAN REALIZADO LAS ENCUESTAS, DADO QUE ESTAS SE HAN IDO ACTUALIZANDO POR TIRADAS Y\n",
    "TIENE VARIABLES OBSOLETAS Y/O COMBINABLES O IGUAL A OTRAS.\n",
    "LO LOGICO AHORA ES SABER CUAL DE NUESTRO CLUSTER NOS LIMITA LA CLASIFICACION, A PARTIR DE AHI VAMOS A BALANCEAR EL DATASET, LIMITANDO LAS MUESTRAS\n",
    "POR EL CLUSTER CON MENOS CANTIDAD DE MUESTRAS, A PARTIR DE AHI EMPEZAREMOS A ARREGLAR LAS COLUMNAS CON DATOS FALTANTES QUE SE PUEDAN ARREGLAR EN FUNCION\n",
    "DEL PORCENAJE DE DATOS FALTANTES, SI NO SE ELIMINARAN\n",
    "IMPORTANTE PARA NO DISTORSIONAR EL DATASET NI SESGARLO HEMOS DE STRATIFICAR EL SAMPLE RESPETANDO CLASES MINORITARIAS.\n",
    "POR ESO ERA IMPORTANTE REALIZAR CLUSTERIZACION DE PAISES CERCANOS GEOGRAFICAMENTE Y CON SIMILITUDES POLITICAS/ECONOMICAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRIMER PASO ELIMINAMOS VALORES UNICOS QUE HACEN REFERENCIA A LA INCONCLUSION CATEGORICA DE LA VARIABLE (VALORES INUTILES COMO NO LO SÉ, PREFIERO NO DECIRLO, ETC.)\n",
    "#Se eliminaran todas las muestras con los valores 66,77,88,99 en todos los bloques\n",
    "EliminarValores1 = [66, 77, 88, 99]\n",
    "df_rd = df_rd[~df_rd.isin(EliminarValores1).any(axis=1)]\n",
    "df_rd.reset_index(drop=True, inplace=True)\n",
    "df_rd.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como hay valores unicos validos en algunas de las columnas como 7,8 y 9. habra que filtrar manualmente las variables en cuestion\n",
    "#BLOQUE 1: df_clean1['netusoft','nwsptot','ppltrst','rdtot','tvtot']\n",
    "v1 = [7, 8, 9]\n",
    "# 1. Usar .isin() para cada columna individualmente y luego combinar las condiciones con OR\n",
    "mask = (df_rd['netusoft'].isin(v1)) | (df_rd['nwsptot'].isin(v1)) | (df_rd['ppltrst'].isin(v1)) | (df_rd['rdtot'].isin(v1)) | (df_rd['tvtot'].isin(v1))\n",
    "# 2. Invertir la máscara para seleccionar las filas que NO contienen los valores en v1\n",
    "df_rd = df_rd[~mask]\n",
    "# 3. Resetear el índice (opcional, pero recomendado)\n",
    "df_rd.reset_index(drop=True, inplace=True)\n",
    "# Imprimir las primeras filas para verificar\n",
    "print(df_rd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOQUE 2\n",
    "#Como hay valores unicos validos en algunas de las columnas como 7,8 y 9. habra que filtrar manualmente las variables en cuestion\n",
    "v1 = [7, 8, 9]\n",
    "B2L = ['actrolga', 'bctprd', 'dclagr', 'dclaid', 'dclcrm', 'dclenv', 'dclmig', 'dclwlfr', 'dmcntov', 'euftf', 'gincdif', 'ginveco', 'lawobey', 'lrscale', 'polintr', 'stfeco', 'stfedu', 'stfgov', 'stfhlth', 'stflife', 'trstplc', 'trstplt', 'trstun', 'trstsci', 'imbgeco']\n",
    "mask = df_rd[B2L].isin(v1).any(axis=1)\n",
    "df_rd = df_rd[~mask]\n",
    "df_rd.reset_index(drop=True, inplace=True)\n",
    "print(df_rd.head())\n",
    "'''\n",
    "VAR actrolga - Able to take active role in political group(Posible variable Target BLOQUE 2)\n",
    "Value\tCategory\n",
    "1\tNot at all able\n",
    "2\tA little able\n",
    "3\tQuite able\n",
    "4\tVery able\n",
    "5\tCompletely able\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "'''\n",
    "#Como se observa en una de las variables filtradas, los valores unicos superiores o iguales a 6, que hacen referencia a valores no validos han sido eliminados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOQUE 3\n",
    "B3L = ['aesfdrk','blgetmg','brncntr','dscrgnd','dscrrlg','facntr','health','mocntr']\n",
    "mask = df_rd[B3L].isin(v1).any(axis=1)\n",
    "df_rd = df_rd[~mask]\n",
    "df_rd.reset_index(drop=True, inplace=True)\n",
    "print(df_rd.head())\n",
    "'''Var aesfdrk - Feeling of safety of walking alone in local area after dark\n",
    "Value\tCategory\n",
    "1\tVery safe\n",
    "2\tSafe\n",
    "3\tUnsafe\n",
    "4\tVery unsafe\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "'''\n",
    "#Tal y como vemos en la descripcion de la variable factorizada, la eliminacion de datos invalidos ha sido exitosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rd.to_csv('C:/GitHubRepos/ProyectoFinal/data/raw/ZEV3.csv', index=False)\n",
    "%reset -f\n",
    "import pandas as pd\n",
    "df_rd = pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/raw/ZEV3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "n_muestras_por_cluster = 1182\n",
    "\n",
    "df_control = pd.DataFrame()\n",
    "\n",
    "for i in range(4):  # Iterar sobre los 4 clusters (0, 1, 2, 3)\n",
    "    df_cluster = df_rd[df_rd['GroupCntry'] == i]\n",
    "\n",
    "    # Si el cluster tiene menos de 1182 muestras, toma todas las muestras disponibles\n",
    "    n_muestras_cluster = min(n_muestras_por_cluster, len(df_cluster))\n",
    "\n",
    "    df_control_cluster = df_cluster.sample(n=n_muestras_cluster, random_state=42)\n",
    "    df_control = pd.concat([df_control, df_control_cluster], ignore_index=True)\n",
    "\n",
    "print(\"\\nDistribución de clusters en df_control:\")\n",
    "print(df_control['GroupCntry'].value_counts())\n",
    "\n",
    "# Eliminamos las muestras del grupo de control del dataframe original\n",
    "indices_control = df_control.index\n",
    "df_rd = df_rd.drop(indices_control)\n",
    "df_rd.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\nDistribución de clusters en df_rd:\")\n",
    "print(df_rd['GroupCntry'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a comprobar los valores unicos de paises agrupados por nuestro cluster de paises.\n",
    "unique_countries = df_control.groupby('GroupCntry')['País'].unique()\n",
    "print(unique_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a crear el cluster de control, una mezcla aleatoria de 295 muestras de cada uno de los cluster en un quinto, que sera el\n",
    "#que demuestre la hipotesis nula ya que no se agruparan los datos por razones o geograficas.\n",
    "import pandas as pd\n",
    "\n",
    "# ... (tu código anterior para cargar y procesar df_control) ...\n",
    "\n",
    "def balancear_y_crear_cluster5(df_control):\n",
    "\n",
    "    grupos_unicos = df_control['GroupCntry'].unique().tolist() # Guarda los grupos unicos en una lista\n",
    "\n",
    "    # Quitar 2 muestras aleatorias de cada clúster existente\n",
    "    for grupo in grupos_unicos: # Itera sobre la lista de grupos unicos\n",
    "        df_grupo = df_control[df_control['GroupCntry'] == grupo]\n",
    "        muestras_a_quitar = df_grupo.sample(n=2, replace=False)\n",
    "        df_control.drop(muestras_a_quitar.index, inplace=True)  # Modifica df_control directamente\n",
    "\n",
    "    # Crear el quinto clúster con muestras aleatorias de los otros clústeres\n",
    "    df_cluster5 = pd.DataFrame()\n",
    "    for grupo in grupos_unicos: # Itera sobre la lista de grupos unicos\n",
    "        df_grupo = df_control[df_control['GroupCntry'] == grupo]\n",
    "        muestras_para_cluster5 = df_grupo.sample(n=295, replace=False)\n",
    "        df_cluster5 = pd.concat([df_cluster5, muestras_para_cluster5], ignore_index=True)\n",
    "\n",
    "    df_cluster5['GroupCntry'] = 4  # Asigna el número de clúster 5\n",
    "\n",
    "    # Concatenar el nuevo clúster con el DataFrame original (que ya ha sido modificado)\n",
    "    df_control = pd.concat([df_control, df_cluster5], ignore_index=True)  # Sobreescribe df_control\n",
    "\n",
    "    return df_control  # Devuelve el DataFrame modificado (aunque se haya modificado directamente)\n",
    "\n",
    "df_control = balancear_y_crear_cluster5(df_control)\n",
    "\n",
    "# Verifica el tamaño de los clústeres\n",
    "print(df_control.groupby('GroupCntry').size())\n",
    "\n",
    "# Imprime los primeros registros del DataFrame modificado para verificar los cambios\n",
    "print(df_control.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_countries = df_control.groupby('GroupCntry')['País'].unique()\n",
    "print(unique_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El cluster de control de nuestro dataframe es el 5 , GroupCntry==4: ['Francia' 'Países Bajos' 'Alemania' 'Bélgica' 'Suecia' 'Austria'\n",
    " 'Portugal' 'Lituania' 'Eslovenia' 'Hungría' 'Bulgaria' 'Croacia'\n",
    " 'República Eslovaca' 'Estonia' 'Grecia' 'Polonia' 'Letonia' 'Chipre'\n",
    " 'España' 'Italia' 'Serbia' 'Chequia' 'Rusia' 'Ucrania' 'Israel']\n",
    "Contiene muestras de todos los clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_control.to_csv('C:/GitHubRepos/ProyectoFinal/data/raw/ZEV3.csv', index=False)\n",
    "%reset -f\n",
    "import pandas as pd\n",
    "df_control= pd.read_csv('C:/GitHubRepos/ProyectoFinal/data/raw/ZEV3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rango: 1-25%\n",
      "  Grupo 0:\n",
      "    - pplfair: 0.00%\n",
      "    - pplhlp: 0.00%\n",
      "    - ppltrst: 0.00%\n",
      "    - bctprd: 0.00%\n",
      "    - euftf: 17.88%\n",
      "    - gincdif: 0.00%\n",
      "    - lrscale: 0.00%\n",
      "    - polintr: 0.00%\n",
      "    - stfeco: 0.00%\n",
      "    - stfedu: 0.00%\n",
      "    - stfgov: 0.00%\n",
      "    - stfhlth: 0.00%\n",
      "    - stflife: 0.00%\n",
      "    - trstplc: 0.00%\n",
      "    - trstplt: 0.00%\n",
      "    - trstun: 0.00%\n",
      "    - imbgeco: 0.00%\n",
      "    - aesfdrk: 0.00%\n",
      "    - blgetmg: 20.68%\n",
      "    - brncntr: 0.00%\n",
      "    - dscrgnd: 0.00%\n",
      "    - dscrrlg: 0.00%\n",
      "    - facntr: 0.00%\n",
      "    - happy: 0.00%\n",
      "    - health: 0.00%\n",
      "    - mocntr: 0.00%\n",
      "    - rlgdgr: 0.00%\n",
      "    - sclmeet: 0.00%\n",
      "    - impenva: 14.75%\n",
      "    - impfreea: 14.75%\n",
      "    - impfuna: 14.75%\n",
      "    - impricha: 14.75%\n",
      "    - impsafea: 14.75%\n",
      "    - imptrada: 14.75%\n",
      "    - ipadvnta: 14.75%\n",
      "    - ipcrtiva: 14.75%\n",
      "    - ipeqopta: 14.75%\n",
      "    - iplylfra: 14.75%\n",
      "    - ipmodsta: 14.75%\n",
      "    - ipudrsta: 14.75%\n",
      "    - País: 0.00%\n",
      "    - P_fc: 0.00%\n",
      "    - GroupCntry: 0.00%\n",
      "  Grupo 1:\n",
      "    - pplfair: 0.00%\n",
      "    - pplhlp: 0.00%\n",
      "    - ppltrst: 0.00%\n",
      "    - bctprd: 0.00%\n",
      "    - euftf: 19.58%\n",
      "    - gincdif: 0.00%\n",
      "    - lrscale: 0.00%\n",
      "    - polintr: 0.00%\n",
      "    - stfeco: 0.00%\n",
      "    - stfedu: 0.00%\n",
      "    - stfgov: 0.00%\n",
      "    - stfhlth: 0.00%\n",
      "    - stflife: 0.00%\n",
      "    - trstplc: 0.00%\n",
      "    - trstplt: 0.00%\n",
      "    - trstun: 0.00%\n",
      "    - imbgeco: 0.00%\n",
      "    - aesfdrk: 0.00%\n",
      "    - brncntr: 0.00%\n",
      "    - dscrgnd: 0.00%\n",
      "    - dscrrlg: 0.00%\n",
      "    - facntr: 0.00%\n",
      "    - happy: 0.00%\n",
      "    - health: 0.00%\n",
      "    - mocntr: 0.00%\n",
      "    - rlgdgr: 0.00%\n",
      "    - sclmeet: 0.00%\n",
      "    - impenva: 4.66%\n",
      "    - impfreea: 4.66%\n",
      "    - impfuna: 4.66%\n",
      "    - impricha: 4.66%\n",
      "    - impsafea: 4.66%\n",
      "    - imptrada: 4.66%\n",
      "    - ipadvnta: 4.66%\n",
      "    - ipcrtiva: 4.66%\n",
      "    - ipeqopta: 4.66%\n",
      "    - iplylfra: 4.66%\n",
      "    - ipmodsta: 4.66%\n",
      "    - ipudrsta: 4.66%\n",
      "    - País: 0.00%\n",
      "    - P_fc: 0.00%\n",
      "    - GroupCntry: 0.00%\n",
      "  Grupo 2:\n",
      "    - pplfair: 0.00%\n",
      "    - pplhlp: 0.00%\n",
      "    - ppltrst: 0.00%\n",
      "    - bctprd: 0.00%\n",
      "    - euftf: 13.14%\n",
      "    - gincdif: 0.00%\n",
      "    - lrscale: 0.00%\n",
      "    - polintr: 0.00%\n",
      "    - stfeco: 0.00%\n",
      "    - stfedu: 0.00%\n",
      "    - stfgov: 0.00%\n",
      "    - stfhlth: 0.00%\n",
      "    - stflife: 0.00%\n",
      "    - trstplc: 0.00%\n",
      "    - trstplt: 0.00%\n",
      "    - trstun: 0.00%\n",
      "    - imbgeco: 0.00%\n",
      "    - aesfdrk: 0.00%\n",
      "    - brncntr: 0.00%\n",
      "    - dscrgnd: 0.00%\n",
      "    - dscrrlg: 0.00%\n",
      "    - facntr: 0.00%\n",
      "    - happy: 0.00%\n",
      "    - health: 0.00%\n",
      "    - inprdsc: 23.47%\n",
      "    - mocntr: 0.00%\n",
      "    - rlgdgr: 0.00%\n",
      "    - sclmeet: 0.00%\n",
      "    - impenva: 15.34%\n",
      "    - impfreea: 15.34%\n",
      "    - impfuna: 15.34%\n",
      "    - impricha: 15.34%\n",
      "    - impsafea: 15.34%\n",
      "    - imptrada: 15.34%\n",
      "    - ipadvnta: 15.34%\n",
      "    - ipcrtiva: 15.34%\n",
      "    - ipeqopta: 15.34%\n",
      "    - iplylfra: 15.34%\n",
      "    - ipmodsta: 15.34%\n",
      "    - ipudrsta: 15.34%\n",
      "    - País: 0.00%\n",
      "    - P_fc: 0.00%\n",
      "    - GroupCntry: 0.00%\n",
      "  Grupo 3:\n",
      "    - pplfair: 0.00%\n",
      "    - pplhlp: 0.00%\n",
      "    - ppltrst: 0.00%\n",
      "    - bctprd: 0.00%\n",
      "    - euftf: 19.92%\n",
      "    - gincdif: 0.00%\n",
      "    - lrscale: 0.00%\n",
      "    - polintr: 0.00%\n",
      "    - stfeco: 0.00%\n",
      "    - stfedu: 0.00%\n",
      "    - stfgov: 0.00%\n",
      "    - stfhlth: 0.00%\n",
      "    - stflife: 0.00%\n",
      "    - trstplc: 0.00%\n",
      "    - trstplt: 0.00%\n",
      "    - trstun: 0.00%\n",
      "    - imbgeco: 0.00%\n",
      "    - aesfdrk: 0.00%\n",
      "    - blgetmg: 15.25%\n",
      "    - brncntr: 0.00%\n",
      "    - dscrgnd: 0.00%\n",
      "    - dscrrlg: 0.00%\n",
      "    - facntr: 0.00%\n",
      "    - happy: 0.00%\n",
      "    - health: 0.00%\n",
      "    - mocntr: 0.00%\n",
      "    - rlgdgr: 0.00%\n",
      "    - sclmeet: 0.00%\n",
      "    - impenva: 13.31%\n",
      "    - impfreea: 13.14%\n",
      "    - impfuna: 13.31%\n",
      "    - impricha: 13.14%\n",
      "    - impsafea: 13.14%\n",
      "    - imptrada: 13.31%\n",
      "    - ipadvnta: 13.14%\n",
      "    - ipcrtiva: 13.14%\n",
      "    - ipeqopta: 13.14%\n",
      "    - iplylfra: 13.31%\n",
      "    - ipmodsta: 13.14%\n",
      "    - ipudrsta: 13.14%\n",
      "    - País: 0.00%\n",
      "    - P_fc: 0.00%\n",
      "    - GroupCntry: 0.00%\n",
      "  Grupo 4:\n",
      "    - pplfair: 0.00%\n",
      "    - pplhlp: 0.00%\n",
      "    - ppltrst: 0.00%\n",
      "    - bctprd: 0.00%\n",
      "    - euftf: 18.22%\n",
      "    - gincdif: 0.00%\n",
      "    - lrscale: 0.00%\n",
      "    - polintr: 0.00%\n",
      "    - stfeco: 0.00%\n",
      "    - stfedu: 0.00%\n",
      "    - stfgov: 0.00%\n",
      "    - stfhlth: 0.00%\n",
      "    - stflife: 0.00%\n",
      "    - trstplc: 0.00%\n",
      "    - trstplt: 0.00%\n",
      "    - trstun: 0.00%\n",
      "    - imbgeco: 0.00%\n",
      "    - aesfdrk: 0.00%\n",
      "    - blgetmg: 24.07%\n",
      "    - brncntr: 0.00%\n",
      "    - dscrgnd: 0.00%\n",
      "    - dscrrlg: 0.00%\n",
      "    - facntr: 0.00%\n",
      "    - happy: 0.00%\n",
      "    - health: 0.00%\n",
      "    - mocntr: 0.00%\n",
      "    - rlgdgr: 0.00%\n",
      "    - sclmeet: 0.00%\n",
      "    - impenva: 11.27%\n",
      "    - impfreea: 11.27%\n",
      "    - impfuna: 11.27%\n",
      "    - impricha: 11.27%\n",
      "    - impsafea: 11.27%\n",
      "    - imptrada: 11.27%\n",
      "    - ipadvnta: 11.27%\n",
      "    - ipcrtiva: 11.27%\n",
      "    - ipeqopta: 11.27%\n",
      "    - iplylfra: 11.27%\n",
      "    - ipmodsta: 11.27%\n",
      "    - ipudrsta: 11.27%\n",
      "    - País: 0.00%\n",
      "    - P_fc: 0.00%\n",
      "    - GroupCntry: 0.00%\n",
      "------------------------------\n",
      "Rango: 26-50%\n",
      "  Grupo 0:\n",
      "    - tvtot: 37.88%\n",
      "    - inprdsc: 47.71%\n",
      "  Grupo 1:\n",
      "    - tvtot: 41.78%\n",
      "    - blgetmg: 25.59%\n",
      "    - inprdsc: 39.15%\n",
      "  Grupo 2:\n",
      "    - netusoft: 41.69%\n",
      "    - actrolga: 41.69%\n",
      "    - atchctr: 41.69%\n",
      "    - atcherp: 41.69%\n",
      "    - blgetmg: 34.24%\n",
      "  Grupo 3:\n",
      "    - tvtot: 36.86%\n",
      "    - inprdsc: 45.85%\n",
      "  Grupo 4:\n",
      "    - tvtot: 43.81%\n",
      "    - inprdsc: 38.56%\n",
      "------------------------------\n",
      "Rango: 51-75%\n",
      "  Grupo 0:\n",
      "    - netusoft: 62.12%\n",
      "    - nwsptot: 52.29%\n",
      "    - rdtot: 52.29%\n",
      "    - actrolga: 62.12%\n",
      "    - atchctr: 62.12%\n",
      "    - atcherp: 62.12%\n",
      "    - ccnthum: 69.66%\n",
      "  Grupo 1:\n",
      "    - netusoft: 58.22%\n",
      "    - nwsptot: 60.85%\n",
      "    - rdtot: 60.85%\n",
      "    - actrolga: 58.22%\n",
      "    - atchctr: 58.22%\n",
      "    - atcherp: 58.22%\n",
      "    - ccnthum: 69.58%\n",
      "  Grupo 2:\n",
      "    - tvtot: 58.31%\n",
      "    - ccnthum: 51.61%\n",
      "  Grupo 3:\n",
      "    - netusoft: 63.14%\n",
      "    - nwsptot: 54.15%\n",
      "    - rdtot: 54.15%\n",
      "    - actrolga: 63.14%\n",
      "    - atchctr: 63.14%\n",
      "    - atcherp: 63.14%\n",
      "    - ccnthum: 72.71%\n",
      "  Grupo 4:\n",
      "    - netusoft: 56.19%\n",
      "    - nwsptot: 61.44%\n",
      "    - rdtot: 61.44%\n",
      "    - actrolga: 56.19%\n",
      "    - atchctr: 56.19%\n",
      "    - atcherp: 56.19%\n",
      "    - ccnthum: 65.93%\n",
      "------------------------------\n",
      "Rango: 76-100%\n",
      "  Grupo 0:\n",
      "    - dclagr: 89.83%\n",
      "    - dclaid: 89.83%\n",
      "    - dclcrm: 89.83%\n",
      "    - dclenv: 89.83%\n",
      "    - dclmig: 89.83%\n",
      "    - dclwlfr: 89.83%\n",
      "    - dmcntov: 95.08%\n",
      "    - ginveco: 89.83%\n",
      "    - lawobey: 89.83%\n",
      "    - trstsci: 85.85%\n",
      "    - vteumbgb: 100.00%\n",
      "  Grupo 1:\n",
      "    - dclagr: 94.49%\n",
      "    - dclaid: 94.49%\n",
      "    - dclcrm: 94.49%\n",
      "    - dclenv: 94.49%\n",
      "    - dclmig: 94.49%\n",
      "    - dclwlfr: 94.49%\n",
      "    - dmcntov: 90.17%\n",
      "    - ginveco: 94.49%\n",
      "    - lawobey: 94.49%\n",
      "    - trstsci: 85.59%\n",
      "    - vteumbgb: 100.00%\n",
      "  Grupo 2:\n",
      "    - nwsptot: 76.53%\n",
      "    - rdtot: 76.53%\n",
      "    - dclagr: 91.69%\n",
      "    - dclaid: 91.69%\n",
      "    - dclcrm: 91.69%\n",
      "    - dclenv: 91.69%\n",
      "    - dclmig: 91.69%\n",
      "    - dclwlfr: 91.69%\n",
      "    - dmcntov: 88.98%\n",
      "    - ginveco: 91.69%\n",
      "    - lawobey: 91.69%\n",
      "    - trstsci: 81.86%\n",
      "    - vteumbgb: 100.00%\n",
      "  Grupo 3:\n",
      "    - dclagr: 96.61%\n",
      "    - dclaid: 96.61%\n",
      "    - dclcrm: 96.61%\n",
      "    - dclenv: 96.61%\n",
      "    - dclmig: 96.61%\n",
      "    - dclwlfr: 96.61%\n",
      "    - dmcntov: 87.88%\n",
      "    - ginveco: 96.61%\n",
      "    - lawobey: 96.61%\n",
      "    - trstsci: 86.95%\n",
      "    - vteumbgb: 100.00%\n",
      "  Grupo 4:\n",
      "    - dclagr: 93.05%\n",
      "    - dclaid: 93.05%\n",
      "    - dclcrm: 93.05%\n",
      "    - dclenv: 93.05%\n",
      "    - dclmig: 93.05%\n",
      "    - dclwlfr: 93.05%\n",
      "    - dmcntov: 91.10%\n",
      "    - ginveco: 93.05%\n",
      "    - lawobey: 93.05%\n",
      "    - trstsci: 84.92%\n",
      "    - vteumbgb: 100.00%\n",
      "------------------------------\n",
      "\n",
      "DataFrame modificado (con columnas eliminadas):\n",
      "   netusoft  pplfair  pplhlp  ppltrst  tvtot  actrolga  bctprd  euftf  \\\n",
      "0       NaN        5       2        5    6.0       NaN       2    2.0   \n",
      "1       3.0        0       0        0    NaN       2.0       2    5.0   \n",
      "2       4.0        4       3        4    NaN       1.0       1    2.0   \n",
      "3       5.0        5       2        2    NaN       3.0       2    0.0   \n",
      "4       5.0        5       7        5    NaN       2.0       2    2.0   \n",
      "\n",
      "   gincdif  lrscale  ...  imptrada  ipadvnta  ipcrtiva  ipeqopta  iplylfra  \\\n",
      "0        1        2  ...       3.0       3.0       2.0       2.0       2.0   \n",
      "1        2        4  ...       NaN       NaN       NaN       NaN       NaN   \n",
      "2        1        3  ...       5.0       6.0       3.0       3.0       1.0   \n",
      "3        1        5  ...       2.0       5.0       3.0       1.0       1.0   \n",
      "4        1        5  ...       3.0       4.0       3.0       1.0       1.0   \n",
      "\n",
      "   ipmodsta  ipudrsta      País  P_fc  GroupCntry  \n",
      "0       4.0       3.0   Francia     8           0  \n",
      "1       NaN       NaN  Alemania     5           0  \n",
      "2       2.0       3.0  Alemania     5           0  \n",
      "3       1.0       2.0  Alemania     5           0  \n",
      "4       2.0       1.0  Alemania     5           0  \n",
      "\n",
      "[5 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "#Ahora vamos a abordar el tema de los datos faltantes en nuestro dataframe. Vamos a usar este criterio.\n",
    "#SI los datos faltantes superan el 50% del total de muestras vamos a eliminar la columna o buscar datos en webs externas\n",
    "#Que nos ofrezcan informacion referente a la variable y podamos introducir datos, si es necesario transformaremos nuestra variable\n",
    "#A los datos entrantes.\n",
    "import pandas as pd\n",
    "\n",
    "def analizar_nans_por_grupo(df, rangos_a_eliminar=None):\n",
    "    \"\"\"\n",
    "    Analiza los valores NaN en un DataFrame, mostrando el porcentaje por rango y agrupado por 'GroupCntry'.\n",
    "    Además, permite eliminar columnas según el rango especificado.\n",
    "\n",
    "    Args:\n",
    "        df: El DataFrame a analizar.\n",
    "        rangos_a_eliminar (list, opcional): Lista de rangos cuyos columnas se eliminarán. \n",
    "                                            Debe contener cadenas como \"1-25%\", \"26-50%\", etc.\n",
    "                                            Si es None, no se elimina ninguna columna.\n",
    "\n",
    "    Returns:\n",
    "        Un diccionario anidado con el análisis de NaNs (o el DataFrame modificado si se eliminaron columnas).\n",
    "    \"\"\"\n",
    "\n",
    "    grupos = df['GroupCntry'].unique()\n",
    "    rangos = {\n",
    "        \"1-25%\": {},\n",
    "        \"26-50%\": {},\n",
    "        \"51-75%\": {},\n",
    "        \"76-100%\": {}\n",
    "    }\n",
    "\n",
    "    for grupo in grupos:\n",
    "        df_grupo = df[df['GroupCntry'] == grupo]\n",
    "        porcentaje_nans = df_grupo.isnull().sum() / len(df_grupo) * 100\n",
    "\n",
    "        for columna, porcentaje in porcentaje_nans.items():\n",
    "            if porcentaje <= 25:\n",
    "                if grupo not in rangos[\"1-25%\"]:\n",
    "                    rangos[\"1-25%\"][grupo] = []\n",
    "                rangos[\"1-25%\"][grupo].append((columna, porcentaje))\n",
    "            elif porcentaje <= 50:\n",
    "                if grupo not in rangos[\"26-50%\"]:\n",
    "                    rangos[\"26-50%\"][grupo] = []\n",
    "                rangos[\"26-50%\"][grupo].append((columna, porcentaje))\n",
    "            elif porcentaje <= 75:\n",
    "                if grupo not in rangos[\"51-75%\"]:\n",
    "                    rangos[\"51-75%\"][grupo] = []\n",
    "                rangos[\"51-75%\"][grupo].append((columna, porcentaje))\n",
    "            else:\n",
    "                if grupo not in rangos[\"76-100%\"]:\n",
    "                    rangos[\"76-100%\"][grupo] = []\n",
    "                rangos[\"76-100%\"][grupo].append((columna, porcentaje))\n",
    "    \n",
    "    if rangos_a_eliminar:\n",
    "        columnas_a_eliminar = []\n",
    "        for rango in rangos_a_eliminar:\n",
    "            if rango in rangos:\n",
    "                for grupo in rangos[rango]:\n",
    "                    for columna, _ in rangos[rango][grupo]:\n",
    "                        columnas_a_eliminar.append(columna)\n",
    "        df = df.drop(columnas_a_eliminar, axis=1)  # Elimina las columnas del DataFrame original\n",
    "        return df  # Devuelve el DataFrame modificado\n",
    "    else:\n",
    "        return rangos  # Devuelve el diccionario con el análisis de NaNs\n",
    "\n",
    "# Ejemplo de uso (sin eliminar columnas):\n",
    "resultados = analizar_nans_por_grupo(df_control)\n",
    "for rango, grupos in resultados.items():\n",
    "    print(f\"Rango: {rango}\")\n",
    "    for grupo, columnas in grupos.items():\n",
    "        print(f\"  Grupo {grupo}:\")\n",
    "        if columnas:\n",
    "            for columna, porcentaje in columnas:\n",
    "                print(f\"    - {columna}: {porcentaje:.2f}%\")\n",
    "        else:\n",
    "            print(\"    Ninguna\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Ejemplo de uso (eliminando columnas en rangos específicos):\n",
    "df_modificado = analizar_nans_por_grupo(df_control, rangos_a_eliminar=[\"76-100%\"])  # Elimina columnas en los rangos 51-75% y 76-100%\n",
    "\n",
    "print(\"\\nDataFrame modificado (con columnas eliminadas):\")\n",
    "print(df_modificado.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todas las variables con un rango de 76% a 100% de datos faltantes han sido eliminadas, vamos a abordar ahora a las del Rango3(del 51%al 75%)\n",
    "#Vamos a intentar conseguir datos externos para llenar de una forma inteligente los datos faltantes, si es necesario dando formato a la variable\n",
    "#para compatibilizar los valores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 1. Identificar el cluster con menos muestras\n",
    "conteos_cluster = df_rd['GroupCntry'].value_counts()\n",
    "cluster_minoritario = conteos_cluster.idxmin()\n",
    "n_samples_cluster_minoritario = conteos_cluster.min()\n",
    "\n",
    "# 2. Calcular el tamaño total del dataset balanceado\n",
    "# Usaremos el tamaño del cluster minoritario como referencia\n",
    "n_samples_total = n_samples_cluster_minoritario * len(conteos_cluster)  # Multiplicado por el número de clusters\n",
    "\n",
    "# 3. StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=n_samples_total / len(df_rd), random_state=42)\n",
    "\n",
    "# 4. Aplicar StratifiedShuffleSplit\n",
    "for train_index, test_index in sss.split(df_rd, df_rd['GroupCntry']):\n",
    "    df_balanced = df_rd.iloc[test_index]\n",
    "\n",
    "# 5. Verificar el balanceo de clases\n",
    "print(df_balanced['GroupCntry'].value_counts())\n",
    "\n",
    "# 6. Shuffle (opcional)\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(df_balanced.head())\n",
    "\n",
    "# Guardar (opcional)\n",
    "# df_balanced.to_csv('df_balanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AHORA TENEMOS LOS CUATRO CLUSTERS, LOS TRES PRIMEROS CON DATOS ECONOMICOS DE LA ZONA EURO Y EL CUARTO\n",
    "VARIOS PAISES PROXIMOS A LA UE, ADEMAS DE TENER VINCULOS SOVIETICOS, TENEMOS ISRAEL QUE TIENE SIMILITUDES CULTURALES CON\n",
    "LOS PAISES OCCIDENTALES DE EUROPA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_ZE23CL_nulls=df_ZE23Cl.isnull().sum()\n",
    "#Tenemos datos faltantes en casi todas las columnas debido a que nuestra fuente de datos es una recoleccion de muestras de varias ediciones de encuestas. Antes de rellenar los missing values con el criterio apropiado\n",
    "#Vamos a desechar todas las muestras que tengan ciertos valores factorizados que no aportan nada:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos observado, el numero de filas ha bajado drasticamente. \n",
    "Dependiendo del numero de valores unicos de cada variable, los missing values tendran diferente valor numerico factorizado, habra que observar variable a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "v1=[7,8,9]\n",
    "df_clean1 = df_clean1[~df_clean1['netusoft'].isin(v1)]\n",
    "df_clean1.reset_index(drop=True, inplace=True)\n",
    "df_clean1['netusoft'].describe()\n",
    "'''VAR netusoft - Internet use, how often\n",
    "Value\tCategory\n",
    "1\tNever\n",
    "2\tOnly occasionally\n",
    "3\tA few times a week\n",
    "4\tMost days\n",
    "5\tEvery day\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "'''\n",
    "#Se confirma la buena eliminacion de datos invalidos. Conluimos el Bloque 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#BLOQUE 2: df_rd['actrolga','bctprd','dclagr','dclaid','dclcrm','dclenv', 'dclmig', 'dclwlfr', 'dmcntov', 'euftf', 'gincdif', 'ginveco', \n",
    "#'lawobey', 'lrscale', 'polintr','stfeco','stfedu', 'stfgov', 'stfhlth', 'stflife', 'trstplc', 'trstplt', 'trstun', 'trstsci', 'imbgeco']\n",
    "#Solo eliminaremos los valores 7,8,9 a las columnas que estos mismos hagan referencia a datos invalidos.\n",
    "v1=[7,8,9]\n",
    "B2lista = ['actrolga', 'bctprd', 'dclagr', 'dclaid', 'dclcrm', 'dclenv', 'dclmig', 'dclwlfr', 'gincdif', 'ginveco', 'lawobey', 'polintr']\n",
    "df_clean2 = df_clean1[~df_clean1[B2lista].isin(v1).any(axis=1)]\n",
    "df_clean2.reset_index(drop=True, inplace=True)\n",
    "df_clean2['actrolga'].describe()\n",
    "'''\n",
    "VAR actrolga - Able to take active role in political group(Posible variable Target BLOQUE 2)\n",
    "Value\tCategory\n",
    "1\tNot at all able\n",
    "2\tA little able\n",
    "3\tQuite able\n",
    "4\tVery able\n",
    "5\tCompletely able\n",
    "//7\tRefusal*\n",
    "//8\tDon't know*\n",
    "//9\tNo answer*\n",
    "'''\n",
    "#Como se observa en una de las variables filtradas, los valores unicos superiores o iguales a 6, que hacen referencia a valores no validos han sido eliminados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bloque 4: df_clean3['impenva','impfreea','impfuna','impricha','impsafea','imptrada','ipadvnta','ipcrtiva','iplylfra','ipmodsta','ipudrsta']\n",
    "\n",
    "Afortunadamente este Bloque ya esta bien filtrado, no existen valores numericos factorizados 7,8,9 que hagan referencia a datos invalidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Borramos variables obsoletas\n",
    "import gc\n",
    "B3lista = None\n",
    "df_clean2 = None\n",
    "v1=None\n",
    "gc.collect()\n",
    "\n",
    "df_clean3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criterio de relleno de datos faltantes. Al ser variables categóricas factorizadas sus datos han de ser exactos, por lo que usare la moda, pero voy a agrupar de forma rigurosa el numero de filas\n",
    "# las cuales voy a rellenar teniendo en cuenta de si el encuestado es de origen del pais donde se le hace el estudio , extrangero o si tiene herencia cultural extrangera debido a que uno de sus padres\n",
    "#es extrangero. Son datos categoricos que pueden distorsionar las muestras rellenadas.\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have df_clean3 with columns 'cntry' and others\n",
    "\n",
    "# Factorize 'cntry' (optional, for numerical representation)\n",
    "try:\n",
    "  df_clean3['cntryfcz'] = pd.factorize(df_clean3['cntry'])[0]\n",
    "except (ValueError, TypeError):  # Handle potential errors during factorize\n",
    "  print(\"Error occurred during factorizing 'cntry'. Check for non-string values or inconsistencies.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#vamos a rellenar datos faltantes de las columnas mas importantes del dataset, primero de ello vamos a eliminar 3 columnas realizando una nueva con ellas como combinacion\n",
    "#Respetando sus estadisticas.\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "'''Var orig - determinar_origen \n",
    "Value\tCategory\n",
    "1 Nacional ambos padres\n",
    "2 Nacional, almenos un padre extranjero'\n",
    "3 'Extranjero'\n",
    "'''\n",
    "# Crear un imputador que reemplace los valores faltantes por la moda\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Aplicar la imputación a las columnas 'brncntr', 'mocntr' y 'facntr'\n",
    "df_clean3[['brncntr', 'mocntr', 'facntr']] = imputer.fit_transform(df_clean3[['brncntr', 'mocntr', 'facntr']])\n",
    "\n",
    "# Crear la variable 'origfcz' basada en las nuevas variables imputadas\n",
    "# (Aquí puedes adaptar la lógica según tus necesidades)\n",
    "df_clean3['origfcz'] = np.where((df_clean3['brncntr'] == 1) & (df_clean3['mocntr'] == 1), 1,  # Ambos padres nacionales\n",
    "                   np.where((df_clean3['brncntr'] == 1) | (df_clean3['mocntr'] == 1), 2,  # Al menos un padre extranjero\n",
    "                            3))  # Ninguno de los padres nació en el país"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Considero que la combinacion de esas tres variables en una con tres posibles valores es apropiada para el calculo y el ahorro de espacion en el dataframe\n",
    "df_clean3['origfcz'].describe()\n",
    "df_clean4 = df_clean3.drop(['brncntr', 'mocntr', 'facntr'], axis=1,)\n",
    "del df_clean3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Primero de todo vamos a limpiar de missing values a las columnas con datos faltantes utilizando la moda a traves de KNN que realiza una asignacion modal en este caso\n",
    "#teniendo en cuenta las ubicaciones de las distribuciones de las variables que se agrupan por centroides vectorialmente óptimas.\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# Identificar columnas con pocos datos faltantes\n",
    "columns_with_few_missing = df_clean4.columns[df_clean4.isnull().mean() < 0.21]\n",
    "\n",
    "# Imputar por moda las columnas con pocos datos faltantes\n",
    "imputer_mode = SimpleImputer(strategy='most_frequent')\n",
    "df_clean4[columns_with_few_missing] = imputer_mode.fit_transform(df_clean4[columns_with_few_missing])\n",
    "\n",
    "# Imputar el resto de las columnas con KNN\n",
    "imputer_knn = KNNImputer(n_neighbors=6)\n",
    "df_imputed = imputer.fit_transform(df_clean4,)\n",
    "df_clean5 = df_imputed = pd.DataFrame(df_imputed, columns=df_clean4.columns, index=df_clean4.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df_clean5.info()\n",
    "print(df_clean5['cntry'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "df_clean5['origfcz'] = df_clean5['origfcz'].astype(str)\n",
    "\n",
    "Origen = {'1':'Nacional', '2':'Mixto', '3':'Extranjero'}\n",
    "\n",
    "df_clean5['cntry'] = df_clean5['cntry'].map(PaisesEuro)\n",
    "df_clean5['orig'] = df_clean5['origfcz'].map(Origen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean5['orig'].unique(),'\\n',df_clean5['origfcz'].unique(),'\\n',df_clean5['cntry'].unique(),'\\n',df_clean5['cntryfcz'].unique())\n",
    "#Como vemos ahora tenemos nuestras variables correctamente y el dataset original de 357k filas en condiciones para poder trabajar con el."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agrupar por país\n",
    "grouped = df_clean5.groupby('cntryfcz')\n",
    "\n",
    "# Calcular el tamaño de la muestra por grupo (ajusta el factor según tu necesidad)\n",
    "sample_size = 100000\n",
    "group_sizes = grouped.size()\n",
    "group_probs = group_sizes / group_sizes.sum()\n",
    "samples_per_group = (group_probs * sample_size).astype(int)\n",
    "\n",
    "# Crear un nuevo DataFrame vacío\n",
    "df_rd = pd.DataFrame()\n",
    "\n",
    "# Muestrear aleatoriamente dentro de cada grupo\n",
    "for group, data in grouped:\n",
    "    sample = data.sample(n=samples_per_group[group])\n",
    "    df_rd= pd.concat([df_rd, sample])\n",
    "\n",
    "# Restablecer el índice\n",
    "df_rd = df_rd.reset_index(drop=True)\n",
    "df_rd = df_rd.dropna(subset=['cntry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hacemos una Lista de todas las variables para hacer una tabla en SQL y poder utilizar los datos en nuestro documento principal del EDA.\n",
    "import pandas as pd\n",
    "\n",
    "def vartypev2(df):\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    tipos_exactos = df.dtypes.to_dict()\n",
    "\n",
    "    data = []\n",
    "    for col in num_cols:\n",
    "        data.append({'Tipo': 'Numérico', 'Tipo_de_Dato_SQL': 'REAL', 'Columna': col})  # Ajusta 'REAL' según tu SGBD\n",
    "    for col in cat_cols:\n",
    "        data.append({'Tipo': 'Categórico', 'Tipo_de_Dato_SQL': 'TEXT', 'Columna': col})\n",
    "\n",
    "    data.append({'Tipo': 'Total Numérico', 'Columna': len(num_cols), 'Tipo_de_Dato_SQL': 'INTEGER'})\n",
    "    data.append({'Tipo': 'Total Categórico', 'Columna': len(cat_cols), 'Tipo_de_Dato_SQL': 'INTEGER'})\n",
    "    data.append({'Tipo': 'Total General', 'Columna': len(df.columns), 'Tipo_de_Dato_SQL': 'INTEGER'})\n",
    "\n",
    "    df_tipos = pd.DataFrame(data)\n",
    "    return df_tipos\n",
    "\n",
    "# Ejemplo de uso\n",
    "df_rd_vars = vartypev2(df_rd)\n",
    "print(df_rd_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAMOS POR CONCLUIDO LA EXTRACCION Y PREPARACION DEL DATASET PRINCIPAL.\n",
    "EL WEBSCRAPPING Y ALMACENAMIENTO DE LAS TABLAS DEL BANCO MUNDIAL SE DARAN EN EL EXPLORER DATARESEARCH 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".FPJBGPCSobremesa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
